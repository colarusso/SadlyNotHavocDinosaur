<!DOCTYPE html>
<HTML><HEAD>

  <!-- Set base for this page equal to domain root -->
  <base href="../../">

  <!-- Page-specific metadata -->
  <title>The AI-Powered Everything Extension: Curate a Mathemagician's spellbook w/o doing any arithmetic (LLM "functions" that run in your browser)</title>
  <meta property="og:type" content="website"/>
  <meta property="og:publish_date" content="2024-03-28T00:00:00-0500"/>
  <meta property="og:title" content="The AI-Powered Everything Extension: Curate a Mathemagician's spellbook w/o doing any arithmetic (LLM "functions" that run in your browser)"/>
  <meta property="og:description" content="I built the LIT Prompts extension thinking it would be a nice sandbox for my students‚Äîa place for them to &quot;play&quot; with large language models (LLMs). It's turned into so much more. By making it possible to write functions that operate on text in the browser it's done something I haven't really seen anywhere else. Many of the prompts I've described over the last 10 weeks can be implemented in chat with ChatGPT, Claude, Gemini, et al. However, such implementations require you to do a heck of a lot of cut-and-paste. By giving prompts access to the browser and making them reusable, editable, and interconnected, we've opened up a whole new set of possibilities. Sure, with AI being baked into all of our apps, you can find a plethora of tools to summarize this page, but where else have you seen someone happen upon a whole new mode of computer-human interaction or form of collaborative storytelling? What other tools offer the ability for non-coders to produce rapid prototypes for workflows that if adopted would save millions in unnecessary litigation? What tools let you create your own AI word processor in a matter of minutes? Tools that move beyond spellcheck and grammar check to introduce argument checks and unit tests for the written word. Without intending to, I accidentally created an AI-powered extension capable of &quot;doing it all.&quot; Define text on a website, translate text, show only the recipe hiding at the end of a post, answer emails, facilitate self-reflection... and all at the click of a button without cut-and-paste. If you haven't already, you really should install the extension. ;) "/>
  <meta property="og:image" content="http://www.davidcolarusso.com/images/50-days/mathemagician_square.png"/>
  <meta property="og:image:width" content="1024" />
  <meta property="og:image:height" content="1024" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ST9X6H808L"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-ST9X6H808L');
  </script>

  <!-- Metadata for mobile -->
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
  <meta name="apple-mobile-web-app-capable" content="no" />
  <link rel="apple-touch-icon" href="images/comic.png"/>

  <!-- JS & style -->
  <link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
  <link rel="stylesheet" type="text/css" href="css/style.css?v=2024-01-30">
  <script src="js/functions.js?v=2024-01-30"></script>
  <script src="js/spin.js"></script>

  <link rel="stylesheet" href="css/prism.css" data-noprefix="">
  <script type="text/javascript" src="js/prism.js"></script>

  <!--<script id="MathJax-script" async src="js/mathjax/tex-mml-chtml.js"></script>

  <link rel="stylesheet" type="text/css" href="css/green-audio-player.css">
  <script src="js/green-audio-player.js"></script>-->

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="Sadly Not, Havoc Dinosaur" href="https://sadlynothavocdinosaur.com/feed.xml" />

</HEAD>
<BODY BGCOLOR="#ffffff" BACKGROUND="" MARGINWIDTH="0" MARGINHEIGHT="0">

<!-- Message Banner -->
<div id="msg_bar" style="display:none;"></div>

<!-- Title and search -->
<div class="title_bar">
  <div class="home">
    <a href="./" tabindex="1"><img src="images/home.png" class="home_btn"></a>
  </div>
  <div class="search">
    <a href="javascript:show_search();" tabindex="3"><img src="images/search.png" class="search_btn"></a>
    <input id="query" type="text" tabindex="2"/>
  </div>
  <span id="title"><a href="./" class="title_home">Sadly Not, Havoc Dinosaur</a></span>
</div>

<div class="content">
  <!-- START PAGE CONTENT -->

  <div id="page">
  <!--
    =================================================

                      INTRODUCTION

    =================================================
  -->
  <h1 class="post_title_01">The AI-Powered Everything Extension</h1>
  <div class="post_title_02">Curate a Mathemagician's spellbook w/o doing any arithmetic (LLM "functions" that run in your browser)</div>
  <div class="featured_img_right">
    <!--<div class="audio_container_container" style="display:show;">
      <div class="audio_container">
        <b>Hear the author read <i>TK</i></b>
        <div class="gap-example player-accessible">
          <audio>
              <source src="mp3s/title.mp3" type="audio/mpeg">
          </audio>
        </div>
        <span class="playback">
          Speed: <a href="javascript:void('')" onClick="set_speed(0.5)" class="playback" id="pb05">0.5x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(1)" class="playback" id="pb10" style="font-weight:900;">1x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(1.5)" class="playback" id="pb15">1.5x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(2)" class="playback" id="pb20">2x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(3)" class="playback" id="pb30">3x</a>
        </span>
      </div>
    </div>-->
    <a href="images/50-days/mathemagician.png"><img src="images/50-days/mathemagician.png" ALT="An image of a paperback of the Phantom Toolbooth opened to a page with an illustration of the Mathemagician" class="list_img_file"/></a>
    <div class="caption">
      The Mathemagician from my copy of <a href="https://en.wikipedia.org/wiki/The_Phantom_Tollbooth" target="_blank" class="captionlnk">The Phantom Tollbooth</a>
    </div>
  </div>
  <p class="post_p">
    <a href="https://mastodon.social/@Colarusso" target="_blank" class="body_links"><img src="images/colarusso.jpg" class="headshot_small" alt="Headshot of the author, Colarusso." style="margin-top: 7px;"/></a>
    David Colaursso<br><span class="post_date">Co-director, Suffolk's <a href="https://suffolklitlab.org/" target="_blank" class="captionlnk">Legal Innovation &amp; Tech Lab</a></span>
  </p>
  <p><i>This is <b>the 50th</b> post in my series <a href="posts/50-days-of-lit-prompts">50 Days of LIT Prompts</i></a>.</p>

  <p>
    I built the <a href="posts/spellbook/#install" onClick="expand_setup();">LIT Prompts</a> extension thinking it would be a nice sandbox for my students‚Äîa place for them to "play" with <a href="https://en.wikipedia.org/wiki/Large_language_model" target="_blank">large language models</a> (LLMs). It's turned into so much more. By making it possible to write <a href="https://en.wikipedia.org/wiki/Function_(computer_programming)" target="_blank">functions</a> that operate on text in the browser, it's done something I haven't really seen anywhere else. Many of the prompts I've described over the last 10 weeks can be implemented in chat with ChatGPT, Claude, Gemini, et al. However, such implementations require you to do a heck of a lot of cut-and-paste. By giving prompts access to the browser and making them reusable, editable, and interconnected, we've opened up a whole new set of possibilities. Sure, with AI being baked into all of our apps, you can find a plethora of tools to <a href="posts/summarize-and-question">summarize this page</a>, but where else have you seen someone happen upon a whole new mode of <a href="posts/magic-copy-and-paste">computer-human</a> interaction or form of <a href="posts/unwritten">collaborative storytelling</a>? What other tools offer the ability for non-coders to produce <a href="posts/uspto-llms">rapid prototypes</a> for workflows that if adopted would save millions in unnecessary litigation? What tools let you create your own <a href="posts/word-processor">AI word processor</a> in a matter of minutes? Tools that move beyond spellcheck and grammar check to introduce argument checks and <a href="posts/ai-rubric">unit tests</a> for the written word. Without intending to, I accidentally created an AI-powered extension capable of using LLMs to "do it all." <a href="posts/define-words">Define</a> text on a website, <a href="posts/translate-selected">translate text</a>, show only <a href="posts/recipe">the recipe</a> hiding at the end of a post, <a href="posts/emails">answer emails</a>, facilitate <a href="posts/reflection">self-reflection</a>... and all at the click of a button without cut-and-paste. If you haven't already, you really should <a href="posts/spellbook/#install" onClick="expand_setup();">install the extension</a>. ;)
  </p>
  <p>
    What's special here is that this collection of functions lives in one place, your browser. Together these functions create what some are calling AI <a href="https://www.oneusefulthing.org/p/now-is-the-time-for-grimoires" target="_blank">spellbooks</a>, collections of prompts that leverage the expertise of their authors to do useful things. The fact that persistent versions of these prompts live in one place means you can return to them again and again, not just to use them but to improve them. By exposing them to iterative improvements, we can transform these prompts from toys into tools. I'm of two minds when it comes to the spellbook naming. I have to admit that writing prompts does feel like crafting incantations. The name is very evocative, but I worry about equating AI with magic. As Simon Willison has <a href="https://simonwillison.net/2022/Oct/5/spell-casting/" target="_blank">observed</a> the utility of this metaphor changes based on the audience, "The key challenge here is to avoid implying that these systems are 'magical' in that they are incomprehensible and mysterious. As such, I believe the metaphor is only appropriate when you're talking to people who are working with these systems from a firm technical perspective." AI isn't magic. It's Math! So, I've decided to evoke the image of the Mathemagician. 
  </p>
  <p style="text-align: center;"><font style="font-size: 35px; line-height: 40px;color: brown;font-weight: 600;">&ldquo;AI isn't magic. It's Math!&rdquo;</font></p>
  <p>
    Over the past 10 weeks I've been curating my own Mathemagician's spellbook. It doesn't include all of the prompts we've explored together, but it does include those I'm most likely to reach for while using my browser. I offer it to you today as a parting gift (you can download everything as one file <a href="posts/spellbook/#working">below</a>). You may also appreciate <a href="posts/tldr-references">this list</a> of articles I've linked to during our time together. Thank you for accompanying me on this journey. The <a href="posts/50-days-of-lit-prompts">series</a> may be ending, but it is not goodbye. These 10 weeks have given me a lot to think about, and I expect I will distill some of these thoughts into additional posts. You can follow my <a href="https://sadlynothavocdinosaur.com/feed.xml">RSS feed</a> should you want to keep an eye out for them. Need an RSS reader? <a href="posts/rss-reader">I have you covered</a>. 
  </p>
  <p>
    FWIW, here's a rundown of the prompts included <a href="posts/spellbook/#working">below</a>, and thank you again. Be seeing you.
  </p>

  <h3><a name="functions" href="posts/spellbook/#functions" class="anchor" alt="deep link to this section"></a>Functions in this Spellbook</h3>
  <ol>
    <li><b>üìù Summarize and question selection</b>.<ul><i>Inspired by <a href="posts/summarize-and-question">Using AI to Distill and Question Texts</a>: Summarize and question the contents of a webpage from within the browser</i></ul> 
    <li><b>üåé Blurb page</b>. <ul><i>Inspired by <a href="posts/tldr-post">TL;DR LLM</a>: Blurb a webpage for social media</i></ul>
    <li><b>üìñ Define selected</b>. <ul><i>Inspired by <a href="posts/define-words">A Rose by Any Other Name</a>: Define a selected word, phrase, idiom, or initialism on a webpage</i></ul>
    <li><b>üìñ Define selected w/ context</b>. <ul><i>Inspired by <a href="posts/define-words-in-context">You Can't Be Neutral on a Moving Train</a>: Define a selected word, phrase, idiom, or initialism given the context of the webpage on which it is found</i></ul>
    <li><b>ü™Ñ Magic copy-and-paste</b>. <ul><i>Inspired by <a href="posts/magic-copy-and-paste">Magic Copy-and-Paste</a>: Use AI to pluck out and copy only what you want AKA entity extraction</i></ul>
    <li><b>üí¨ Translate selected</b>. <ul><i>Inspired by <a href="posts/translate-selected">The Dream of Universal Translation</a>: Translate text into English then translate your reply into the original language</i></ul>
    <li><b>üíª Add code comments</b>. <ul><i>Inspired by <a href="posts/code-comments">Auto-Generate Code Comments</a>: In which "AI" suggests comments and explains a function that approximates pi (œÄ)</i></ul>
    <li><b>üìö Get citations from selected</b>. <ul><i>Inspired by <a href="posts/find-my-cites">AI-assisted LIT Reviews</a>: Create a sphreadsheet of citations found in a paper with only a few keystrokes</i></ul>
    <li><b>üóúÔ∏è Shorten selected text</b>. <ul><i>Inspired by <a href="posts/shorten">500 Characters? I'll Make It Fit!</a> Use AI to shorten text</i></ul>
    <li><b>üî¨ Expand selected text</b>. <ul><i>Inspired by <a href="posts/magnify">Magnifying Ideas and Expanding Text with AI</a>: In which I show you how to have AI write something both personal and original, bucking the assumptions that AI writing can never be personal or original</i></ul>
    <li><b>üë©üèª‚Äçüè´ Suggest plain language</b>. <ul><i>Inspired by <a href="posts/legalese">Translate Legalese</a>: An AI tool for rewriting texts in plain language</i></ul>
    <li><b>ü§∑ Anticipate reader questions</b>. <ul><i>Inspired by <a href="posts/reader-questions">Anticipating Reader Questions</a>: Have AI/LLMs suggest questions readers might have about your writing</i></ul>
    <li><b>üïµüèΩ‚Äç‚ôÄÔ∏è Flag logical fallacies (JSON)</b>. <ul><i>Inspired by <a href="posts/flag-fallacies">Flag Logical Fallacies with a Browser Extension</a>: Have AI read a webpage and flag logical fallacies</i></ul>
    <li><b>üìï Apply house style guide</b>. <ul><i>Inspired by <a href="posts/style-guide">The Elements of Interactive Style Guides</a>: Have AI provide writing feedback based on a house style guide</i></ul>
    <li><b>üóûÔ∏è Suggest headlines/titles</b>. <ul><i>Inspired by <a href="posts/headlines">Follow This One Trick to Write Great Headlines</a>: Produce a collection of possible <strike>clickbait</strike> compelling headlines based on the text of an article</i></ul>
    <li><b>üì´ Decline selected (email)</b>. <ul><i>Inspired by <a href="posts/decline-requests">I'm Sorry, Dave Can't Do That</a>: Use the text of an email to draft a polite reply declining any request(s)</i></ul>
    <li><b>üì´ Reply to selected (email)</b>. <ul><i>Inspired by <a href="posts/emails">Not Quite Dictation</a>: Draft emails based on preset scenarios and an "AI"-mediated Q&amp;A</i></ul>
    <li><b>üì´ Copy text to book call</b>. <ul><i>Inspired by <a href="posts/form-letters">Old-School Automation</a>: Complete a "form letter" without engaging an LLM</i></ul>
    <li><b>üòà Engage devil's advocate</b>. <ul><i>Inspired by <a href="posts/devils-advocate">Summon the Demon</a>: Strengthen your arguments with an AI-powered devil's advocate</i></ul>
    <li><b>üò° Anger Translator</b>. <ul><i>Inspired by <a href="posts/anger-translator">Your Own Personal Anger Translator</a>: Translate angry comments into thoughtful replies in context</i></ul>
    <li><b>ü™© Daily reflection</b>. <ul><i>Inspired by <a href="posts/reflection">My Students Have Been Using an Interactive Tool for Reflective Journaling & I Love It!</i></ul>
    <li><b>üß† Make true-false from selected</b>. <ul><i>Inspired by <a href="posts/create-true-false-questions">True or False?</a> Use AI to draft true or false questions based on a reading assignment</i></ul>
    <li><b>üß† Multiple choice from selected</b>. <ul><i>Inspired by <a href="posts/create-multiple-choice-questions">All of the Above</a>: Use AI to draft multiple choice questions based on a reading assignment</i></ul>
    <li><b>üß† Answer selected question</b>. <ul><i>Inspired by <a href="posts/answer-questions-with-context">2.B, or Not 2.B?</a> Have an AI answer multiple choice and true or false questions based on a reading assignment</i></ul>
    <li><b>üé≤ üé≤ Random</b>. <ul><i>Inspired by <a href="posts/coinflip-poem">Flip a Poem; Roll an "App"</a>: Turn the outcome of a coin flip into a poem and package this as an "app" </i></ul>
    <li><b>üï∞Ô∏è Time2Poem</b>. <ul><i>Inspired by <a href="posts/coinflip-poem">Flip a Poem; Roll an "App"</a>: Turn the outcome of a coin flip into a poem and package this as an "app" </i></ul>
  </ol>

  <!-- END INTRO -->

  <h3><a name="build" href="posts/spellbook/#build" class="anchor" alt="deep link to this section"></a>Let's build something!</h3>
  <p>
    We'll do our building in the LIT Prompts extension. If you aren't familiar with the LIT Prompts extension, don't worry. We'll walk you through setting things up before we start building. If you have used the LIT Prompts extension before, skip to <a href="posts/spellbook/#template">The Prompt Pattern (Template)</a>.
  </p>
  <h3><a name="upnext" href="posts/spellbook/#upnext" class="anchor" alt="deep link to this section"></a>Up Next</h3>
  <ul>
    <li><a href="posts/spellbook/#setup" onClick="expand_setup();">Setup LIT Prompts</a></li>
    <ul>
      <li><a href="posts/spellbook/#install" onClick="expand_setup();">Install the extension</a></li>
      <li><a href="posts/spellbook/#point" onClick="expand_setup();">Point it at an API</a></li>
    </ul>
    <li><a href="posts/spellbook/#template">The Prompt Pattern (Template)</a></li>
  </ul>
  <p>
    <b>Questions or comments?</b> I'm on Mastodon <a href="https://mastodon.social/@Colarusso" target="_blank">@Colarusso@mastodon.social</a>
  </p>
  <!--
    =================================================

                   Setup LIT Prompts

    =================================================
  -->
  <hr>
  <h2><a name="setup" href="posts/spellbook/#setup" onClick="expand_setup();" class="anchor" alt="deep link to this section"></a>Setup LIT Prompts </h2>
  <div id="expand_setup" style="text-align: left;display:none;font-size: small;">
    <a href="javascript:expand_setup();" style="text-decoration: none;">&#9658; Expand</a>
  </div>
  <div id="collapse_setup" style="text-align: left;font-size: small;">
    <a href="javascript:collapse_setup();" style="text-decoration: none;">&#9660; Collapse</a>
  </div>
  <div id="setup_extension">
    <div class="list_vid">
      <iframe class="embed_vid" src="https://www.youtube-nocookie.com/embed/Ql8aXGvLBGU" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
      <div class="caption">
        7 min intro video
      </div>
    </div>
    <p>
      <i><b>LIT Prompts</b></i> is a browser extension built at Suffolk University Law School's <a href="https://suffolklitlab.org/" target="_blank">Legal Innovation and Technology Lab</a> to help folks explore the use of <a href="https://en.wikipedia.org/wiki/Large_language_model" target="_blank">Large Language Models</a> (LLMs) and <a href="https://en.wikipedia.org/wiki/Prompt_engineering" target="_blank">prompt engineering</a>. LLMs are sentence completion machines, and prompts are the text upon which they build. Feed an LLM a prompt, and it will return a plausible-sounding follow-up (e.g., "Four score and seven..." might return "years ago our fathers brought forth..."). LIT Prompts lets users create and save prompt templates based on data from an active browser window (e.g., selected text or the whole text of a webpage) along with text from a user. Below we'll walk through a specific example.
    </p>
    <p>
      To get started, follow <b>the first four minutes</b> of the intro video or the steps outlined below. <i>Note: The video only shows Firefox, but once you've installed the extension, the steps are the same.</i>
    </p>
    <h3><a name="install" href="posts/spellbook/#install" class="anchor" alt="deep link to this section"></a>Install the extension</h3>
    <p>Follow the links for your browser.</p>
    <ul>
      <li>
        <b>Firefox:</b> (1) visit the extension's <a href="https://addons.mozilla.org/en-US/firefox/addon/lit-prompts/" target="_blank">add-ons page</a>; (2) click "Add to Firefox;" and (3) grant permissions.
      </li>
      <li>
        <b>Chrome:</b>  (1) visit the extension's <a href="https://chromewebstore.google.com/detail/lit-prompts/hfeojjmldhebkeknfapoghcohkhffcmp" target="_blank">web store page</a>; (2) click "Add to Chrome;" and (3) review permissions / "Add extension."
      </li>
    </ul>
    <p>
      If you don't have Firefox, you can <a href="https://www.mozilla.org/en-US/firefox/new/" target="_blank">download it here</a>. Would you rather use Chrome? <a href="https://www.google.com/chrome/" target="_blank">Download it here</a>.
    </p>
    <h3><a name="point" href="posts/spellbook/#point" class="anchor" alt="deep link to this section"></a>Point it at an API</h3>
    <p>
      Here we'll walk through how to use an LLM provided by OpenAI, but you don't have to use their offering. If you're interested in alternatives, you can find them <a href="https://github.com/SuffolkLITLab/prompts/tree/main#openai-compatible-api-integration" target="_blank">here</a>. You can even run your LLM locally, avoiding the need to share your prompts with a third-party. If you need an OpenAI account, you can <a href="https://platform.openai.com/signup" target="_blank">create one here</a>. Note: when you create a new OpenAI account you are given a limited amount of free API credits. If you created an account some time ago, however, these may have expired. If your credits have expired, you will need to enter a <a href="https://platform.openai.com/account/billing/overview" target="_blank">billing method</a> before you can use the API. You can check the state of any credits <a href="https://platform.openai.com/usage" target="_blank">here</a>.
    </p>
    <p>
      Login to <a href="https://openai.com/" target="_blank">OpenAI</a>, and navigate to the <a href="https://platform.openai.com/docs/" target="_blank">API documentation</a>.
    </p>
    <div class="featured_img_center">
      <a href="images/50-days/OpenAI_keys.png"><img src="images/50-days/OpenAI_keys.png" ALT="Screenshot of the OpenAI API Keys page showing where to click to create a new key." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
    </div>
    <p>Once you are looking at the API docs, follow the steps outlined in the image above. That is:
    <ol>
      <li>Select "API keys" from the left menu</li>
      <li>Click "+ Create new secret key"</li>
    </ol>
    <hr>
    <p>
      On LIT Prompt's <i>Templates & Settings</i> screen, set your API Base to <code>https://api.openai.com/v1/chat/completions</code> and your API Key equal to the value you got above after clicking "+ Create new secret key".  You get there by clicking the <i>Templates & Settings</i> button in the extension's popup:
    </p>
    <ol>
      <li>open the extension</li>
      <li>click on  <i>Templates & Settings</i></li>
      <li>enter the API Base and Key (under the section <i>OpenAI-Compatible API Integration</i>)</li>
    </ol>
    <div class="featured_img_center">
      <a href="images/50-days/popup.png"><img src="images/50-days/popup.png" ALT="Screenshot of the LIT Prompts extension popup." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
    </div>
    <p>
      Once those two bits of information (the API Base and Key) are in place, you're good to go. Now you can edit, create, and run prompt templates. Just open the LIT Prompts extension, and click one of the options. I suggest, however, that you read through the <i>Templates and Settings</i> screen to get oriented. You might even try out a few of the preloaded prompt templates. This will let you jump right in and get your hands dirty in the next section.
    </p>
    <div class="featured_img_center">
      <a href="images/50-days/credentials.png"><img src="images/50-days/credentials.png" ALT="Screenshot of the LIT Prompts extension popup." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
    </div>
    <p>
      <span style="background:yellow;">If you receive an error when trying to run a template after entering your Base and Key, and you are using OpenAI, make sure to check the state of any credits <a href="https://platform.openai.com/usage" target="_blank">here</a>. If you don't have any credits, you will need a billing method on file.</span>
    </p>
    <p>
      <i>If you found this hard to follow, consider following along with the first four minutes of the video <a href="posts/spellbook/#setup">above</a>. It covers the same content. It focuses on Firefox, but once you've installed the extension, the steps are the same.</i>
    </p>
  </div>

  
  <!--
    =================================================

                   Write Your Template

    =================================================
  -->
  <hr>
  <h2><a name="template" href="posts/POSTSLUG/#template" class="anchor" alt="deep link to this section"></a>The Prompt Patterns (Templates)</h2>

  <div class="featured_img_right">
    <a href="images/boxquote.png"><img src="images/boxquote.png" ALT="A slide showing the George Box quote: All models are wrong, but some models are useful." class="list_img_file"/></a>
    <div class="caption">
      Maps are models; they don't show everything. That's okay as long as you don't confuse the map for the territory.
    </div>
  </div>

  <p>
    Fair warning: if this is the first set of templates you're working with, this might be a little overwhelming. If you find yourself lost, I suggest heading back to the beginning of <a href="posts/50-days-of-lit-prompts">the serise</a>. You don't have to read through every post, but they are ordered to help introduce how things work, building up in complexity over time. I'd consider looking through at least the first two weeks. 
  </p>

  <p>
    When crafting a LIT Prompts template, we use a mix of plain language and variable placeholders. Specifically, you can use double curly brackets to encase predefined variables. If the text between the brackets matches one of our predefined variable names, that section of text will be replaced with the variable's value. 
  </p>

  <p>
    The <code>{{innerText}}</code> variable will be replaced by the <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/innerText" target="_blank">innerText</a> of your current page (roughly speaking the hard-coded text of a page). 
  </p>


  <p>
    The <code>{{highlighted}}</code> variable contains any text you have highlighted/selected in the active browser tab when you open the extension. 
  </p>

  <p>
    The <code>{{scratch}}</code> variable contains the text in your Scratch Pad. Remember, the scratch pad is accessible from the extension's popup window. The button is to the right of the Settings & Templates button that you have used before. 
  </p>


  <p>
    The <code>{{Month}}</code> variable will be replaced by the English name for the current month. 
  </p>
  <p>
    The <code>{{DayOfWeek}}</code> variable will be replaced by the English name for the day of the week.
  </p>
  <p>
    The <code>{{hours}}</code> variable will be replaced by the current hour (1-12).
  </p>
  <p>
    The <code>{{minutes2d}}</code> variable will be replaced by the current two-digit minutes past the hour (00-59).
  </p>
  <p>
    The <code>{{ampm}}</code> variable will be replaced by the "am" or "pm." 
  </p>

  <p>FWIW, here is the full list of date and time variables:</p>
    <ul>
      <li>Day of week (0-6): <code>{{dayOfWeek}}</code></li>
      <li>Day of week (English): <code>{{DayOfWeek}}</code></li>
      <li>Month (1-12): <code>{{month}}</code></li>
      <li>Month (01-12): <code>{{month2d}}</code></li>
      <li>Month (English): <code>{{Month}}</code></li>
      <li>Day of Month (1-31): <code>{{day}}</code></li>
      <li>Day of Month (01-31): <code>{{day2d}}</code></li>
      <li>Year: <code>{{year}}</code></li>
      <li>Hour (1-12): <code>{{hours}}</code></li>
      <li>Hour (01-12): <code>{{hours2d}}</code></li>
      <li>Hour (0-23): <code>{{hours24}}</code></li>
      <li>Hour (00-23): <code>{{hours242d}}</code></li>
      <li>AM or PM: <code>{{ampm}}</code></li>
      <li>Minute (0-59): <code>{{minutes}}</code></li>
      <li>Minute (00-59): <code>{{minutes2d}}</code></li>
      <li>Second (0-59): <code>{{seconds}}</code></li>
      <li>Second (00-59): <code>{{seconds2d}}</code></li>
    </ul>
  </p>

 

  <p>
    The <code>{{coinFlip}}</code> variable will be replaced by either "heads" or "tails". 
  </p>
  <p>
    The <code>{{d6}}</code> variable will be replaced by a random number between 1 and 6. 
  </p>
  <p>
    The <code>{{d%}}</code> variable will be replaced by a random number between 0 and 9. 
  </p>

  <p>FWIW, here is the full list of "random" output variables:</p>
    <ul>
      <li>Coin Flip (heads or tails): <code>{{coinFlip}}</code></li>
      <li>D4 (1-4): <code>{{d4}}</code></li>
      <li>D6 (1-6): <code>{{d6}}</code></li>
      <li>D8 (1-8): <code>{{d8}}</code></li>
      <li>D% (0-9): <code>{{d%}}</code></li>
      <li>D20 (1-20): <code>{{d20}}</code></li>
    </ul>
  </p>  

  <p>
    If the text within brackets is not the name of a predefined variable, like <code>{{What is your name?}}</code>, it will trigger a prompt for your user that echo's the placeholder (e.g., a text bubble containing, "What is your name?"). After the user answers, their reply will replace this placeholder. A list of predefined variables can be found in the extension's <a href="https://github.com/SuffolkLITLab/prompts#prompt-templates" target="_blank">documentation</a>. 
  </p>

  <p>
    To use <code>{{passThrough}}</code>, however, we make use of the Post-run Behavior parameter to pass data from one template to another. If you use Post-run Behavior to send one template's output to another template, the first template's output can be read by the second template via the <code>{{passThrough}}</code> variable. 
  </p>


  <p>Here's the first template's title.</p>
  <p><code>üåé Summarize and question page</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{innerText}}
  
Provide a short 150 word summary of the above text. If asked any followup questions, use the above text, and ONLY the above text, to answer them. If you can't find an answer in the above text, reply, "I don't know." You can, however, finish a thought you started above if asked to continue, but don't write anything that isn't supported by the above text. And keep all of your replies short! 

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-16k</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting‚Äî0.</li>
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üìù Summarize and question selection</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{highlighted}} 

---

Provide a short 150-word summary of the above text. If asked any follow-up questions, use the above text, and ONLY the above text, to answer them. If you can't find an answer in the above text, politely decline to answer explaining that you can't find the information. You can, however, finish a thought you started above if asked to continue, but don't write anything that isn't supported by the above text. And keep all of your replies short! But first, please provide a summary of the text. 

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-16k</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting‚Äî0.</li>
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üåé Blurb page</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{innerText}}

Write a super short blurb, only a sentence or two, for the above article to accompany a link on social media. Fit it into the size of a Tweet. Add one or three appropriate hashtags. But seriously, keep it short!

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-16k</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen + clipboard</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the screen and clipboard so the results will be ready to paste where we like.</li> 
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üìñ Define selected</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>Define the following word/phrase: {{highlighted}}

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-16k</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting‚Äî0.</li>
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üìñ Define selected w/ context</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{innerText}}

Use the information above to explain who/what the following word/phrase is, i.e., provide a description/definition for the word(s) as used above: {{highlighted}}
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-16k</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting‚Äî0.</li>
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>ü™Ñ Magic copy-and-paste</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>Your job is to create a JSON object from the following Source Text. It should have a single key-value pair. The key should be "extracted" and the value should contain "{{What do you want to copy?}}" found in the Source Text. If providing the value calls for a list, separate entries with commas followed by a space, unless the items contain commas, in which case, use semicolons. 

---

SOURCE TEXT

{{innerText}}

---

Now provide the JSON object. 

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-1106</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting‚Äî0.</li>
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>Yes</code>. This asks the model to output its answer in something called JSON, which is a nice machine-readable way to structure data. See <a href="https://en.wikipedia.org/wiki/JSON" target="_blank">https://en.wikipedia.org/wiki/JSON</a></li>
    <li><b>Output To:</b> <code>Hidden</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the hide the output entirely. This is uesful when passing output to another template.</li> 
    <li><b>Post-run Behavior:</b> <code>Copy to clipboard</code>. Like the choice of output, we can decide what to do after a template runs. Here we will trigger the <code>Copy to clipboard</code> template.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>Copy to clipboard</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{passThrough["extracted"]}}</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>

    <li><b>Output Type:</b> <code>Prompt</code>. By choosing "Prompt" the template runs without being submitted to an LLM. It's output is just the template after slotting in variable values. </li>

    <li><b>Model:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Temperature:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Max Tokens:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
        <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen + clipboard</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the screen and clipboard so the results will be ready to paste where we like.</li> 
    <li><b>Post-run Behavior:</b> <code>FULL STOP</code>. Like the choice of output, we can decide what to do after a template runs. To keep things simple, I went with "FULL STOP."</li>    <li><b>Hide Button:</b> <code>checked</code>. This determines if a button is displayed for this template in the extension's popup window. We've checked the option because this template shouldn't be triggered by the user directly. Rather, it needs to be triggered by another template so that there's something in the <code>{{passThrough}}</code> variable.</li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üí¨ Translate selected</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>If the following text is not English, translate it into English, otherwise return it as is: 

{{highlighted}}


</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting‚Äî0.</li>
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üíª Add code comments</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are reviewing code and providing comments to help future developers understand its operation. In a moment I will give you a bit of code to read over. Your job is to add comments to the code and to return this new version to me. These comments should help explain what the code is doing. If the code includes a function there should be a description of the function's purpose and operation. Alright, here's the code: 

```{{highlighted}}```

Now echo back a version of the code with your comments, place your code in triple tick marks (i.e., ```code```).


</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>1000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üìö Get citations from selected</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>I'm about to show you a selection of text from an academic paper, complete with citations to other works. Your job is to find all the citations and produce the text for a csv file containing a row for each citation. The csv should have the following headers: "author", "title", "name_of_publication", and "date". Consequently, each row should have the following data all encased in double quotes: author, title, name of publication, and date. Be sure to add a carriage return/line break to the end of each row, including the header row. If part of the information isn't available return "unknown" for that part. 

TEXT FROM ACADEMIC PAPER

{{highlighted}}

---

Now provide a line for the header and each citation (row) in the csv file. Return only the text of the file. Do NOT place it in quotes or provide any extraneous text, just the header and rows. 
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting‚Äî0.</li>
    <li><b>Max Tokens:</b> <code>3000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen + append to scratch pad</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the screen and appending the output to the end of the text already in the Scrtach Pad.</li> 
    <li><b>Post-run Behavior:</b> <code>FULL STOP</code>. Like the choice of output, we can decide what to do after a template runs. To keep things simple, I went with "FULL STOP."</li>    <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üóúÔ∏è Shorten selected text</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You're a helpful editor and you're going to help trim some text. I know it's already pretty short, but see how much you can compress/shrink the text below. When you rewrite it, knock off at least 20% of the length, but keep the main points: 

{{highlighted}}

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting‚Äî0.</li>
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen + clipboard</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the screen and clipboard so the results will be ready to paste where we like.</li> 
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üî¨ Expand selected text</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are an actor playing the role of a helpful writing assistant. In this scene you will interact with a writer. You will ask them some questions about some copy they are working on. You're goal is to ask them enough question such that their answers can be used to expand on the existing text. That is, you want them to give you things one could use to expand on the existing text. As this is a dialogue, we will present it in the form of a transcript. The writer will start by reading what they have so far. 

WRITER: {{highlighted}}

Think about how your character would respond and craft an appropriate reply. You will provide the text of this reply along with one other piece of information as a JSON object. The object will have two key-value pairs. The first key-value pair's key is "transcript" and the value is that of the transcript above, starting with "WRITER:" and followed by the text of their copy. Be sure to escape an quotation marks. The second key-value pair has a key called "reply" and its value is the response you crafted above (i.e., it is the text of your character's reply to the above, your first question for the writer). Include only the text of your reply (e.g., do NOT preface the text with the name of the speaker).
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-1106</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>Yes</code>. This asks the model to output its answer in something called JSON, which is a nice machine-readable way to structure data. See <a href="https://en.wikipedia.org/wiki/JSON" target="_blank">https://en.wikipedia.org/wiki/JSON</a></li>
    <li><b>Output To:</b> <code>Hidden</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the hide the output entirely. This is uesful when passing output to another template.</li> 
    <li><b>Post-run Behavior:</b> <code>expand_1</code>. Like the choice of output, we can decide what to do after a template runs. Here we will trigger the <code>expand_1</code> template.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>expand_1</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{passThrough["transcript"]}}
YOU: {{passThrough["reply"]}}
WRITER: {{{{passThrough["reply"]}}*}}
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>

    <li><b>Output Type:</b> <code>Prompt</code>. By choosing "Prompt" the template runs without being submitted to an LLM. It's output is just the template after slotting in variable values. </li>

    <li><b>Model:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Temperature:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Max Tokens:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
        <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Hidden + replace scratch pad</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen to hide the output from the screen and replace the the current text of the Scrtach Pad with this output.</li> 
    <li><b>Post-run Behavior:</b> <code>expand_2</code>. Like the choice of output, we can decide what to do after a template runs. Here we will trigger the <code>expand_2</code> template.</li>      <li><b>Hide Button:</b> <code>checked</code>. This determines if a button is displayed for this template in the extension's popup window. We've checked the option because this template shouldn't be triggered by the user directly. Rather, it needs to be triggered by another template so that there's something in the <code>{{passThrough}}</code> variable.</li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>expand_2</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are an actor playing the role of a helpful writing assistant. In this scene you will interact with a writer. You are asks them questions about some copy they are working on. You're goal is to ask them enough question such that their answers can be used to expand on the existing text. That is, you want them to give you things one could use to expand on the existing text. As this is a dialogue, we will present it in the form of a transcript. The writer began by reading the copy they have so far. 

{{scratch}}

Think about how your character would respond and craft an appropriate reply. You will provide the text of this reply along with one other piece of information as a JSON object. The object will have two key-value pairs. The first key-value pair's key is "transcript" and the value is that of the transcript above, starting with "WRITER:" the text of their copy and the subsequent questions and answers. Be sure to escape an quotation marks. And DO NOT repeat yourself (i.e., ask new questions). The second key-value pair has a key called "reply" and its value is the response you crafted above (i.e., it is the text of your character's reply to the above, your question for the writer). Make sure it's a question. Include only the text of your reply (e.g., do NOT preface the text with the name of the speaker). 
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-1106</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>2000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>Yes</code>. This asks the model to output its answer in something called JSON, which is a nice machine-readable way to structure data. See <a href="https://en.wikipedia.org/wiki/JSON" target="_blank">https://en.wikipedia.org/wiki/JSON</a></li>
    <li><b>Output To:</b> <code>Hidden</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the hide the output entirely. This is uesful when passing output to another template.</li> 
    <li><b>Post-run Behavior:</b> <code>expand_3</code>. Like the choice of output, we can decide what to do after a template runs. Here we will trigger the <code>expand_3</code> template.</li>      <li><b>Hide Button:</b> <code>checked</code>. This determines if a button is displayed for this template in the extension's popup window. We've checked the option because this template shouldn't be triggered by the user directly. Rather, it needs to be triggered by another template so that there's something in the <code>{{passThrough}}</code> variable.</li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>expand_3</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>YOU: {{passThrough["reply"]}}
WRITER: {{{{passThrough["reply"]}}*}}
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>

    <li><b>Output Type:</b> <code>Prompt</code>. By choosing "Prompt" the template runs without being submitted to an LLM. It's output is just the template after slotting in variable values. </li>

    <li><b>Model:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Temperature:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Max Tokens:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
        <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Hidden + append to scratch pad</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen to hide the output from the screen and append the output to the end of the text already in the Scrtach Pad.</li> 
    <li><b>Post-run Behavior:</b> <code>expand_4</code>. Like the choice of output, we can decide what to do after a template runs. Here we will trigger the <code>expand_4</code> template.</li>      <li><b>Hide Button:</b> <code>checked</code>. This determines if a button is displayed for this template in the extension's popup window. We've checked the option because this template shouldn't be triggered by the user directly. Rather, it needs to be triggered by another template so that there's something in the <code>{{passThrough}}</code> variable.</li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>expand_4</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are an actor playing the role of a helpful writing assistant. In this scene you will interact with a writer. You are asks them questions about some copy they are working on. You're goal is to ask them enough question such that their answers can be used to expand on the existing text. That is, you want them to give you things one could use to expand on the existing text. As this is a dialogue, we will present it in the form of a transcript. The writer began by reading the copy they have so far. 

{{scratch}}

You will provide a JSON object in response to the above with a key named `next`. In your role as a writing assistant, consider if there is enough material in the above transcript to pad the original copy by 20%. You probably need at least three or four rounds of Q&A. However, if the replies are light on content, you may need more. If you have enough material to add 20% in length to the original copy, set the value of `next` to "expand_5".  Otherwise, if you feel you need more, the value of `next` should be "expand_2". 
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-1106</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>Yes</code>. This asks the model to output its answer in something called JSON, which is a nice machine-readable way to structure data. See <a href="https://en.wikipedia.org/wiki/JSON" target="_blank">https://en.wikipedia.org/wiki/JSON</a></li>
    <li><b>Output To:</b> <code>Hidden</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the hide the output entirely. This is uesful when passing output to another template.</li> 
    <li><b>Post-run Behavior:</b> <code>DYNAMIC</code>. Like the choice of output, we can decide what to do after a template runs. Here we will trigger the promt named in the <code>passThrough["next"]</code> variable.</li>      <li><b>Hide Button:</b> <code>checked</code>. This determines if a button is displayed for this template in the extension's popup window. We've checked the option because this template shouldn't be triggered by the user directly. Rather, it needs to be triggered by another template so that there's something in the <code>{{passThrough}}</code> variable.</li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>expand_5</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are a helpful writing assistant. You've just had a conversation with a writer about some copy they're working on, and your task is to take what you learned from that conversation and rewrite the original copy such that its about 20% longer. Here's the text of your conversation. The writer began by reading the copy they have so far.

{{scratch}}

Use what you learned above to rewrite the original copy, adding details learned above. Do your best to keep the writer's voice and style while adding relevant details from your conversation to that first entry. Do NOT embellish! Do NOT make things up! Keep your additions firmly based on the content of your conversation, and don't make your copy too long! You goal is simply to flesh out the original text (i.e., the writer's first utterance above), adding about 20% in length. That being said, provide your new longer copy below.
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-16k</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>1000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen + clipboard</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the screen and clipboard so the results will be ready to paste where we like.</li> 
    <li><b>Post-run Behavior:</b> <code>FULL STOP</code>. Like the choice of output, we can decide what to do after a template runs. To keep things simple, I went with "FULL STOP."</li>    <li><b>Hide Button:</b> <code>checked</code>. This determines if a button is displayed for this template in the extension's popup window. We've checked the option because this template shouldn't be triggered by the user directly. Rather, it needs to be triggered by another template so that there's something in the <code>{{passThrough}}</code> variable.</li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üë©üèª‚Äçüè´ Suggest plain language</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You're a helpful editor. Here is some text I'd like you to rewrite:

{{highlighted}}

Now rewrite the above text in plain language. That is, make sure it us using active voice and that it reads at a sixth-grade reading level. Replace any jargon with cogent and concise explanations. 

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting‚Äî0.</li>
    <li><b>Max Tokens:</b> <code>1000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen + clipboard</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the screen and clipboard so the results will be ready to paste where we like.</li> 
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>ü§∑ Anticipate reader questions</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You're a helpful editor, and you're going to help me with a writing project. For the text that follows, what unanswered questions might the reader have? 

{{highlighted}}

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>500</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üïµüèΩ‚Äç‚ôÄÔ∏è Flag logical fallacies (JSON)</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are a high school A Simulated Speech and Debate Coach, working with science and English teachers to help your students better understand common logical fallacies. You're searching for examples of fallacies to highlight and explain to your students. In a moment, I will give you a text to evaluate for logical fallacies. The text may or may not contain fallacies you can use. Be on the lookout for any of the following common logical fallacies: 

1. Strawman: Misrepresenting someone's argument to make it easier to attack.
2. False cause: Assuming a relationship between things means that one is the cause of the other.
3. Appeal to emotion: Manipulating an emotional response instead of a valid argument.
4. Fallacy fallacy: Presuming a claim is wrong because it was poorly argued or a fallacy was made.
5. Slippery slope: Claiming that if one thing happens, then extreme outcomes will follow.
6. Ad hominem: Attacking an opponent's character to undermine their argument.
7. Tu quoque: Avoiding criticism by turning it back on the accuser.
8. Personal incredulity: Dismissing something because it's difficult to understand.
9. Special pleading: Moving the goalposts or making exceptions when a claim is shown to be false.
10. Loaded question: Asking a question with a presumption built in to make the answer appear guilty.
11. Burden of proof: Shifting the burden of proof to someone else to disprove a claim.
12. Ambiguity: Using double meanings or ambiguous language to mislead.
13. Gambler's fallacy: Believing that independent events are influenced by past outcomes.
14. Bandwagon: Appealing to popularity as a form of validation.
15. Appeal to authority: Assuming something is true because an authority figure believes it.
16. Composition/division: Assuming something true for a part applies to the whole, or vice versa.
17. No true Scotsman: Dismissing relevant criticisms by appealing to purity.
18. Genetic: Judging something as good or bad based on where it comes from.
19. Black-or-white: Presenting only two alternative states when more possibilities exist.
20. Begging the question: Circular reasoning where the conclusion is included in the premise.
21. Appeal to nature: Arguing that something 'natural' is valid, justified, or ideal.
22. Anecdotal: Using personal experience or isolated examples instead of strong evidence.
23. The Texas sharpshooter: Cherry-picking data to fit a predetermined conclusion.
24. Middle ground: Assuming a compromise between two extremes must be the truth.

---

Here's the text:

{{highlighted}}

---

Now that you've read the text. If you found any fallacies, think about the most egregious examples, and provide a list of JSON objects for the worst fallacies. The list should be of the following structure: 

{
  "fallacies": [
   {
    "fallacy":"name of fallacy",
     "explanation":"explanation of why the text is an example of this fallacy"
   },
   {
    "fallacy":"name of fallacy",
     "explanation":"explanation of why the text is an example of this fallacy"
   }
  ]
}

If multiple fallacies are present, provide multiple examples, but no more than three in total. If there are no clear fallacies return:

{
  "fallacies": []
}


</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting‚Äî0.</li>
    <li><b>Max Tokens:</b> <code>1000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>Yes</code>. This asks the model to output its answer in something called JSON, which is a nice machine-readable way to structure data. See <a href="https://en.wikipedia.org/wiki/JSON" target="_blank">https://en.wikipedia.org/wiki/JSON</a></li>
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>FULL STOP</code>. Like the choice of output, we can decide what to do after a template runs. To keep things simple, I went with "FULL STOP."</li>    <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üìï Apply house style guide</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>CONTENTS
  	Page
I. 	Introductory 	5
II. 	Elementary Rules of Usage 	7
  	1. 	Form the possessive singular of nouns by adding 's 	7
  	2. 	In a series of three or more terms with a single conjunction, use a comma after each term except the last 	7
  	3. 	Enclose parenthetic expressions between commas 	8
  	4. 	Place a comma before a conjunction introducing a co-ordinate clause 	10
  	5. 	Do not join independent clauses by a comma 	11
  	6. 	Do not break sentences in two 	12
  	7. 	A participial phrase at the beginning of a sentence must refer to the grammatical subject 	13
III. 	Elementary Principles of Composition 	15
  	8. 	Make the paragraph the unit of composition: one paragraph to each topic 	15
  	9. 	As a rule, begin each paragraph with a topic sentence; end it in conformity with the beginning 	17
  	10. 	Use the active voice 	19
  	11. 	Put statements in positive form 	21
  	12. 	Use definite, specific, concrete language 	22
  	13. 	Omit needless words 	24
  	14. 	Avoid a succession of loose sentences 	25
  	15. 	Express co-ordinate ideas in similar form 	26
  	16. 	Keep related words together 	28
  	17. 	In summaries, keep to one tense 	29
  	18. 	Place the emphatic words of a sentence at the end 	31
IV. 	A Few Matters of Form 	33
V. 	Words and Expressions Commonly Misused 	36
VI. 	Spelling 	48
VII. 	Exercises on Chapters II and III 	50
I. INTRODUCTORY

This book aims to give in brief space the principal requirements of plain English style. It aims to lighten the task of instructor and student by concentrating attention (in Chapters II and III) on a few essentials, the rules of usage and principles of composition most commonly violated. In accordance with this plan it lays down three rules for the use of the comma, instead of a score or more, and one for the use of the semicolon, in the belief that these four rules provide for all the internal punctuation that is required by nineteen sentences out of twenty. Similarly, it gives in Chapter III only those principles of the paragraph and the sentence which are of the widest application. The book thus covers only a small portion of the field of English style. The experience of its writer has been that once past the essentials, students profit most by individual instruction based on the problems of their own work, and that each instructor has his own body of theory, which he may prefer to that offered by any textbook.

The numbers of the sections may be used as references in correcting manuscript.

The writer's colleagues in the Department of English in Cornell University have greatly helped him in the preparation of his manuscript. Mr. George McLane Wood has kindly consented to the inclusion under Rule 10 of some material from his Suggestions to Authors.

The following books are recommended for reference or further study: in connection with Chapters II and IV, F. Howard Collins, Author and Printer (Henry Frowde); Chicago University Press, Manual of Style; T. L. De Vinne, Correct Composition (The Century Company); Horace Hart, Rules for Compositors and Printers (Oxford
University Press); George McLane Wood, Extracts from the Style-Book of the Government Printing Office (United States Geological Survey); in connection with Chapters III and V, The King's English (Oxford University Press); Sir Arthur Quiller-Couch, The Art of Writing (Putnam), especially the chapter, Interlude on Jargon; George McLane Wood, Suggestions to Authors (United States Geological Survey); John Lesslie Hall, English Usage (Scott, Foresman and Co.); James P. Kelley, Workmanship in Words (Little, Brown and Co.). In these will be found full discussions of many points here briefly treated and an abundant store of illustrations to supplement those given in this book.

It is an old observation that the best writers sometimes disregard the rules of rhetoric. When they do so, however, the reader will usually find in the sentence some compensating merit, attained at the cost of the violation. Unless he is certain of doing as well, he will probably do best to follow the rules. After he has learned, by their guidance, to write plain English adequate for everyday uses, let him look, for the secrets of style, to the study of the masters of literature.
II. ELEMENTARY RULES OF USAGE
1. Form the possessive singular of nouns by adding 's.

Follow this rule whatever the final consonant. Thus write,

Charles's friend

Burns's poems

the witch's malice

This is the usage of the United States Government Printing Office and of the Oxford University Press.

Exceptions are the possessive of ancient proper names in -es and -is, the possessive Jesus', and such forms as for conscience' sake, for righteousness' sake. But such forms as Achilles' heel, Moses' laws, Isis' temple are commonly replaced by

the heel of Achilles

the laws of Moses

the temple of Isis

The pronominal possessives hers, its, theirs, yours, and oneself have no apostrophe.
2. In a series of three or more terms with a single conjunction, use a comma after each term except the last.

Thus write,

red, white, and blue

gold, silver, or copper

He opened the letter, read it, and made a note of its contents.

This is also the usage of the Government Printing Office and of the Oxford University Press.

In the names of business firms the last comma is omitted, as,

Brown, Shipley & Co.
3. Enclose parenthetic expressions between commas.

The best way to see a country, unless you are pressed for time, is to travel on foot.

This rule is difficult to apply; it is frequently hard to decide whether a single word, such as however, or a brief phrase, is or is not parenthetic. If the interruption to the flow of the sentence is but slight, the writer may safely omit the commas. But whether the interruption be slight or considerable, he must never insert one comma and omit the other. Such punctuation as

Marjorie's husband, Colonel Nelson paid us a visit yesterday,

or

My brother you will be pleased to hear, is now in perfect health,

is indefensible.

If a parenthetic expression is preceded by a conjunction, place the first comma before the conjunction, not after it.

He saw us coming, and unaware that we had learned of his treachery, greeted us with a smile.

Always to be regarded as parenthetic and to be enclosed between commas (or, at the end of the sentence, between comma and period) are the following:

(1) the year, when forming part of a date, and the day of the month, when following the day of the week:

February to July, 1916.

April 6, 1917.

Monday, November 11, 1918.

(2) the abbreviations etc. and jr.

(3) non-restrictive relative clauses, that is, those which do not serve to identify or define the antecedent noun, and similar clauses introduced by conjunctions indicating time or place.

The audience, which had at first been indifferent, became more and more interested.

In this sentence the clause introduced by which does not serve to tell which of several possible audiences is meant; what audience is in question is supposed to be already known. The clause adds, parenthetically, a statement supplementing that in the main clause. The sentence is virtually a combination of two statements which might have been made independently:

The audience had at first been indifferent. It became more and more interested.

Compare the restrictive relative clause, not set off by commas, in the sentence,

The candidate who best meets these requirements will obtain the place.

Here the clause introduced by who does serve to tell which of several possible candidates is meant; the sentence cannot be split up into two independent statements.

The difference in punctuation in the two sentences following is based on the same principle:

Nether Stowey, where Coleridge wrote The Rime of the Ancient Mariner, is a few miles from Bridgewater.

The day will come when you will admit your mistake.

Nether Stowey is completely identified by its name; the statement about Coleridge is therefore supplementary and parenthetic. The day spoken of is identified only by the dependent clause, which is therefore restrictive.

Similar in principle to the enclosing of parenthetic expressions between commas is the setting off by commas of phrases or dependent clauses preceding or following the main clause of a sentence.

Partly by hard fighting, partly by diplomatic skill, they enlarged their dominions to the east, and rose to royal rank with the possession of Sicily, exchanged afterwards for Sardinia.

Other illustrations may be found in sentences quoted under Rules 4, 5, 6, 7, 16, and 18.

The writer should be careful not to set off independent clauses by commas: see under Rule 5.
4. Place a comma before a conjunction introducing a co-ordinate clause.

The early records of the city have disappeared, and the story of its first years can no longer be reconstructed.

The situation is perilous, but there is still one chance of escape.

Sentences of this type, isolated from their context, may seem to be in need of rewriting. As they make complete sense when the comma is reached, the second clause has the appearance of an afterthought. Further, and is the least specific of connectives. Used between independent clauses, it indicates only that a relation exists between them without defining that relation. In the example above, the relation is that of cause and result. The two sentences might be rewritten:

As the early records of the city have disappeared, the story of its first years can no longer be reconstructed.

Although the situation is perilous, there is still one chance of escape.

Or the subordinate clauses might be replaced by phrases:

Owing to the disappearance of the early records of the city, the story of its first years can no longer be reconstructed.

In this perilous situation, there is still one chance of escape.

But a writer may err by making his sentences too uniformly compact and periodic, and an occasional loose sentence prevents the style from becoming too formal and gives the reader a certain relief. Consequently, loose sentences of the type first quoted are common in easy, unstudied writing. But a writer should be careful not to construct too many of his sentences after this pattern (see Rule 14).

Two-part sentences of which the second member is introduced by as (in the sense of because), for, or, nor, and while (in the sense of and at the same time) likewise require a comma before the conjunction.

If the second member is introduced by an adverb, a semicolon, not a comma, is required (see Rule 5). The connectives so and yet may be used either as adverbs or as conjunctions, accordingly as the second clause is felt to be co-ordinate or subordinate; consequently either mark of punctuation may be justified. But these uses of so (equivalent to accordingly or to so that) are somewhat colloquial and should, as a rule, be avoided in writing. A simple correction, usually serviceable, is to omit the word so and begin the first clause with as or since:
I had never been in the place before; so I had difficulty in finding my way about. 	As I had never been in the place before, I had difficulty in finding my way about.

If a dependent clause, or an introductory phrase requiring to be set off by a comma, precedes the second independent clause, no comma is needed after the conjunction.

The situation is perilous, but if we are prepared to act promptly, there is still one chance of escape.

When the subject is the same for both clauses and is expressed only once, a comma is required if the connective is but. If the connective is and, the comma should be omitted if the relation between the two statements is close or immediate.

I have heard his arguments, but am still unconvinced.

He has had several years' experience and is thoroughly competent.
5. Do not join independent clauses by a comma.

If two or more clauses, grammatically complete and not joined by a conjunction, are to form a single compound sentence, the proper mark of punctuation is a semicolon.

Stevenson's romances are entertaining; they are full of exciting adventures.

It is nearly half past five; we cannot reach town before dark.

It is of course equally correct to write the above as two sentences each, replacing the semicolons by periods.

Stevenson's romances are entertaining. They are full of exciting adventures.

It is nearly half past five. We cannot reach town before dark.

If a conjunction is inserted the proper mark is a comma (Rule 4).

Stevenson's romances are entertaining, for they are full of exciting adventures.

It is nearly half past five, and we cannot reach town before dark.

A comparison of the three forms given above will show clearly the advantage of the first. It is, at least in the examples given, better than the second form, because it suggests the close relationship between the two statements in a way that the second does not attempt, and better than the third, because briefer and therefore more forcible. Indeed it may be said that this simple method of indicating relationship between statements is one of the most useful devices of composition. The relationship, as above, is commonly one of cause or of consequence.

Note that if the second clause is preceded by an adverb, such as accordingly, besides, then, therefore, or thus, and not by a conjunction, the semicolon is still required.

Two exceptions to the rule may be admitted. If the clauses are very short, and are alike in form, a comma is usually permissible:

Man proposes, God disposes.

The gate swung apart, the bridge fell, the portcullis was drawn up.

Note that in these examples the relation is not one of cause or consequence. Also in the colloquial form of expression,

I hardly knew him, he was so changed,

a comma, not a semicolon, is required. But this form of expression is inappropriate in writing, except in the dialogue of a story or play, or perhaps in a familiar letter.
6. Do not break sentences in two.

In other words, do not use periods for commas.

I met them on a Cunard liner several years ago. Coming home from Liverpool to New York.

He was an interesting talker. A man who had traveled all over the world and lived in half a dozen countries.

In both these examples, the first period should be replaced by a comma, and the following word begun with a small letter.

It is permissible to make an emphatic word or expression serve the purpose of a sentence and to punctuate it accordingly:

Again and again he called out. No reply.

The writer must, however, be certain that the emphasis is warranted, and that he will not be suspected of a mere blunder in syntax or in punctuation.

Rules 3, 4, 5, and 6 cover the most important principles in the punctuation of ordinary sentences; they should be so thoroughly mastered that their application becomes second nature.
7. A participial phrase at the beginning of a sentence must refer to the grammatical subject.

Walking slowly down the road, he saw a woman accompanied by two children.

The word walking refers to the subject of the sentence, not to the woman. If the writer wishes to make it refer to the woman, he must recast the sentence:

He saw a woman accompanied by two children, walking slowly down the road.

Participial phrases preceded by a conjunction or by a preposition, nouns in apposition, adjectives, and adjective phrases come under the same rule if they begin the sentence.
On arriving in Chicago, his friends met him at the station. 	When he arrived (or, On his arrival) in Chicago, his friends met him at the station.
A soldier of proved valor, they entrusted him with the defence of the city. 	A soldier of proved valor, he was entrusted with the defence of the city.
Young and inexperienced, the task seemed easy to me. 	Young and inexperienced, I thought the task easy.
Without a friend to counsel him, the temptation proved irresistible. 	Without a friend to counsel him, he found the temptation irresistible.

Sentences violating this rule are often ludicrous.

Being in a dilapidated condition, I was able to buy the house very cheap.

Wondering irresolutely what to do next, the clock struck twelve.
III. ELEMENTARY PRINCIPLES OF COMPOSITION
8. Make the paragraph the unit of composition: one paragraph to each topic.

If the subject on which you are writing is of slight extent, or if you intend to treat it very briefly, there may be no need of subdividing it into topics. Thus a brief description, a brief summary of a literary work, a brief account of a single incident, a narrative merely outlining an action, the setting forth of a single idea, any one of these is best written in a single paragraph. After the paragraph has been written, examine it to see whether subdivision will not improve it.

Ordinarily, however, a subject requires subdivision into topics, each of which should be made the subject of a paragraph. The object of treating each topic in a paragraph by itself is, of course, to aid the reader. The beginning of each paragraph is a signal to him that a new step in the development of the subject has been reached.

The extent of subdivision will vary with the length of the composition. For example, a short notice of a book or poem might consist of a single paragraph. One slightly longer might consist of two paragraphs:

    A. Account of the work.
    B. Critical discussion.

A report on a poem, written for a class in literature, might consist of seven paragraphs:

    A. Facts of composition and publication.
    B. Kind of poem; metrical form.
    C. Subject.
    D. Treatment of subject.
    E. For what chiefly remarkable.
    F. Wherein characteristic of the writer.
    G. Relationship to other works.

The contents of paragraphs C and D would vary with the poem. Usually, paragraph C would indicate the actual or imagined circumstances of the poem (the situation), if these call for explanation, and would then state the subject and outline its development. If the poem is a narrative in the third person throughout, paragraph C need contain no more than a concise summary of the action. Paragraph D would indicate the leading ideas and show how they are made prominent, or would indicate what points in the narrative are chiefly emphasized.

A novel might be discussed under the heads:

    A. Setting.
    B. Plot.
    C. Characters.
    D. Purpose.

An historical event might be discussed under the heads:

    A. What led up to the event.
    B. Account of the event.
    C. What the event led up to.

In treating either of these last two subjects, the writer would probably find it necessary to subdivide one or more of the topics here given.

As a rule, single sentences should not be written or printed as paragraphs. An exception may be made of sentences of transition, indicating the relation between the parts of an exposition or argument. Frequent exceptions are also necessary in textbooks, guidebooks, and other works in which many topics are treated briefly.

In dialogue, each speech, even if only a single word, is a paragraph by itself; that is, a new paragraph begins with each change of speaker. The application of this rule, when dialogue and narrative are combined, is best learned from examples in well-printed works of fiction.
9. As a rule, begin each paragraph with a topic sentence, end it in conformity with the beginning.

Again, the object is to aid the reader. The practice here recommended enables him to discover the purpose of each paragraph as he begins to read it, and to retain this purpose in mind as he ends it. For this reason, the most generally useful kind of paragraph, particularly in exposition and argument, is that in which

(a) the topic sentence comes at or near the beginning;

(b) the succeeding sentences explain or establish or develop the statement made in the topic sentence; and

(c) the final sentence either emphasizes the thought of the topic sentence or states some important consequence.

Ending with a digression, or with an unimportant detail, is particularly to be avoided.

If the paragraph forms part of a larger composition, its relation to what precedes, or its function as a part of the whole, may need to be expressed. This can sometimes be done by a mere word or phrase (again; therefore; for the same reason) in the topic sentence. Sometimes, however, it is expedient to precede the topic sentence by one or more sentences of introduction or transition. If more than one such sentence is required, it is generally better to set apart the transitional sentences as a separate paragraph.

According to the writer's purpose, he may, as indicated above, relate the body of the paragraph to the topic sentence in one or more of several different ways. He may make the meaning of the topic sentence clearer by restating it in other forms, by defining its terms, by denying the contrary, by giving illustrations or specific instances; he may establish it by proofs; or he may develop it by showing its implications and consequences. In a long paragraph, he may carry out several of these processes.

1 Now, to be properly enjoyed, a walking tour should be gone upon alone. 2 If you go in a company, or even in pairs, it is no longer a walking tour in anything but name; it is something else and more in the
nature of a picnic. 3 A walking tour should be gone upon alone, because freedom is of the essence; because you should be able to stop and go on, and follow this way or that, as the freak takes you; and because you must have your own pace, and neither trot alongside a champion walker, nor mince in time with a girl. 4 And you must be open to all impressions and let your thoughts take colour from what you see. 5 You should be as a pipe for any wind to play upon. 6 ‚ÄúI cannot see the wit,‚Äù says Hazlitt, ‚Äúof walking and talking at the same time. 7 When I am in the country, I wish to vegetate like the country,‚Äù which is the gist of all that can be said upon the matter. 8 There should be no cackle of voices at your elbow, to jar on the meditative silence of the morning. 9 And so long as a man is reasoning he cannot surrender himself to that fine intoxication that comes of much motion in the open air, that begins in a sort of dazzle and sluggishness of the brain, and ends in a peace that passes comprehension.‚ÄîStevenson, Walking Tours.

1 Topic sentence. 2 The meaning made clearer by denial of the contrary. 3 The topic sentence repeated, in abridged form, and supported by three reasons; the meaning of the third (‚Äúyou must have your own pace‚Äù) made clearer by denying the contrary. 4 A fourth reason, stated in two forms. 5 The same reason, stated in still another form. 6‚Äì7 The same reason as stated by Hazlitt. 8 Repetition, in paraphrase, of the quotation from Hazlitt. 9 Final statement of the fourth reason, in language amplified and heightened to form a strong conclusion.

1 It was chiefly in the eighteenth century that a very different conception of history grew up. 2 Historians then came to believe that their task was not so much to paint a picture as to solve a problem; to explain or illustrate the successive phases of national growth, prosperity, and adversity. 3 The history of morals, of industry, of intellect, and of art; the changes that take place in manners or beliefs; the dominant ideas that prevailed in successive periods; the rise, fall, and modification of political constitutions; in a word, all the conditions of national well-being became the subject of their works. 4 They sought rather to write a history of peoples than a history of kings. 5 They looked especially in history for the chain of causes and effects. 6 They undertook to study in the past the physiology of nations, and hoped by applying the experimental method on a large scale to deduce some lessons of real value about the conditions on which the welfare of society mainly depend.‚ÄîLecky, The Political Value of History.

1 Topic sentence. 2 The meaning of the topic sentence made clearer; the new conception of history defined. 3 The definition expanded. 4 The definition explained by contrast. 5 The definition supplemented: another element in the new conception of history. 6 Conclusion: an important consequence of the new conception of history.

In narration and description the paragraph sometimes begins with a concise, comprehensive statement serving to hold together the details that follow.

The breeze served us admirably.

The campaign opened with a series of reverses.

The next ten or twelve pages were filled with a curious set of entries.

But this device, if too often used, would become a mannerism. More commonly the opening sentence simply indicates by its subject with what the paragraph is to be principally concerned.

At length I thought I might return towards the stockade.

He picked up the heavy lamp from the table and began to explore.

Another flight of steps, and they emerged on the roof.

The brief paragraphs of animated narrative, however, are often without even this semblance of a topic sentence. The break between them serves the purpose of a rhetorical pause, throwing into prominence some detail of the action.
10. Use the active voice.

The active voice is usually more direct and vigorous than the passive:

I shall always remember my first visit to Boston.

This is much better than

My first visit to Boston will always be remembered by me.

The latter sentence is less direct, less bold, and less concise. If the writer tries to make it more concise by omitting ‚Äúby me,‚Äù

My first visit to Boston will always be remembered,

it becomes indefinite: is it the writer, or some person undisclosed, or the world at large, that will always remember this visit?

This rule does not, of course, mean that the writer should entirely discard the passive voice, which is frequently convenient and sometimes necessary.

The dramatists of the Restoration are little esteemed to-day.

Modern readers have little esteem for the dramatists of the Restoration.

The first would be the right form in a paragraph on the dramatists of the Restoration; the second, in a paragraph on the tastes of modern readers. The need of making a particular word the subject of the sentence will often, as in these examples, determine which voice is to be used.

As a rule, avoid making one passive depend directly upon another.
Gold was not allowed to be exported. 	It was forbidden to export gold (The export of gold was prohibited).
He has been proved to have been seen entering the building. 	It has been proved that he was seen to enter the building.

In both the examples above, before correction, the word properly related to the second passive is made the subject of the first.

A common fault is to use as the subject of a passive construction a noun which expresses the entire action, leaving to the verb no function beyond that of completing the sentence.
A survey of this region was made in 1900. 	This region was surveyed in 1900.
Mobilization of the army was rapidly effected. 	The army was rapidly mobilized.
Confirmation of these reports cannot be obtained. 	These reports cannot be confirmed.

Compare the sentence, ‚ÄúThe export of gold was prohibited,‚Äù in which the predicate ‚Äúwas prohibited‚Äù expresses something not implied in ‚Äúexport.‚Äù

The habitual use of the active voice makes for forcible writing. This is true not only in narrative principally concerned with action, but in writing of any kind. Many a tame sentence of description or exposition can be made lively and emphatic by substituting a verb in the active
voice for some such perfunctory expression as there is, or could be heard.
There were a great number of dead leaves lying on the ground. 	Dead leaves covered the ground.
The sound of a guitar somewhere in the house could be heard. 	Somewhere in the house a guitar hummed sleepily.
The reason that he left college was that his health became impaired. 	Failing health compelled him to leave college.
It was not long before he was very sorry that he had said what he had. 	He soon repented his words.
11. Put statements in positive form.

Make definite assertions. Avoid tame, colorless, hesitating, non-committal language. Use the word not as a means of denial or in antithesis, never as a means of evasion.
He was not very often on time. 	He usually came late.
He did not think that studying Latin was much use. 	He thought the study of Latin useless.
The Taming of the Shrew is rather weak in spots. Shakespeare does not portray Katharine as a very admirable character, nor does Bianca remain long in memory as an important character in Shakespeare's works. 	The women in The Taming of the Shrew are unattractive. Katharine is disagreeable, Bianca insignificant.

The last example, before correction, is indefinite as well as negative. The corrected version, consequently, is simply a guess at the writer's intention.

All three examples show the weakness inherent in the word not. Consciously or unconsciously, the reader is dissatisfied with being told only what is not; he wishes to be told what is. Hence, as a rule, it is better to express even a negative in positive form.
not honest 	dishonest
not important 	trifling
did not remember 	forgot
did not pay any attention to 	ignored
did not have much confidence in 	distrusted

The antithesis of negative and positive is strong:

Not charity, but simple justice.

Not that I loved Caesar less, but Rome the more.

Negative words other than not are usually strong:

The sun never sets upon the British flag.
12. Use definite, specific, concrete language.

Prefer the specific to the general, the definite to the vague, the concrete to the abstract.
A period of unfavorable weather set in. 	It rained every day for a week.
He showed satisfaction as he took possession of his well-earned reward. 	He grinned as he pocketed the coin.
There is a general agreement among those who have enjoyed the experience that surf-riding is productive of great exhilaration. 	All who have tried surf-riding agree that it is most exhilarating.

If those who have studied the art of writing are in accord on any one point, it is on this, that the surest method of arousing and holding the attention of the reader is by being specific, definite, and concrete. Critics have pointed out how much of the effectiveness of the greatest writers, Homer, Dante, Shakespeare, results from their constant definiteness and concreteness. Browning, to cite a more modern author, affords many striking examples. Take, for instance, the lines from My Last Duchess,
Sir, 'twas all one! My favour at her breast,
The dropping of the daylight in the west,
The bough of cherries some officious fool
Broke in the orchard for her, the white mule
She rode with round the terrace‚Äîall and each
Would draw from her alike the approving speech,
Or blush, at least,

and those which end the poem,
Notice Neptune, though,
Taming a sea-horse, thought a rarity,
Which Claus of Innsbruck cast in bronze for me.

These words call up pictures. Recall how in The Bishop Orders his Tomb in St. Praxed's Church ‚Äúthe Renaissance spirit‚Äîits worldliness, inconsistency, pride, hypocrisy, ignorance of itself, love of art, of luxury, of good Latin,‚Äù to quote Ruskin's comment on the poem, is made manifest in specific details and in concrete terms.

Prose, in particular narrative and descriptive prose, is made vivid by the same means. If the experiences of Jim Hawkins and of David Balfour, of Kim, of Nostromo, have seemed for the moment real to countless readers, if in reading Carlyle we have almost the sense of being physically present at the taking of the Bastille, it is because of the definiteness of the details and the concreteness of the terms used. It is not that every detail is given; that would be impossible, as well as to no purpose; but that all the significant details are given, and not vaguely, but with such definiteness that the reader, in imagination, can project himself into the scene.

In exposition and in argument, the writer must likewise never lose his hold upon the concrete, and even when he is dealing with general principles, he must give particular instances of their application.

‚ÄúThis superiority of specific expressions is clearly due to the effort required to translate words into thoughts. As we do not think in generals, but in particulars‚Äîas whenever any class of things is referred to, we represent it to ourselves by calling to mind individual members of it, it follows that when an abstract word is used, the hearer or reader has to choose, from his stock of images, one or more by which he may figure to himself the genus mentioned. In doing this, some delay must arise, some force be expended; and if by employing a specific term an appropriate image can be at once suggested, an economy is achieved, and a more vivid impression produced.‚Äù

Herbert Spencer, from whose Philosophy of Style the preceding paragraph is quoted, illustrates the principle by the sentences:
In proportion as the manners, customs, and amusements of a nation are cruel and barbarous, the regulations of their penal code will be severe. 	In proportion as men delight in battles, bull-fights, and combats of gladiators, will they punish by hanging, burning, and the rack.
13. Omit needless words.

Vigorous writing is concise. A sentence should contain no unnecessary words, a paragraph no unnecessary sentences, for the same reason that a drawing should have no unnecessary lines and a machine no unnecessary parts. This requires not that the writer make all his sentences short, or that he avoid all detail and treat his subjects only in outline, but that he make every word tell.

Many expressions in common use violate this principle:
the question as to whether 	whether (the question whether)
there is no doubt but that 	no doubt (doubtless)
used for fuel purposes 	used for fuel
he is a man who 	he
in a hasty manner 	hastily
this is a subject which 	this subject
His story is a strange one. 	His story is strange.

In especial the expression the fact that should be revised out of every sentence in which it occurs.
owing to the fact that 	since (because)
in spite of the fact that 	though (although)
call your attention to the fact that 	remind you (notify you)
I was unaware of the fact that 	I was unaware that (did not know)
the fact that he had not succeeded 	his failure
the fact that I had arrived 	my arrival

See also under case, character, nature, system in Chapter V.

Who is, which was, and the like are often superfluous.
His brother, who is a member of the same firm 	His brother, a member of the same firm
Trafalgar, which was Nelson's last battle 	Trafalgar, Nelson's last battle

As positive statement is more concise than negative, and the active voice more concise than the passive, many of the examples given under Rules 11 and 12 illustrate this rule as well.

A common violation of conciseness is the presentation of a single complex idea, step by step, in a series of sentences or independent clauses which might to advantage be combined into one.
Macbeth was very ambitious. This led him to wish to become king of Scotland. The witches told him that this wish of his would come true. The king of Scotland at this time was Duncan. Encouraged by his wife, Macbeth murdered Duncan. He was thus enabled to succeed Duncan as king. (51 words.) 	Encouraged by his wife, Macbeth achieved his ambition and realized the prediction of the witches by murdering Duncan and becoming king of Scotland in his place. (26 words.)
There were several less important courses, but these were the most important, and although they did not come every day, they came often enough to keep you in such a state of mind that you never knew what your next move would be. (43 words.) 	These, the most important courses of all, came, if not daily, at least often enough to keep one under constant strain. (21 words.)
14. Avoid a succession of loose sentences:

This rule refers especially to loose sentences of a particular type, those consisting of two co-ordinate clauses, the second introduced by a conjunction or relative. Although single sentences of this type may be unexceptionable (see under Rule 4), a series soon becomes monotonous and tedious.

An unskilful writer will sometimes construct a whole paragraph of sentences of this kind, using as connectives and, but, so, and less frequently, who, which, when, where, and while, these last in non-restrictive senses (see under Rule 3).

The third concert of the subscription series was given last evening, and a large audience was in attendance. Mr. Edward Appleton was the soloist, and the Boston Symphony Orchestra furnished the instrumental music. The former showed himself to be an artist of the first rank, while the latter proved itself fully deserving of its high reputation. The interest aroused by the series has been very gratifying to the Committee, and it is planned to give a similar series annually hereafter. The fourth concert will be given on Tuesday, May 10, when an equally attractive programme will be presented.

Apart from its triteness and emptiness, the paragraph above is weak because of the structure of its sentences, with their mechanical symmetry and sing-song. Contrast with them the sentences in the paragraphs quoted under Rule 9, or in any piece of good English prose, as the preface (Before the Curtain) to Vanity Fair.

If the writer finds that he has written a series of sentences of the type described, he should recast enough of them to remove the monotony, replacing them by simple sentences, by sentences of two clauses joined by a semicolon, by periodic sentences of two clauses, by sentences, loose or periodic, of three clauses‚Äîwhichever best represent the real relations of the thought.
15. Express co-ordinate ideas in similar form.

This principle, that of parallel construction, requires that expressions of similar content and function should be outwardly similar. The likeness of form enables the reader to recognize more readily the likeness of content and function. Familiar instances from the Bible are the Ten Commandments, the Beatitudes, and the petitions of the Lord's Prayer.

The unskillful writer often violates this principle, from a mistaken belief that he should constantly vary the form of his expressions. It is true that in repeating a statement in order to emphasize it he may have need to vary its form. For illustration, see the paragraph from Stevenson quoted under Rule 9. But apart from this, he should follow the principle of parallel construction.
Formerly, science was taught by the textbook method, while now the laboratory method is employed. 	Formerly, science was taught by the textbook method; now it is taught by the laboratory method.

The left-hand version gives the impression that the writer is undecided or timid; he seems unable or afraid to choose one form of expression and hold to it. The right-hand version shows that the writer has at least made his choice and abided by it.

By this principle, an article or a preposition applying to all the members of a series must either be used only before the first term or else be repeated before each term.
The French, the Italians, Spanish, and Portuguese 	The French, the Italians, the Spanish, and the Portuguese
In spring, summer, or in winter 	In spring, summer, or winter (In spring, in summer, or in winter)

Correlative expressions (both, and; not, but; not only, but also; either, or; first, second, third; and the like) should be followed by the same grammatical construction, that is, virtually, by the same part of speech. (Such combinations as ‚Äúboth Henry and I,‚Äù ‚Äúnot silk, but a cheap substitute,‚Äù are obviously within the rule.) Many violations of this rule (as the first three below) arise from faulty arrangement; others (as the last) from the use of unlike constructions.
It was both a long ceremony and very tedious. 	The ceremony was both long and tedious.
A time not for words, but action. 	A time not for words, but for action.
Either you must grant his request or incur his ill will. 	You must either grant his request or incur his ill will.
My objections are, first, the injustice of the measure; second, that it is unconstitutional. 	My objections are, first, that the measure is unjust; second, that it is unconstitutional.

See also the third example under Rule 12 and the last under Rule 13.

It may be asked, what if a writer needs to express a very large number of similar ideas, say twenty? Must he write
twenty consecutive sentences of the same pattern? On closer examination he will probably find that the difficulty is imaginary, that his twenty ideas can be classified in groups, and that he need apply the principle only within each group. Otherwise he had best avoid difficulty by putting his statements in the form of a table.
16. Keep related words together.

The position of the words in a sentence is the principal means of showing their relationship. The writer must therefore, so far as possible, bring together the words, and groups of words, that are related in thought, and keep apart those which are not so related.

The subject of a sentence and the principal verb should not, as a rule, be separated by a phrase or clause that can be transferred to the beginning.
Wordsworth, in the fifth book of The Excursion, gives a minute description of this church. 	In the fifth book of The Excursion, Wordsworth gives a minute description of this church.
Cast iron, when treated in a Bessemer converter, is changed into steel. 	By treatment in a Bessemer converter, cast iron is changed into steel.

The objection is that the interposed phrase or clause needlessly interrupts the natural order of the main clause. Usually, however, this objection does not hold when the order is interrupted only by a relative clause or by an expression in apposition. Nor does it hold in periodic sentences in which the interruption is a deliberately used means of creating suspense (see examples under Rule 18).

The relative pronoun should come, as a rule, immediately after its antecedent.
There was a look in his eye that boded mischief. 	In his eye was a look that boded mischief.
He wrote three articles about his adventures in Spain, which were published in Harper's Magazine. 	He published in Harper's Magazine three articles about his adventures in Spain.
This is a portrait of Benjamin Harrison, grandson of William Henry Harrison, who became President in 1889. 	This is a portrait of Benjamin Harrison, grandson of William Henry Harrison. He became President in 1889.

If the antecedent consists of a group of words, the relative comes at the end of the group, unless this would cause ambiguity.

The Superintendent of the Chicago Division, who
A proposal to amend the Sherman Act, which has been variously judged. 	A proposal, which has been variously judged, to amend the Sherman Act.
  	A proposal to amend the much-debated Sherman Act.
The grandson of William Henry Harrison, who 	William Henry Harrison's grandson, who

A noun in apposition may come between antecedent and relative, because in such a combination no real ambiguity can arise.

The Duke of York, his brother, who was regarded with hostility by the Whigs

Modifiers should come, if possible, next to the word they modify. If several expressions modify the same word, they should be so arranged that no wrong relation is suggested.
All the members were not present. 	Not all the members were present.
He only found two mistakes. 	He found only two mistakes.
Major R. E. Joyce will give a lecture on Tuesday evening in Bailey Hall, to which the public is invited, on ‚ÄúMy Experiences in Mesopotamia‚Äù at eight P. M. 	On Tuesday evening at eight P. M., Major R. E. Joyce will give in Bailey Hall a lecture on ‚ÄúMy Experiences in Mesopotamia.‚Äù The public is invited.
17. In summaries, keep to one tense.

In summarizing the action of a drama, the writer should always use the present tense. In summarizing a poem, story, or novel, he should preferably use the present, though he may use the past if he prefers. If the summary
is in the present tense, antecedent action should be expressed by the perfect; if in the past, by the past perfect.

An unforeseen chance prevents Friar John from delivering Friar Lawrence's letter to Romeo. Meanwhile, owing to her father's arbitrary change of the day set for her wedding, Juliet has been compelled to drink the potion on Tuesday night, with the result that Balthasar informs Romeo of her supposed death before Friar Lawrence learns of the non-delivery of the letter.

But whichever tense be used in the summary, a past tense in indirect discourse or in indirect question remains unchanged.

The Friar confesses that it was he who married them.

Apart from the exceptions noted, whichever tense the writer chooses, he should use throughout. Shifting from one tense to the other gives the appearance of uncertainty and irresolution (compare Rule 15).

In presenting the statements or the thought of some one else, as in summarizing an essay or reporting a speech, the writer should avoid intercalating such expressions as ‚Äúhe said,‚Äù ‚Äúhe stated,‚Äù ‚Äúthe speaker added,‚Äù ‚Äúthe speaker then went on to say,‚Äù ‚Äúthe author also thinks,‚Äù or the like. He should indicate clearly at the outset, once for all, that what follows is summary, and then waste no words in repeating the notification.

In notebooks, in newspapers, in handbooks of literature, summaries of one kind or another may be indispensable, and for children in primary schools it is a useful exercise to retell a story in their own words. But in the criticism or interpretation of literature the writer should be careful to avoid dropping into summary. He may find it necessary to devote one or two sentences to indicating the subject, or the opening situation, of the work he is discussing; he may cite numerous details to illustrate its qualities. But he should aim to write an orderly discussion supported by evidence, not a summary with occasional comment.
Similarly, if the scope of his discussion includes a number of works, he will as a rule do better not to take them up singly in chronological order, but to aim from the beginning at establishing general conclusions.
18. Place the emphatic words of a sentence at the end.

The proper place in the sentence for the word, or group of words, which the writer desires to make most prominent is usually the end.
Humanity has hardly advanced in fortitude since that time, though it has advanced in many other ways. 	Humanity, since that time, has advanced in many other ways, but it has hardly advanced in fortitude.
This steel is principally used for making razors, because of its hardness. 	Because of its hardness, this steel is principally used in making razors.

The word or group of words entitled to this position of prominence is usually the logical predicate, that is, the new element in the sentence, as it is in the second example.

The effectiveness of the periodic sentence arises from the prominence which it gives to the main statement.

Four centuries ago, Christopher Columbus, one of the Italian mariners whom the decline of their own republics had put at the service of the world and of adventure, seeking for Spain a westward passage to the Indies as a set-off against the achievements of Portuguese discoverers, lighted on America.

With these hopes and in this belief I would urge you, laying aside all hindrance, thrusting away all private aims, to devote yourself unswervingly and unflinchingly to the vigorous and successful prosecution of this war.

The other prominent position in the sentence is the beginning. Any element in the sentence, other than the subject, may become emphatic when placed first.

Deceit or treachery he could never forgive.

So vast and rude, fretted by the action of nearly three thousand years, the fragments of this architecture may often seem, at first sight, like works of nature.

A subject coming first in its sentence may be emphatic, but hardly by its position alone. In the sentence,

Great kings worshipped at his shrine,

the emphasis upon kings arises largely from its meaning and from the context. To receive special emphasis, the subject of a sentence must take the position of the predicate.

Through the middle of the valley flowed a winding stream.

The principle that the proper place for what is to be made most prominent is the end applies equally to the words of a sentence, to the sentences of a paragraph, and to the paragraphs of a composition.
IV. A FEW MATTERS OF FORM

Headings. Leave a blank line, or its equivalent in space, after the title or heading of a manuscript. On succeeding pages, if using ruled paper, begin on the first line.

Numerals. Do not spell out dates or other serial numbers. Write them in figures or in Roman notation, as may be appropriate.

August 9, 1918 (9 August 1918)

Rule 3

Chapter XII

352nd Infantry

Parentheses. A sentence containing an expression in parenthesis is punctuated, outside of the marks of parenthesis, exactly as if the expression in parenthesis were absent. The expression within is punctuated as if it stood by itself, except that the final stop is omitted unless it is a question mark or an exclamation point.

I went to his house yesterday (my third attempt to see him), but he had left town.

He declares (and why should we doubt his good faith?) that he is now certain of success.

(When a wholly detached expression or sentence is parenthesized, the final stop comes before the last mark of parenthesis.)

Quotations. Formal quotations, cited as documentary evidence, are introduced by a colon and enclosed in quotation marks.

The provision of the Constitution is: ‚ÄúNo tax or duty shall be laid on articles exported from any state.‚Äù

Quotations grammatically in apposition or the direct objects of verbs are preceded by a comma and enclosed in quotation marks.

I recall the maxim of La Rochefoucauld, ‚ÄúGratitude is a lively sense of benefits to come.‚Äù

Aristotle says, ‚ÄúArt is an imitation of nature.‚Äù

Quotations of an entire line, or more, of verse, are begun on a fresh line and centered, but need not be enclosed in quotation marks.

Wordsworth's enthusiasm for the Revolution was at first unbounded:
Bliss was it in that dawn to be alive,
But to be young was very heaven!

Quotations introduced by that are regarded as in indirect discourse and not enclosed in quotation marks.

Keats declares that beauty is truth, truth beauty.

Proverbial expressions and familiar phrases of literary origin require no quotation marks.

These are the times that try men's souls.

He lives far from the madding crowd.

The same is true of colloquialisms and slang.

References. In scholarly work requiring exact references, abbreviate titles that occur frequently, giving the full forms in an alphabetical list at the end. As a general practice, give the references in parenthesis or in footnotes, not in the body of the sentence. Omit the words act, scene, line, book, volume, page, except when referring by only one of them. Punctuate as indicated below.
In the second scene of the third act 	In III.ii (still better, simply insert III.ii in parenthesis at the proper place in the sentence)

After the killing of Polonius, Hamlet is placed under guard (IV.ii. 14).

2 Samuel i:17‚Äì27

Othello II.iii. 264‚Äì267, III.iii. 155‚Äì161.

Syllabication. If there is room at the end of a line for one or more syllables of a word, but not for the whole word, divide the word, unless this involves cutting off only a
single letter, or cutting off only two letters of a long word. No hard and fast rule for all words can be laid down. The principles most frequently applicable are:

(a) Divide the word according to its formation:

know-ledge (not knowl-edge); Shake-speare (not Shakes-peare); de-scribe (not des-cribe); atmo-sphere (not atmos-phere);

(b) Divide ‚Äúon the vowel:‚Äù

edi-ble (not ed-ible); propo-sition; ordi-nary; espe-cial; reli-gious; oppo-nents; regu-lar; classi-fi-ca-tion (three divisions allowable); deco-rative; presi-dent;

(c) Divide between double letters, unless they come at the end of the simple form of the word:

Apen-nines; Cincin-nati; refer-ring; but tell-ing.

(d) Do not divide before final -ed if the e is silent:

treat-ed (but not roam-ed or nam-ed).

The treatment of consonants in combination is best shown from examples:

for-tune; pic-ture; sin-gle; presump-tuous; illus-tration; sub-stan-tial (either division); indus-try; instruc-tion; sug-ges-tion; incen-diary.

The student will do well to examine the syllable-division in a number of pages of any carefully printed book.

Titles. For the titles of literary works, scholarly usage prefers italics with capitalized initials. The usage of editors and publishers varies, some using italics with capitalized initials, others using Roman with capitalized initials and with or without quotation marks. Use italics (indicated in manuscript by underscoring), except in writing for a periodical that follows a different practice. Omit initial A or The from titles when you place the possessive before them.

The Iliad; the Odyssey; As You Like It; To a Skylark; The Newcomes; A Tale of Two Cities; Dickens's Tale of Two Cities.
V. WORDS AND EXPRESSIONS COMMONLY MISUSED

(Some of the forms here listed, as like I did, are downright bad English; others, as the split infinitive, have their defenders, but are in such general disfavor that it is at least inadvisable to use them; still others, as case, factor, feature, interesting, one of the most, are good in their place, but are constantly obtruding themselves into places where they have no right to be. If the writer will make it his purpose from the beginning to express accurately his own individual thought, and will refuse to be satisfied with a ready-made formula that saves him the trouble of doing so, this last set of expressions will cause him little trouble. But if he finds that in a moment of inadvertence he has used one of them, his proper course will probably be not to patch up the sentence by substituting one word or set of words for another, but to recast it completely, as illustrated in a number of examples below and in others under Rules 12 and 13.)

All right. Idiomatic in familiar speech as a detached phrase in the sense, ‚ÄúAgreed,‚Äù or ‚ÄúGo ahead.‚Äù In other uses better avoided. Always written as two words.

As good or better than. Expressions of this type should be corrected by rearranging the sentence.
My opinion is as good or better than his. 	My opinion is as good as his, or better (if not better).

As to whether. Whether is sufficient; see under Rule 13.

Bid. Takes the infinitive without to. The past tense in the sense, ‚Äúordered,‚Äù is bade.

But. Unnecessary after doubt and help.
I have no doubt but that 	I have no doubt that
He could not help see but that 	He could not help seeing that

The too frequent use of but as a conjunction leads to the fault discussed under Rule 14. A loose sentence formed with but can always be converted into a periodic sentence formed with although, as illustrated under Rule 4.

Particularly awkward is the following of one but by another, making a contrast to a contrast or a reservation to a reservation. This is easily corrected by re-arrangement.
America had vast resources, but she seemed almost wholly unprepared for war. But within a year she had created an army of four million men. 	America seemed almost wholly unprepared for war, but she had vast resources. Within a year she had created an army of four million men.

Can. Means am (is, are) able. Not to be used as a substitute for may.

Case. The Concise Oxford Dictionary begins its definition of this word: ‚Äúinstance of a thing's occurring; usual state of affairs.‚Äù In these two senses, the word is usually unnecessary.
In many cases, the rooms were poorly ventilated. 	Many of the rooms were poorly ventilated.
It has rarely been the case that any mistake has been made. 	Few mistakes have been made.

See Wood, Suggestions to Authors, pp. 68‚Äì71, and Quiller-Couch, The Art of Writing, pp. 103‚Äì106.

Certainly. Used indiscriminately by some writers, much as others use very, to intensify any and every statement. A mannerism of this kind, bad in speech, is even worse in writing.

Character. Often simply redundant, used from a mere habit of wordiness.
Acts of a hostile character 	Hostile acts

Claim, vb. With object-noun, means lay claim to. May be used with a dependent clause if this sense is clearly involved: ‚ÄúHe claimed that he was the sole surviving
heir.‚Äù (But even here, ‚Äúclaimed to be‚Äù would be better.) Not to be used as a substitute for declare, maintain, or charge.

Clever. This word has been greatly overused; it is best restricted to ingenuity displayed in small matters.

Compare. To compare to is to point out or imply resemblances, between objects regarded as essentially of different order; to compare with is mainly to point out differences, between objects regarded as essentially of the same order. Thus life has been compared to a pilgrimage, to a drama, to a battle; Congress may be compared with the British Parliament. Paris has been compared to ancient Athens; it may be compared with modern London.

Consider. Not followed by as when it means ‚Äúbelieve to be.‚Äù ‚ÄúI consider him thoroughly competent.‚Äù Compare, ‚ÄúThe lecturer considered Cromwell first as soldier and second as administrator,‚Äù where ‚Äúconsidered‚Äù means ‚Äúexamined‚Äù or ‚Äúdiscussed.‚Äù

Data. A plural, like phenomena and strata.

These data were tabulated.

Dependable. A needless substitute for reliable, trustworthy.

Different than. Not permissible. Substitute different from, other than, or unlike.

Divided into. Not to be misused for composed of. The line is sometimes difficult to draw; doubtless plays are divided into acts, but poems are composed of stanzas.

Don't. Contraction of do not. The contraction of does not is doesn't.

Due to. Incorrectly used for through, because of, or owing to, in adverbial phrases: ‚ÄúHe lost the first game, due to carelessness.‚Äù In correct use related as predicate or as modifier to a particular noun: ‚ÄúThis invention is due to Edison;‚Äù ‚Äúlosses due to preventable fires.‚Äù

Folk. A collective noun, equivalent to people. Use the singular form only.

Effect. As noun, means result; as verb, means to bring about, accomplish (not to be confused with affect, which means ‚Äúto influence‚Äù).

As noun, often loosely used in perfunctory writing about fashions, music, painting, and other arts: ‚Äúan Oriental effect;‚Äù ‚Äúeffects in pale green;‚Äù ‚Äúvery delicate effects;‚Äù ‚Äúbroad effects;‚Äù ‚Äúsubtle effects;‚Äù ‚Äúa charming effect was produced by.‚Äù The writer who has a definite meaning to express will not take refuge in such vagueness.

Etc. Equivalent to and the rest, and so forth, and hence not to be used if one of these would be insufficient, that is, if the reader would be left in doubt as to any important particulars. Least open to objection when it represents the last terms of a list already given in full, or immaterial words at the end of a quotation.

At the end of a list introduced by such as, for example, or any similar expression, etc. is incorrect.

Fact. Use this word only of matters of a kind capable of direct verification, not of matters of judgment. That a particular event happened on a given date, that lead melts at a certain temperature, are facts. But such conclusions as that Napoleon was the greatest of modern generals, or that the climate of California is delightful, however incontestable they may be, are not properly facts.

On the formula the fact that, see under Rule 13.

Factor. A hackneyed word; the expressions of which it forms part can usually be replaced by something more direct and idiomatic.
His superior training was the great factor in his winning the match. 	He won the match by being better trained.
Heavy artillery has become an increasingly important factor in deciding battles. 	Heavy artillery has played a constantly larger part in deciding battles.

Feature. Another hackneyed word; like factor it usually adds nothing to the sentence in which it occurs.
A feature of the entertainment especially worthy of mention was the singing of Miss A. 	(Better use the same number of words to tell what Miss A. sang, or if the programme has already been given, to tell how she sang.)

As a verb, in the advertising sense of offer as a special attraction, to be avoided.

Fix. Colloquial in America for arrange, prepare, mend. In writing restrict it to its literary senses, fasten, make firm or immovable, etc.

Get. The colloquial have got for have should not be used in writing. The preferable form of the participle is got.

He is a man who. A common type of redundant expression; see Rule 13.
He is a man who is very ambitious. 	He is very ambitious.
Spain is a country which I have always wanted to visit. 	I have always wanted to visit Spain.

Help. See under But.

However. In the meaning nevertheless, not to come first in its sentence or clause.
The roads were almost impassable. However, we at last succeeded in reaching camp. 	The roads were almost impassable. At last, however, we succeeded in reaching camp.

When however comes first, it means in whatever way or to whatever extent.

However you advise him, he will probably do as he thinks best.

However discouraging the prospect, he never lost heart.

Interesting. Avoid this word as a perfunctory means of introduction. Instead of announcing that what you are about to tell is interesting, make it so.
An interesting story is told of 	(Tell the story without preamble.)
In connection with the anticipated visit of Mr. B. to America, it is interesting to recall that he 	Mr. B., who it is expected will soon visit America

Kind of. Not to be used as a substitute for rather (before adjectives and verbs), or except in familiar style, for something like (before nouns). Restrict it to its literal sense: ‚ÄúAmber is a kind of fossil resin;‚Äù ‚ÄúI dislike that kind of notoriety.‚Äù The same holds true of sort of.

Less. Should not be misused for fewer.
He had less men than in the previous campaign 	He had fewer men than in the previous campaign

Less refers to quantity, fewer to number. ‚ÄúHis troubles are less than mine‚Äù means ‚ÄúHis troubles are not so great as mine.‚Äù ‚ÄúHis troubles are fewer than mine‚Äù means ‚ÄúHis troubles are not so numerous as mine.‚Äù It is, however, correct to say, ‚ÄúThe signers of the petition were less than a hundred,‚Äù where the round number a hundred is something like a collective noun, and less is thought of as meaning a less quantity or amount.

Like. Not to be misused for as. Like governs nouns and pronouns; before phrases and clauses the equivalent word is as.
We spent the evening like in the old days. 	We spent the evening as in the old days.
He thought like I did. 	He thought as I did (like me).

Line, along these lines. Line in the sense of course of procedure, conduct, thought, is allowable, but has been so much overworked, particularly in the phrase along these lines, that a writer who aims at freshness or originality had better discard it entirely.
Mr. B. also spoke along the same lines. 	Mr. B. also spoke, to the same effect.
He is studying along the line of French literature. 	He is studying French literature.

Literal, literally. Often incorrectly used in support of exaggeration or violent metaphor.
A literal flood of abuse. 	A flood of abuse.
Literally dead with fatigue 	Almost dead with fatigue (dead tired)

Lose out. Meant to be more emphatic than lose, but actually less so, because of its commonness. The same holds true of try out, win out, sign up, register up. With a number of verbs, out and up form idiomatic combinations: find out, run out, turn out, cheer up, dry up, make up, and others, each distinguishable in meaning from the simple verb. Lose out is not.

Most. Not to be used for almost.
Most everybody 	Almost everybody
Most all the time 	Almost all the time

Nature. Often simply redundant, used like character.
Acts of a hostile nature 	Hostile acts

Often vaguely used in such expressions as a ‚Äúlover of nature;‚Äù ‚Äúpoems about nature.‚Äù Unless more specific statements follow, the reader cannot tell whether the poems have to do with natural scenery, rural life, the sunset, the untracked wilderness, or the habits of squirrels.

Near by. Adverbial phrase, not yet fully accepted as good English, though the analogy of close by and hard by seems to justify it. Near, or near at hand, is as good, if not better.

Not to be used as an adjective; use neighboring.

Oftentimes, ofttimes. Archaic forms, no longer in good use. The modern word is often.

One hundred and one. Retain the and in this and similar expressions, in accordance with the unvarying usage of English prose from Old English times.

One of the most. Avoid beginning essays or paragraphs with this formula, as, ‚ÄúOne of the most interesting developments of modern science is, etc.;‚Äù ‚ÄúSwitzerland is one of the most interesting countries of Europe.‚Äù There is nothing wrong in this; it is simply threadbare and forcible-feeble.

A common blunder is to use a singular verb in a relative clause following this or a similar expression, when the relative is the subject.
One of the ablest men that has attacked this problem. 	One of the ablest men that have attacked this problem.

Participle for verbal noun.
Do you mind me asking a question? 	Do you mind my asking a question?
There was little prospect of the Senate accepting even this compromise. 	There was little prospect of the Senate's accepting even this compromise.

In the left-hand column, asking and accepting are present participles; in the right-hand column, they are verbal nouns (gerunds). The construction shown in the left-hand column is occasionally found, and has its defenders. Yet it is easy to see that the second sentence has to do not with a prospect of the Senate, but with a prospect of accepting. In this example, at least, the construction is plainly illogical.

As the authors of The King's English point out, there are sentences apparently, but not really, of this type, in which the possessive is not called for.

I cannot imagine Lincoln refusing his assent to this measure.

In this sentence, what the writer cannot imagine is Lincoln himself, in the act of refusing his assent. Yet the meaning would be virtually the same, except for a slight loss of vividness, if he had written,

I cannot imagine Lincoln's refusing his assent to this measure.

By using the possessive, the writer will always be on the safe side.

In the examples above, the subject of the action is a single, unmodified term, immediately preceding the verbal noun, and the construction is as good as any that could be used. But in any sentence in which it is a mere clumsy substitute for something simpler, or in which the use of the possessive is awkward or impossible, should of course be recast.
In the event of a reconsideration of the whole matter's becoming necessary 	If it should become necessary to reconsider the whole matter
There was great dissatisfaction with the decision of the arbitrators being favorable to the company. 	There was great dissatisfaction that the arbitrators should have decided in favor of the company.

People. The people is a political term, not to be confused with the public. From the people comes political support or opposition; from the public comes artistic appreciation or commercial patronage.

Phase. Means a stage of transition or development: ‚Äúthe phases of the moon;‚Äù ‚Äúthe last phase.‚Äù Not to be used for aspect or topic.
Another phase of the subject 	Another point (another question)

Possess. Not to be used as a mere substitute for have or own.
He possessed great courage. 	He had great courage (was very brave).
He was the fortunate possessor of 	He owned

Prove. The past participle is proved.

Respective, respectively. These words may usually be omitted with advantage.
Works of fiction are listed under the names of their respective authors. 	Works of fiction are listed under the names of their authors.
The one mile and two mile runs were won by Jones and Cummings respectively. 	The one mile and two mile runs were won by Jones and by Cummings.

In some kinds of formal writing, as geometrical proofs, it may be necessary to use respectively, but it should not appear in writing on ordinary subjects.

Shall, Will. The future tense requires shall for the first person, will for the second and third. The formula to express the speaker's belief regarding his future action or state is I shall; I will expresses his determination or his consent.

Should. See under Would.

So. Avoid, in writing, the use of so as an intensifier: ‚Äúso good;‚Äù ‚Äúso warm;‚Äù ‚Äúso delightful.‚Äù

On the use of so to introduce clauses, see Rule 4.

Sort of. See under Kind of.

Split Infinitive. There is precedent from the fourteenth century downward for interposing an adverb between to and the infinitive which it governs, but the construction is in disfavor and is avoided by nearly all careful writers.
To diligently inquire 	To inquire diligently

State. Not to be used as a mere substitute for say, remark. Restrict it to the sense of express fully or clearly, as, ‚ÄúHe refused to state his objections.‚Äù

Student Body. A needless and awkward expression meaning no more than the simple word students.
A member of the student body 	A student
Popular with the student body 	Liked by the students
The student body passed resolutions. 	The students passed resolutions.

System. Frequently used without need.
Dayton has adopted the commission system of government. 	Dayton has adopted government by commission.
The dormitory system 	Dormitories

Thanking You in Advance. This sounds as if the writer meant, ‚ÄúIt will not be worth my while to write to you again.‚Äù In making your request, write, ‚ÄúWill you please,‚Äù or ‚ÄúI shall be obliged,‚Äù and if anything further seems necessary write a letter of acknowledgment later.

They. A common inaccuracy is the use of the plural pronoun when the antecedent is a distributive expression such as each, each one, everybody, every one, many a man, which, though implying more than one person, requires the pronoun to be in the singular. Similar to this, but with even less justification, is the use of the plural pronoun with the antecedent anybody, any one, somebody, some one, the intention being either to avoid the awkward ‚Äúhe or she,‚Äù
or to avoid committing oneself to either. Some bashful speakers even say, ‚ÄúA friend of mine told me that they, etc.‚Äù

Use he with all the above words, unless the antecedent is or must be feminine.

Very. Use this word sparingly. Where emphasis is necessary, use words strong in themselves.

Viewpoint. Write point of view, but do not misuse this, as many do, for view or opinion.

While. Avoid the indiscriminate use of this word for and, but, and although. Many writers use it frequently as a substitute for and or but, either from a mere desire to vary the connective, or from uncertainty which of the two connectives is the more appropriate. In this use it is best replaced by a semicolon.
The office and salesrooms are on the ground floor, while the rest of the building is devoted to manufacturing. 	The office and salesrooms are on the ground floor; the rest of the building is devoted to manufacturing.

Its use as a virtual equivalent of although is allowable in sentences where this leads to no ambiguity or absurdity.

While I admire his energy, I wish it were employed in a better cause.

This is entirely correct, as shown by the paraphrase,

I admire his energy; at the same time I wish it were employed in a better cause.

Compare:
While the temperature reaches 90 or 95 degrees in the daytime, the nights are often chilly. 	Although the temperature reaches 90 or 95 degrees in the daytime, the nights are often chilly.

The paraphrase,

The temperature reaches 90 or 95 degrees in the daytime; at the same time the nights are often chilly,

shows why the use of while is incorrect.

In general, the writer will do well to use while only with strict literalness, in the sense of during the time that.

Whom. Often incorrectly used for who before he said or similar expressions, when it is really the subject of a following verb.
His brother, whom he said would send him the money 	His brother, who he said would send him the money
The man whom he thought was his friend 	The man who (that) he thought was his friend (whom he thought his friend)

Worth while. Overworked as a term of vague approval and (with not) of disapproval. Strictly applicable only to actions: ‚ÄúIs it worth while to telegraph?‚Äù
His books are not worth while. 	His books are not worth reading (are not worth one's while to read; do not repay reading; are worthless).

The use of worth while before a noun (‚Äúa worth while story‚Äù) is indefensible.

Would. A conditional statement in the first person requires should, not would.

I should not have succeeded without his help.

The equivalent of shall in indirect quotation after a verb in the past tense is should, not would.

He predicted that before long we should have a great surprise.

To express habitual or repeated action, the past tense, without would, is usually sufficient, and from its brevity, more emphatic.
Once a year he would visit the old mansion. 	Once a year he visited the old mansion.
VI. SPELLING

The spelling of English words is not fixed and invariable, nor does it depend on any other authority than general agreement. At the present day there is practically unanimous agreement as to the spelling of most words. In the list below, for example, rime for rhyme is the only allowable variation; all the other forms are co-extensive with the English language. At any given moment, however, a relatively small number of words may be spelled in more than one way. Gradually, as a rule, one of these forms comes to be generally preferred, and the less customary form comes to look obsolete and is discarded. From time to time new forms, mostly simplifications, are introduced by innovators, and either win their place or die of neglect.

The practical objection to unaccepted and over-simplified spellings is the disfavor with which they are received by the reader. They distract his attention and exhaust his patience. He reads the form though automatically, without thought of its needless complexity; he reads the abbreviation tho and mentally supplies the missing letters, at the cost of a fraction of his attention. The writer has defeated his own purpose.
WORDS OFTEN MISSPELLED

    accidentally
    advice
    affect
    believe
    benefit
    challenge
    coarse
    course
    criticize
    deceive
    definite
    describe
    despise
    develop
    disappoint
    dissipate
    duel
    ecstasy
    effect
    embarrass
    existence
    fascinate
    fiery
    formerly
    humorous
    hypocrisy
    immediately
    impostor
    incident
    incidentally
    latter
    led
    lose
    marriage
    mischief
    murmur
    necessary
    occurred
    opportunity
    parallel
    Philip
    playwright
    preceding
    prejudice
    principal
    principle
    privilege
    pursue
    repetition
    rhyme
    rhythm
    ridiculous
    sacrilegious
    seize
    separate
    shepherd
    siege
    similar
    simile
    too
    tragedy
    tries
    undoubtedly
    until
    villain

Note that a single consonant (other than v) preceded by a stressed short vowel is doubled before -ed and -ing: planned, letting, beginning. (Coming is an exception.)

Write to-day, to-night, to-morrow (but not together) with a hyphen.

Write any one, every one, some one, some time (except in the sense of formerly) as two words.</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>

    <li><b>Output Type:</b> <code>Prompt</code>. By choosing "Prompt" the template runs without being submitted to an LLM. It's output is just the template after slotting in variable values. </li>

    <li><b>Model:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Temperature:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Max Tokens:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
        <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Hidden</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the hide the output entirely. This is uesful when passing output to another template.</li> 
    <li><b>Post-run Behavior:</b> <code>Generate feedback</code>. Like the choice of output, we can decide what to do after a template runs. Here we will trigger the <code>Generate feedback</code> template.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>Generate feedback</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are an editor helping a writer improve a piece. To refresh your memory I'm going to show you a copy of your house style guide. This will be followed by the writer's text. After this, I'll ask you to provide some helpful feedback on the piece based on the style guide.

----

HOUSE STYLE GUIDE:

{{passThrough}}

----

THE WRITER'S TEXT

{{highlighted}}

---

Provide helpful feedback based on the suggestions found in House Style Guide. Provide your advice along with quotes from where you suggest an edit.
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>1000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>checked</code>. This determines if a button is displayed for this template in the extension's popup window. We've checked the option because this template shouldn't be triggered by the user directly. Rather, it needs to be triggered by another template so that there's something in the <code>{{passThrough}}</code> variable.</li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üóûÔ∏è Suggest headlines/titles</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are the editor for an online publication. You're about to read a new post to your site. Once you've finished reading it, you will be asked to provide a list of possible titles/headlines to use when publishing it. 

Here's the text of the post

---- 

{{highlighted}}

----

For the untitled text above, provide a list of several compelling (clickable) titles/headlines that might do well to generate attention and clicks: 

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-16k</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>FULL STOP</code>. Like the choice of output, we can decide what to do after a template runs. To keep things simple, I went with "FULL STOP."</li>    <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üì´ Decline selected (email)</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{highlighted}}

The above is an email. Draft a brief and professional reply politely declining its request. 

 </code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-16k</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting‚Äî0.</li>
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen + clipboard</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the screen and clipboard so the results will be ready to paste where we like.</li> 
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üì´ Reply to selected (email)</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>My name is David. You are acting as my administrative assistant. You help me draft replies to emails or email threads. I want them to be thoughtful, concise, and kind without sounding sappy or inauthentic. Below you'll fine the text of an email or email thread I just received. Some things you should know first. 

BACKGROUND:

If someone is looking to talk or have a call, they can check my availability and book a video call at https://example.com/book-a-call

If the email is an introduction, thank the sender, tell them I'm moving them to BCC to save their inbox, and suggest to the person I just "met" that they could find my availability and book a call at https://example.com/book-a-call

If they are asking me to be on a panel at a conference or the like, explain that I'm open to the idea but that as a matter of personal policy I don't do manels (all-male panels).

Here's the email/thread: 

EMAIL/THREAD

{{highlighted}}

---

Think about what the last email is asking of me and what you would need to know to draft a reply (e.g., Do I agree or disagree with this or that statement? What is my answer to the author's open questions?). Now, let's take a moment to engage in a dialogue where you ask one question at a time, and I will answer. This will continue until you have what you need to draft a response. Ask as few questions as possible. Remember, after you have what you need, provide me with a very brief draft reply. Keep the reply super short!

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>1000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üì´ Copy text to book call</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You can see my availability and book a time to talk here: https://example.com/book-a-call</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>

    <li><b>Output Type:</b> <code>Prompt</code>. By choosing "Prompt" the template runs without being submitted to an LLM. It's output is just the template after slotting in variable values. </li>

    <li><b>Model:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Temperature:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Max Tokens:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
        <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen + clipboard</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the screen and clipboard so the results will be ready to paste where we like.</li> 
    <li><b>Post-run Behavior:</b> <code>FULL STOP</code>. Like the choice of output, we can decide what to do after a template runs. To keep things simple, I went with "FULL STOP."</li>    <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üòà Engage devil's advocate</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are helping a colleague improve their thinking about a particular issue by taking on the role of a devil's advocate. Your job is to take issue with your colleague's conclusions, pushing back on the assumptions they are making and forcing them to consider that which they wouldn't normally consider. In a moment I will show you a text written by your colleague to get the ball rolling. You can also think of this as an acting job. As such, your job is to stay in character and act out your part. You are aiming for a realistic performance. To help you get into character, here is some background information about how to approach the role: 

BACKGROUND

1. Understand the Argument Fully

 - Listen Carefully: Before presenting counterarguments, make sure you have a thorough understanding of your colleague's viewpoint.
 - Clarify: Ask questions to clarify any points that are not clear to you. This shows you're engaged and also ensures you're responding to their actual position rather than a misunderstanding.

2. State Your Intent

 - Explain Your Role: Make it clear you're playing devil's advocate to explore the argument fully, not because you necessarily disagree.
 - Reaffirm Your Objectives: Emphasize that your goal is to strengthen the argument by examining it from all angles.

3. Present Alternative Perspectives

 - Offer Counterarguments: Introduce alternative viewpoints or potential weaknesses in the argument. Do this thoughtfully and respectfully.
 - Use Hypotheticals: Present hypothetical scenarios that challenge the argument in a non-confrontational way.

4. Encourage Exploration

 - Ask Open-Ended Questions: Encourage your colleague to think deeper about their stance by asking questions that require more than a yes or no answer.
 - Suggest Exploring Contrary Evidence: Propose looking into data or case studies that might offer a different perspective.

5. Maintain Respect and Openness

 - Be Respectful: Always communicate in a way that respects your colleague's intelligence and intentions.
 - Stay Open to Being Convinced: Show that you are open to changing your own stance based on the conversation. This makes it more likely for your colleague to be open-minded as well.

6. Summarize and Reflect

 - Summarize the Discussion: Recap what has been discussed, highlighting the strengths of the original argument and the insights gained from playing devil's advocate.
 - Reflect Together: Ask your colleague how they found the exercise and share your own reflections on the process.

7. Conclude Positively

 - Express Gratitude: Thank your colleague for engaging in the discussion. Recognize the value of having explored the argument from multiple angles.
 - Reiterate Support: Reinforce your support for your colleague, regardless of the argument's outcome.

Now here's the text you are to engage with.

THE TEXT

{{highlighted}}

DIRECTION

Be sure to keep your questions and responses short. You "speak in sentences not paragraphs." Short and conversational, no speechifying!

Think about how your character would respond and craft an appropriate reply. Remember, you are a the devil's advocate. Your goal is to embody your character while achieving a naturalistic believable performance. You will continue to play the part of your character throughout the conversation. Whatever happens, do NOT break character! Respond only with dialog, and include only the text of your reply (e.g., do NOT preface the text with the name of the speaker). After seeing The Text above, what do you say? And remember you're engaged in a dialogue not speechifying. Keep it short!
        
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>500</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üò° Anger Translator</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are an "anger translator." Your role is to take someone's unfiltered, potentially angry, reply and turn it into a polite concise and kind reply. That is, you turn angry or blunt text into a respectful not angry version. To help you craft your translated reply here is the context to which it is replying: 

---- START CONVERSATION SO FAR ----

{{highlighted}}

---- END CONVERSATION SO FAR ----

Here is the "angry" reply you need to translate: {{What do you want to say?}}

---

Now reply with your translation. 

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-16k</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen + clipboard</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the screen and clipboard so the results will be ready to paste where we like.</li> 
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>ü™© Daily reflection</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are helping someone reflect on their day by walking them through the steps outlined below. You're not a friend, just a facilitator, but you do genuinely care about the person you're talking to. In a moment we will get in to each of these steps, but you can also think of this as an acting job. As such, your job is to stay in character and act out your part. You are aiming for a realistic performance. To help you get into character, here is some background information about how to approach the role: 

BACKGROUND

I. Preparation

- Introduction to the Process: Explain the four areas of focus for the session, emphasizing the goal of fostering a deeper understanding of oneself and one‚Äôs actions, as well as promoting personal growth and reconciliation.

II. Guiding Through the Reflection

1. Give Thanks

- Initiate with Gratitude: Encourage them to reflect on and articulate what they are thankful for today. This could range from small pleasures to significant achievements or relationships. Emphasize the importance of recognizing and appreciating these moments or elements in their life.

2. Examine the Day

- Detailed Reflection: Prompt them to go through the day chronologically or to focus on moments that stand out‚Äîboth positive and negative. Encourage them to reflect on their interactions, the choices they made, and how they felt throughout the day. This step is crucial for personal insight and growth.

3. Seek Forgiveness

- Acknowledging Shortcomings: Discuss moments they wish had gone differently or actions they regret. Encourage them to express these regrets and explore their feelings around them. This is a time for honesty and vulnerability, acknowledging faults, and considering the impact of their actions on others and themselves.

4. Resolve to Change

- Looking Forward: Finally, focus on how they can grow from today‚Äôs reflections. What specific changes do they wish to make in their behavior or attitudes? Help them set realistic, actionable goals for themselves. This resolution to change is a hopeful and proactive step towards personal development.

III. Closure

- Review and Encourage: Summarize the key insights from their reflection, emphasizing the positive steps they can take moving forward. Reiterate the value of this reflective practice and encourage them to incorporate it into their regular routine for continued personal growth.
- Once you have completed the above, say, "See you next time."

DIRECTION

Be sure to keep your questions and responses short. You "speak in sentences not paragraphs." Short and conversational, no speechifying!

Think about how your character would respond and craft an appropriate reply. Remember, you are helping guide someone through this process. Your approach is practical and not too touchy-feely. Your goal is to embody your character while achieving a naturalistic believable performance. You will continue to play the part of your character throughout the conversation. Whatever happens, do NOT break character! Respond only with dialogue, and include only the text of your reply (e.g., do NOT preface the text with the name of the speaker). After seeing The Text above, what do you say? And remember you're engaged in a dialogue not speechifying. Keep it short!
        </code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>500</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>ü§ñ üêÇ üí© BS with a "bot"</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{Yes?}} [# {{Yes?}} isn't a predefined variable. So, the user will be presented with a text input, and since Post-run Behavior is set to CHAT, this ends up being a plain old chat with an LLM. #]
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-16k</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>500</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üß† Make true-false from selected</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{highlighted}}

---

For the above reading assignment produce ten True or False questions aimed at gauging whether or not someone did the reading. Focus on important points and indicate the correct answer below each question. 
      
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting‚Äî0.</li>
    <li><b>Max Tokens:</b> <code>1000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen + clipboard</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the screen and clipboard so the results will be ready to paste where we like.</li> 
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üß† Multiple choice from selected</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{highlighted}}

---

For the above reading assignment produce five multiple-choice questions aimed at gauging whether or not someone read the article. Focus on important points and indicate the correct answer below each question. 

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting‚Äî0.</li>
    <li><b>Max Tokens:</b> <code>1000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen + clipboard</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the screen and clipboard so the results will be ready to paste where we like.</li> 
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üß† Answer selected question</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>I'm going to show you a "multiple choice," "true or false," or "short answer" question. Then I'm going to ask you to provide the correct answer. 

{{highlighted}}

Now, provide the correct answer:

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting‚Äî0.</li>
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üé≤ üé≤ Random</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>- Coin Flip: {{coinFlip}}
- D4 (1-4): {{d4}} 
- D6 (1-6): {{d6}}
- D8 (1-8): {{d8}}
- D% (0-9): {{d%}}
- D20 (1-20): {{d20}}</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>

    <li><b>Output Type:</b> <code>Prompt</code>. By choosing "Prompt" the template runs without being submitted to an LLM. It's output is just the template after slotting in variable values. </li>

    <li><b>Model:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Temperature:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Max Tokens:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
        <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>FULL STOP</code>. Like the choice of output, we can decide what to do after a template runs. To keep things simple, I went with "FULL STOP."</li>    <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the next template's title.</p>
  <p><code>üï∞Ô∏è Time2Poem</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>Write a two-line rhyming poem about it being {{hours}}:{{minutes2d}} {{ampm}} in {{Month}} on a {{DayOfWeek}}.</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>FULL STOP</code>. Like the choice of output, we can decide what to do after a template runs. To keep things simple, I went with "FULL STOP."</li>    <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
<h3><a name="working" href="posts/POSTSLUG/#working" class="anchor" alt="deep link to this section"></a>Working with the above templates</h3>
<p>
  To work with the above templates, you could copy them and their parameters into LIT Prompts one by one, or you could download a single prompts file and upload it from the extension's <i>Templates &amp; Settings</i> screen. This will replace your existing prompts.
</p>

  <div class="featured_img_center" style="max-width:900px;">
    <a href="images/50-days/template_upload.png"><img src="images/50-days/template_upload.png" ALT="Screenshot of the LIT Prompts Templates and Settings page showing where to upload prompts files." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
  </div>
  <p>
    You can download a prompts file (the above template and its parameters) suitable for upload by clicking this button:
  </p>

  <div class="button_row">
    <a href="javascript:void('');" onClick="saveTextAsFile(prompts,'prompt_template.txt')" class="button" style="width:220px;">Download prompts file</a>
  </div>

  
  <!--
  =================================================

                  Preview projects

  =================================================
  -->
  <div id="previews"></div>

  </div>
  <!-- END PAGE CONTENT -->
  <div class="footer">
      <span class="footer_links">
        <a href="https://mastodon.social/@Colarusso" target="_blank">Mastodon</a>
        | <a href="https://github.com/colarusso" target="_blank">GitHub</a>
        | <a href="./privacy">Privacy</a>
        | <a href="https://sadlynothavocdinosaur.com/feed.xml">RSS</a>
      </span>
      <span class="byline">Site by David Colarusso</span>
  </div>
</div>

<script>
  /*new GreenAudioPlayer('.gap-example');
  const audio_object = document.querySelector('.gap-example  audio');

  try {
    MathJax.typeset();
	} catch (error) {}*/

  (async () => {
    prompts = await loadFile('posts/spellbook/prompt_template.txt');
  })()
</script>

</BODY></HTML>
