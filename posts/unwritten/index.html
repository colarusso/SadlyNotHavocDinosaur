<!DOCTYPE html>
<HTML><HEAD>

  <!-- Set base for this page equal to domain root -->
  <base href="../../">

  <!-- Page-specific metadata -->
  <title>The Library of Unwritten Books: Create novel novellas on-demand, become a reader-author</title>
  <meta property="og:type" content="website"/>
  <meta property="og:publish_date" content="2024-03-25T00:00:00-0500"/>
  <meta property="og:title" content="The Library of Unwritten Books: Create novel novellas on-demand, become a reader-author"/>
  <meta property="og:description" content="The Library of Unwritten Books may be the coolest thing I've ever made, not the most important, but the coolest. It creates 'novel novellas' on-demand. Unlike text-adventure games with fixed texts, these stories are an open-ended exercise in collaborative storytelling. You are a reader-author. Large language models (LLMs) mediate your collaboration, re-shaping and reflecting your words and those of authors past. "/>
  <meta property="og:image" content="http://www.davidcolarusso.com/images/50-days/library.png"/>
  <meta property="og:image:width" content="1024" />
  <meta property="og:image:height" content="576" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ST9X6H808L"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-ST9X6H808L');
  </script>

  <!-- Metadata for mobile -->
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
  <meta name="apple-mobile-web-app-capable" content="no" />
  <link rel="apple-touch-icon" href="images/comic.png"/>

  <!-- JS & style -->
  <link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
  <link rel="stylesheet" type="text/css" href="css/style.css?v=2024-01-30">
  <script src="js/functions.js?v=2024-01-30"></script>
  <script src="js/spin.js"></script>

  <link rel="stylesheet" href="css/prism.css" data-noprefix="">
  <script type="text/javascript" src="js/prism.js"></script>

  <!--<script id="MathJax-script" async src="js/mathjax/tex-mml-chtml.js"></script>

  <link rel="stylesheet" type="text/css" href="css/green-audio-player.css">
  <script src="js/green-audio-player.js"></script>-->

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="Sadly Not, Havoc Dinosaur" href="https://sadlynothavocdinosaur.com/feed.xml" />

</HEAD>
<BODY BGCOLOR="#ffffff" BACKGROUND="" MARGINWIDTH="0" MARGINHEIGHT="0">

<!-- Message Banner -->
<div id="msg_bar" style="display:none;"></div>

<!-- Title and search -->
<div class="title_bar">
  <div class="home">
    <a href="./" tabindex="1"><img src="images/home.png" class="home_btn"></a>
  </div>
  <div class="search">
    <a href="javascript:show_search();" tabindex="3"><img src="images/search.png" class="search_btn"></a>
    <input id="query" type="text" tabindex="2"/>
  </div>
  <span id="title"><a href="./" class="title_home">Sadly Not, Havoc Dinosaur</a></span>
</div>

<div class="content">
  <!-- START PAGE CONTENT -->

  <div id="page">
  <!--
    =================================================

                      INTRODUCTION

    =================================================
  -->
  <h1 class="post_title_01">The Library of Unwritten Books</h1>
  <div class="post_title_02">Create novel novellas on-demand, become a reader-author</div>
  <div class="featured_img_right">
    <!--<div class="audio_container_container" style="display:show;">
      <div class="audio_container">
        <b>Hear the author read <i>TK</i></b>
        <div class="gap-example player-accessible">
          <audio>
              <source src="mp3s/title.mp3" type="audio/mpeg">
          </audio>
        </div>
        <span class="playback">
          Speed: <a href="javascript:void('')" onClick="set_speed(0.5)" class="playback" id="pb05">0.5x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(1)" class="playback" id="pb10" style="font-weight:900;">1x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(1.5)" class="playback" id="pb15">1.5x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(2)" class="playback" id="pb20">2x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(3)" class="playback" id="pb30">3x</a>
        </span>
      </div>
    </div>-->
    <a href="images/50-days/library.png"><img src="images/50-days/library.png" ALT="Stuttgart public library" class="list_img_file"/></a>
    <div class="caption">
      Stuttgart Public Library by <a href="https://www.flickr.com/photos/75487768@N04/51252814218/in/photostream/" target="_blank" class="captionlnk">barnyz</a>
    </div>
  </div>
  <p class="post_p">
    <a href="https://mastodon.social/@Colarusso" target="_blank" class="body_links"><img src="images/colarusso.jpg" class="headshot_small" alt="Headshot of the author, Colarusso." style="margin-top: 7px;"/></a>
    David Colaursso<br><span class="post_date">Co-director, Suffolk's <a href="https://suffolklitlab.org/" target="_blank" class="captionlnk">Legal Innovation &amp; Tech Lab</a></span>
  </p>
  <p><i>This is <b>the 46th</b> post in my series <a href="posts/50-days-of-lit-prompts">50 Days of LIT Prompts</i></a>.</p>

  <p>
    <a href="https://libraryofunwrittenbooks.org/" target="_blank">The Library of Unwritten Books</a> may be the coolest thing I've ever made, not the most important, but the coolest. It creates "novel novellas" on-demand. Unlike text-adventure games with fixed texts, these stories are an open-ended exercise in collaborative storytelling. You are a reader-author. Large language models (LLMs) mediate your collaboration, re-shaping and reflecting your words and those of authors past. I'm reminded of <a href="https://www.youtube.com/watch?v=2dStjjuqM4M" target="_blank">these words</a> from Carl Sagan.
  </p>

  <blockquote>
    What an astonishing thing a book is. It's a flat object made from a tree with flexible parts on which are imprinted lots of funny dark squiggles. But one glance at it and you're inside the mind of another person, maybe somebody dead for thousands of years. Across the millennia, an author is speaking clearly and silently inside your head, directly to you. Writing is perhaps the greatest of human inventions, binding together people who never knew each other, citizens of distant epochs. Books break the shackles of time. A book is proof that humans are capable of working magic.
  </blockquote>

  <p>
    This Library is a different sort of magic, for instead of transporting its readers into the mind of a single author it places us somewhere in the zeitgeist. LLMs, as we know, are machines for completing sentences. They work by predicting the next plausible string of words. As Ted Chiang observed, they are <a href="https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web" target="_blank">blurry JPEGs of the Web</a>. We harness this fact to produce something novel based on the input of our reader-authors, the "compressed" writings used to train the LLM, and random chance.
  </p>

  <div class="featured_img">
    <a href="images/50-days/itsatrap.gif"><img src="images/50-days/itsatrap.gif" alt="Admiral Ackbar ''It's a trap!'' GIF" class="list_img_file"></a>
    <div class="caption">
      Admiral Ackbar "It's a trap!" GIF
    </div>
  </div>

  <p>
    It's worth noting that some folks have equated the training of these models with theft, but I don't think that's right. What they offer is something much stranger than copies. In a real sense, they are mathematical distillations of a zeitgeist found in their training data. A rebuttal of "scraping is stealing" is beyond the scope of this post. So, I'll point you to the <a href="https://pluralistic.net/2024/03/13/hey-look-over-there/#lets-you-and-he-fight" target="_blank">words of Cory Doctorow</a> who makes clear such a framing is not only ahistorical but also a trap! As for model outputs, that is a <a href="https://www.techdirt.com/2024/01/05/copyright-liability-on-llms-should-mostly-fall-on-the-prompter-not-the-service/" target="_blank">different matter</a>. In the end, I suspect the real answer to the <a href="https://www.newyorker.com/science/annals-of-artificial-intelligence/will-ai-become-the-new-mckinsey" target="_blank">fears sparked by AI</a> isn't copyright. It's antitrust and labor law. Unions. The answer involves unions. You really should read the <a href="https://pluralistic.net/2024/03/13/hey-look-over-there/#lets-you-and-he-fight" target="_blank">Doctorow piece</a>. FWIW, folks are also <a href="https://venturebeat.com/ai/the-first-fairly-trained-ai-large-language-model-is-here/" target="_blank">working on</a> training models entirely on licened or <a href="https://huggingface.co/blog/Pclanglais/common-corpus" target="_blank">public domain</a> works. Now, back to The Library.
  </p>

  <p>
    Remember this: as a reader-author what you read is a reflection of what you write. If you respond passively, providing short replies or taking only the road presented, your journey will stay safe and predictable. If, however, you embrace your role as an author, there is much to explore for you are exploring the shadows cast by the cultural artifacts upon which the model was trained. Be warned, you might not like <a href="https://dl.acm.org/doi/10.1145/3442188.3445922" target="_blank">what you find</a>. Then again, you may discover something beautiful. Afterall, we and our artifacts contain multitudes.
  </p>
  <p>
    If you examine <a href="posts/unwritten/#template" target="_blank">the prompts that power</a> The Library, you'll see they are asked to lean into genre convention. You'll also discover that the action is driven by mechanics similar to that of many popular role playing games. As the reader-author you are asked what actions you want to take. The LLM evaluates how likely you are to succeed in the world of your story (e.g., is this realistic fiction or fantasy). Based on this assessment it assigns a "difficulty" to the task and rolls a virtual dice behind the scenes. If the roll is high enough you succeed, too low you fail. The LLM shares the result in prose, moves the story forward a beat, and asks for you to take the wheel.
  </p>
  <p>
    If you want to understand how it all fits together and make your own library, the templates below lay it all out. My hope is that you will download LIT Prompts and tweak the templates' language to your liking. However, I expect you may first be interested in experiencing this new role of reader-author. So, I invite you to check out your own unwritten book <a href="https://libraryofunwrittenbooks.org/" target="_blank">here</a>. 
  </p>


  <!-- END INTRO -->

  <h3><a name="build" href="posts/unwritten/#build" class="anchor" alt="deep link to this section"></a>Let's build something!</h3>
  <p>
    We'll do our building in the LIT Prompts extension. If you aren't familiar with the LIT Prompts extension, don't worry. We'll walk you through setting things up before we start building. If you have used the LIT Prompts extension before, skip to <a href="posts/unwritten/#template">The Prompt Pattern (Template)</a>.
  </p>
  <h3><a name="upnext" href="posts/unwritten/#upnext" class="anchor" alt="deep link to this section"></a>Up Next</h3>
  <ul>
    <li><a href="posts/unwritten/#setup" onClick="expand_setup();">Setup LIT Prompts</a></li>
    <ul>
      <li><a href="posts/unwritten/#install" onClick="expand_setup();">Install the extension</a></li>
      <li><a href="posts/unwritten/#point" onClick="expand_setup();">Point it at an API</a></li>
    </ul>
    <li><a href="posts/unwritten/#template">The Prompt Pattern (Template)</a></li>
    <li><a href="posts/unwritten/#tires">Kick the Tires</a></li>
    <li><a href="posts/unwritten/#references">TL;DR References</a></li>
  </ul>
  <p>
    <b>Questions or comments?</b> I'm on Mastodon <a href="https://mastodon.social/@Colarusso" target="_blank">@Colarusso@mastodon.social</a>
  </p>
  <!--
    =================================================

                   Setup LIT Prompts

    =================================================
  -->
  <hr>
  <h2><a name="setup" href="posts/unwritten/#setup" onClick="expand_setup();" class="anchor" alt="deep link to this section"></a>Setup LIT Prompts </h2>
  <div id="expand_setup" style="text-align: left;display:none;font-size: small;">
    <a href="javascript:expand_setup();" style="text-decoration: none;">&#9658; Expand</a>
  </div>
  <div id="collapse_setup" style="text-align: left;font-size: small;">
    <a href="javascript:collapse_setup();" style="text-decoration: none;">&#9660; Collapse</a>
  </div>
  <div id="setup_extension">
    <div class="list_vid">
      <iframe class="embed_vid" src="https://www.youtube-nocookie.com/embed/Ql8aXGvLBGU" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
      <div class="caption">
        7 min intro video
      </div>
    </div>
    <p>
      <i><b>LIT Prompts</b></i> is a browser extension built at Suffolk University Law School's <a href="https://suffolklitlab.org/" target="_blank">Legal Innovation and Technology Lab</a> to help folks explore the use of <a href="https://en.wikipedia.org/wiki/Large_language_model" target="_blank">Large Language Models</a> (LLMs) and <a href="https://en.wikipedia.org/wiki/Prompt_engineering" target="_blank">prompt engineering</a>. LLMs are sentence completion machines, and prompts are the text upon which they build. Feed an LLM a prompt, and it will return a plausible-sounding follow-up (e.g., "Four score and seven..." might return "years ago our fathers brought forth..."). LIT Prompts lets users create and save prompt templates based on data from an active browser window (e.g., selected text or the whole text of a webpage) along with text from a user. Below we'll walk through a specific example.
    </p>
    <p>
      To get started, follow <b>the first four minutes</b> of the intro video or the steps outlined below. <i>Note: The video only shows Firefox, but once you've installed the extension, the steps are the same.</i>
    </p>
    <h3><a name="install" href="posts/unwritten/#install" class="anchor" alt="deep link to this section"></a>Install the extension</h3>
    <p>Follow the links for your browser.</p>
    <ul>
      <li>
        <b>Firefox:</b> (1) visit the extension's <a href="https://addons.mozilla.org/en-US/firefox/addon/lit-prompts/" target="_blank">add-ons page</a>; (2) click "Add to Firefox;" and (3) grant permissions.
      </li>
      <li>
        <b>Chrome:</b>  (1) visit the extension's <a href="https://chromewebstore.google.com/detail/lit-prompts/hfeojjmldhebkeknfapoghcohkhffcmp" target="_blank">web store page</a>; (2) click "Add to Chrome;" and (3) review permissions / "Add extension."
      </li>
    </ul>
    <p>
      If you don't have Firefox, you can <a href="https://www.mozilla.org/en-US/firefox/new/" target="_blank">download it here</a>. Would you rather use Chrome? <a href="https://www.google.com/chrome/" target="_blank">Download it here</a>.
    </p>
    <h3><a name="point" href="posts/unwritten/#point" class="anchor" alt="deep link to this section"></a>Point it at an API</h3>
    <p>
      Here we'll walk through how to use an LLM provided by OpenAI, but you don't have to use their offering. If you're interested in alternatives, you can find them <a href="https://github.com/SuffolkLITLab/prompts/tree/main#openai-compatible-api-integration" target="_blank">here</a>. You can even run your LLM locally, avoiding the need to share your prompts with a third-party. If you need an OpenAI account, you can <a href="https://platform.openai.com/signup" target="_blank">create one here</a>. Note: when you create a new OpenAI account you are given a limited amount of free API credits. If you created an account some time ago, however, these may have expired. If your credits have expired, you will need to enter a <a href="https://platform.openai.com/account/billing/overview" target="_blank">billing method</a> before you can use the API. You can check the state of any credits <a href="https://platform.openai.com/usage" target="_blank">here</a>.
    </p>
    <p>
      Login to <a href="https://openai.com/" target="_blank">OpenAI</a>, and navigate to the <a href="https://platform.openai.com/docs/" target="_blank">API documentation</a>.
    </p>
    <div class="featured_img_center">
      <a href="images/50-days/OpenAI_keys.png"><img src="images/50-days/OpenAI_keys.png" ALT="Screenshot of the OpenAI API Keys page showing where to click to create a new key." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
    </div>
    <p>Once you are looking at the API docs, follow the steps outlined in the image above. That is:
    <ol>
      <li>Select "API keys" from the left menu</li>
      <li>Click "+ Create new secret key"</li>
    </ol>
    <hr>
    <p>
      On LIT Prompt's <i>Templates & Settings</i> screen, set your API Base to <code>https://api.openai.com/v1/chat/completions</code> and your API Key equal to the value you got above after clicking "+ Create new secret key".  You get there by clicking the <i>Templates & Settings</i> button in the extension's popup:
    </p>
    <ol>
      <li>open the extension</li>
      <li>click on  <i>Templates & Settings</i></li>
      <li>enter the API Base and Key (under the section <i>OpenAI-Compatible API Integration</i>)</li>
    </ol>
    <div class="featured_img_center">
      <a href="images/50-days/popup.png"><img src="images/50-days/popup.png" ALT="Screenshot of the LIT Prompts extension popup." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
    </div>
    <p>
      Once those two bits of information (the API Base and Key) are in place, you're good to go. Now you can edit, create, and run prompt templates. Just open the LIT Prompts extension, and click one of the options. I suggest, however, that you read through the <i>Templates and Settings</i> screen to get oriented. You might even try out a few of the preloaded prompt templates. This will let you jump right in and get your hands dirty in the next section.
    </p>
    <div class="featured_img_center">
      <a href="images/50-days/credentials.png"><img src="images/50-days/credentials.png" ALT="Screenshot of the LIT Prompts extension popup." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
    </div>
    <p>
      <span style="background:yellow;">If you receive an error when trying to run a template after entering your Base and Key, and you are using OpenAI, make sure to check the state of any credits <a href="https://platform.openai.com/usage" target="_blank">here</a>. If you don't have any credits, you will need a billing method on file.</span>
    </p>
    <p>
      <i>If you found this hard to follow, consider following along with the first four minutes of the video <a href="posts/unwritten/#setup">above</a>. It covers the same content. It focuses on Firefox, but once you've installed the extension, the steps are the same.</i>
    </p>
  </div>

  <!--
    =================================================

                   Write Your Template

    =================================================
  -->
  <hr>
  <h2><a name="template" href="posts/unwritten/#template" class="anchor" alt="deep link to this section"></a>The Prompt Patterns (Templates)</h2>

  <div class="featured_img_right">
    <a href="images/boxquote.png"><img src="images/boxquote.png" ALT="A slide showing the George Box quote: All models are wrong, but some models are useful." class="list_img_file"/></a>
    <div class="caption">
      Maps are models; they don't show everything. That's okay as long as you don't confuse the map for the territory.
    </div>
  </div>

  <p>
    When crafting a LIT Prompts template, we use a mix of plain language and variable placeholders. Specifically, you can use double curly brackets to encase predefined variables. If the text between the brackets matches one of our predefined variable names, that section of text will be replaced with the variable's value. Today we'll be using the <code>{{scratch}}</code>, <code>{{passThrough}}</code>, and <code>{{d20}}</code> varaiables. See the extension's <a href="https://github.com/SuffolkLITLab/prompts#prompt-templates" target="_blank">documentation</a>.
  </p>

  <p>
    If the text within brackets is not the name of a predefined variable, like <code>{{What do you want to do?}}</code>, it will trigger a prompt for your user that echo's the placeholder (e.g., a text bubble containing, "What do you want to do?"). After the user answers, their reply will replace this placeholder. A list of predefined variables can be found in the extension's <a href="https://github.com/SuffolkLITLab/prompts#prompt-templates" target="_blank">documentation</a>. We'll use this behavior to ask you a set of questions before creating your story. 
  </p>
  <p>
    We'll also use JSON mode to format some of the prompt outputs as <a href="https://en.wikipedia.org/wiki/JSON" target="_blank">JSON</a>, and as we know from our <a href="posts/translate-selected">translation template</a>, when the passThrough variable is JSON, you can access top-level keys by calling them like this <code>{{{{passThrough["title"]}}}}</code>
  </p>
  <p>
    If you've been following along, the template behavior should be pretty straight forward. <span style="background:yellow">However, I discovered a bug in the extension while exporting today's templates.</span> Normally, if you ask the same question (e.g., <code>{{What do you want to do?}}</code>) multiple times the extension will only ever ask it once per run. To allow for the possibility that you might want to re-ask the same question when using a string of templates, I introduced the ability to force prompting of user questions by placing an asterisk before the final set of curly brackets (e.g., <code>{{What do you want to do?*}}</code>).
  </p>
  <p>
    I have this working over at <a href="https://libraryofunwrittenbooks.org/" target="_blank">The Library of Unwritten Books</a> and am updating the extension, but Chrome and Firefox can take a week or so before approving updates. Which is to say, if you try to use the templates as shared below you will enter a loop because it will fail to aske you <code>{{What do you want to do?}}</code> more than once. That is it will just reuse your first answer over and over again. You can avoid this behavior by changing the post-run behavior for "append 2" to "FULL STOP" instead of "Role Play 01." Of course, to move forward you will have to run "Pick up where you left off" after each round. When the bug fix is live, I'll this post to make that clear. That being said...
  </p>

  <p>Here's the template's title.</p>
  <p><code>Read-write a new story</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>Produce a JSON object where the key is "genre" and the value is ```{{6. What genre does this story belong to (e.g., sci-fi, realistic fiction, romance)?}}```</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-1106</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting—0.</li>
    <li><b>Max Tokens:</b> <code>500</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>Yes</code>. This asks the model to output its answer in something called JSON, which is a nice machine-readable way to structure data. See <a href="https://en.wikipedia.org/wiki/JSON" target="_blank">https://en.wikipedia.org/wiki/JSON</a></li>
    <li><b>Output To:</b> <code>Hidden</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the hide the output entirely. This is uesful when passing output to another template.</li> 
    <li><b>Post-run Behavior:</b> <code>Frame</code>. Like the choice of output, we can decide what to do after a template runs. Here we will trigger the <code>Frame</code> template.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the template's title.</p>
  <p><code>Frame</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are the game master for a table-top role-playing game. You are playing with one other player. They will inhabit the role of the protagonist, making decisions for how they move through the world. You will craft a story around their decisions, providing texture and playing the parts of all the other characters. First, I'm going to give you some context so you understand the narrative expectations. 

========

Setting and genre: 

- Your story telling should adhere to the following genre expectations: {{6. What genre does this story belong to (e.g., sci-fi, realistic fiction, romance)?}}
- Your story will start at the following place: {{5. Where does your story take place?}}
- Your story is set at the following time: {{4. When does your story take place?}}

========

The Protagonist:

Earlier you asked the other player to fill in a character sheet for the protagonist. Here are their answers. Use them to help you shape the story.

- name: {{3. What is your character's name?}}
- additional notes: {{2. Anything else I should know that's important to your character (e.g., age, gender, sexual orientation, physical appearance, background)? Keep it brief, and it's okay to say, "No."}}

========

Additional Notes:

You also asked them if there was anything else would like you to incorporate into the story. This is what they said: 

{{1. Is there anything else you would like me to incorporate into the story? Special "notes" you have for this story? Please, keep it short, and it's okay to say, "No."}}

========

Story Arc:

As the story unfolds you should help shape it to follow this basic structure: 

- Section 1 (Exposition): The beginning of the story where the characters, setting, and background information are introduced.
- Section 2 (Rising Action): The series of events that build tension and develop the conflict. This part of the story usually includes obstacles, challenges, and complications that the characters must face.
- Section 3 (Climax): The turning point or the highest point of intensity in the story. It is the moment where the conflict reaches its peak, and the outcome is uncertain. The climax often involves a significant decision, confrontation, or revelation.
- Section 4 (Falling Action): The events that occur after the climax, where the tension starts to decrease. Loose ends are tied up, and the story begins to move towards its resolution.
- Section 5 (Resolution): The final part of the story where the conflict is resolved, and the loose ends are tied up. It provides closure and answers any remaining questions. The resolution can be either positive or negative, depending on the outcome of the story. 

========

Style:

Unless the other player said otherwise, your prose should be high quality like that you would find in a published book with plenty of dialogue. You avoid repeating yourself and try for a coherent story. You also favor describing events over exposition. You follow the writing advice, show don't tell. 

========


</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>

    <li><b>Output Type:</b> <code>Prompt</code>. By choosing "Prompt" the template runs without being submitted to an LLM. It's output is just the template after slotting in variable values. </li>

    <li><b>Model:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Temperature:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Max Tokens:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
        <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Hidden + replace scratch pad</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen to hide the output from the screen and replace the the current text of the Scrtach Pad with this output.</li> 
    <li><b>Post-run Behavior:</b> <code>Introduction</code>. Like the choice of output, we can decide what to do after a template runs. Here we will trigger the <code>Introduction</code> template.</li>      <li><b>Hide Button:</b> <code>checked</code>. This determines if a button is displayed for this template in the extension's popup window. We've checked the option because this template shouldn't be triggered by the user directly. Rather, it needs to be triggered by another template so that there's something in the <code>{{passThrough}}</code> variable.</li>  </ul>
  <p>Here's the template's title.</p>
  <p><code>Pick up where you left off</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{scratch}}

========

Above is a story in progress. Provide a short recap of what's happened so far as if catching the protagonist up on their actions. Be sure to provide enough detail so we know where we left the protagonist at the end of the story so far. Do, however, keep it short. Write no more than three paragraphs. 

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-16k</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>1000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>Role Play 01</code>. Like the choice of output, we can decide what to do after a template runs. Here we will trigger the <code>Role Play 01</code> template.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's the template's title.</p>
  <p><code>Introduction</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{scratch}}

Given the above, write the first two or three paragraphs for the story. Lean heavily into genre expectations. The text should move the story forward and set the stage for the protagonist to take action. The prose should read like that of a novel with plenty of depth and some dialogue. When you're done, I'd like you to return your answer as a JSON object with two key-value pairs. The first key is "opening" and its value should be the opening text of the story. The second key is "title" and it contains an evocative and appropriate title for the type of story you want to tell. 

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.9</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Today we're all about being creative. So, I went with a pretty "creative" setting—0.9.</li>
    <li><b>Max Tokens:</b> <code>1000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>Yes</code>. This asks the model to output its answer in something called JSON, which is a nice machine-readable way to structure data. See <a href="https://en.wikipedia.org/wiki/JSON" target="_blank">https://en.wikipedia.org/wiki/JSON</a></li>
    <li><b>Output To:</b> <code>Hidden</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the hide the output entirely. This is uesful when passing output to another template.</li> 
    <li><b>Post-run Behavior:</b> <code>The story begins</code>. Like the choice of output, we can decide what to do after a template runs. Here we will trigger the <code>The story begins</code> template.</li>      <li><b>Hide Button:</b> <code>checked</code>. This determines if a button is displayed for this template in the extension's popup window. We've checked the option because this template shouldn't be triggered by the user directly. Rather, it needs to be triggered by another template so that there's something in the <code>{{passThrough}}</code> variable.</li>  </ul>
  <p>Here's the template's title.</p>
  <p><code>The story begins</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{passThrough["title"]}}

{{passThrough["opening"]}}

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>

    <li><b>Output Type:</b> <code>Prompt</code>. By choosing "Prompt" the template runs without being submitted to an LLM. It's output is just the template after slotting in variable values. </li>

    <li><b>Model:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Temperature:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Max Tokens:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
        <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen + append to scratch pad</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the screen and appending the output to the end of the text already in the Scrtach Pad.</li> 
    <li><b>Post-run Behavior:</b> <code>Role Play 01</code>. Like the choice of output, we can decide what to do after a template runs. Here we will trigger the <code>Role Play 01</code> template.</li>      <li><b>Hide Button:</b> <code>checked</code>. This determines if a button is displayed for this template in the extension's popup window. We've checked the option because this template shouldn't be triggered by the user directly. Rather, it needs to be triggered by another template so that there's something in the <code>{{passThrough}}</code> variable.</li>  </ul>
  <p>Here's the template's title.</p>
  <p><code>Role Play 01</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{scratch}}

========

Remember, you are the game master for a table-top role-playing game. Above is the story so far. You are playing with one other player. They inhabit the role of the protagonist, making decisions for how they move through the world. You craft a story around their decisions, providing texture and playing the parts of all the other characters, speaking and acting for them as needed. You ask the player who is playing the protagonist what they want to do next, and this is their answer:

{{What do you want to do?*}}

========

To determine if they are successful, start by assessing the likelihood of success for the above action given what you know about the story so far and the relevant genre conventions. Also, consider what you know of the character's skills and motivations and how those might effect the outcome. That is, figure out how hard it will be for the protagonist to do what they want to. Label this difficulty with one of the following labels: 

- Easy
- Medium
- Hard

Now we're going to role a 20-sided dice to see if they are successful. Okay, the dice roll was {{d20}}.

If the difficulty was Easy, the roll ({{d20}}) has to be greater than or equal to 0 for them to succeed. 

If the difficulty was Medium, the roll ({{d20}}) has to be greater than or equal to 4 for them to succeed. 

If the difficulty was Hard, the roll ({{d20}}) has to be greater than or equal to 10 for them to succeed. 

Return a selection of prose, 1 to 2 paragraphs, explaining what happened (i.e., describing what the protagonist did and their success or failure). Be sure to include dialogue when appropriate. The prose should flow naturally from the story so far (text above). You should include details over generalities. Instead of explaining that something occurred, describe each of the events and actions that took place. Show don't tell! Format your reply in JSON with the following key-value pairs:

1. key="difficulty" and the value is the label you applied above.
2. key="difficulty_cutoff" and the value is the numeric cutoff for the specified difficulty. 
3. key="roll" and the value is the outcome of the above dice roll.
4. key="success" and the value is 1 if roll is greater than or equal to the difficulty_cutoff. 
5. key="narrative" and the value is the prose you produce describing what happened.

Be sure to escape quotation marks in any dialogue. And again, remember to show with your words, don't tell. Walk through events and actions one by one in great detail. Also, if this event seems too much like what has come before, take it in a different direction. Avoid repeating phrases that already appear in the story.
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-1106</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.8</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be.</li>
    <li><b>Max Tokens:</b> <code>1000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>Yes</code>. This asks the model to output its answer in something called JSON, which is a nice machine-readable way to structure data. See <a href="https://en.wikipedia.org/wiki/JSON" target="_blank">https://en.wikipedia.org/wiki/JSON</a></li>
    <li><b>Output To:</b> <code>Hidden</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the hide the output entirely. This is uesful when passing output to another template.</li> 
    <li><b>Post-run Behavior:</b> <code>move forward</code>. Like the choice of output, we can decide what to do after a template runs. Here we will trigger the <code>move forward</code> template.</li>      <li><b>Hide Button:</b> <code>checked</code>. This determines if a button is displayed for this template in the extension's popup window. We've checked the option because this template shouldn't be triggered by the user directly. Rather, it needs to be triggered by another template so that there's something in the <code>{{passThrough}}</code> variable.</li>  </ul>
  <p>Here's the template's title.</p>
  <p><code>move forward</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You're helping write a short story. In a moment I'll give you the text so far, and I want you to add a beat, probably about two or three paragraphs worth, more if it includes dialogue. The text should move the story forward and set the stage for the protagonist to take action. Here's what you have so far: 

{{scratch}}

{{passThrough["narrative"]}}

---

Now, write the next beat of the story. Be sure to include dialogue when appropriate, and don't try to cram a lot of exposition in, opt instead for setting up an action for the protagonist. Show don't tell. Return your answer as a JSON object with two key-value pairs.

1. "last_beat" where the value is the last beat of the story. That is ```{{passThrough["narrative"]}}```
2. "next_beat" where the the value is the text you would like to add to the story. Pay particular attention to all of the story before the last beat to make sure the next beat (this bit) makes sense. 

"next_beat" should be no more than three paragraphs. It can go two ways: 

(1) the section continues and the beat stops after setting the stage for the protagonist to act; or 

(2) you decide the section should end because it has done its job as described under the Story Arc. For a section to end, it must have run its course as specified by the Story Arc (e.g., the first section should involve Exposition and the last Resolution). If you think we're at the end of a section, "next_beat" should be the beginning of the next section and it should start with "---\n\n," where "\n\n" is a double carriage return. This section break should be followed by an opening paragraph for the new section, setting things up with the section's goal in mind. 

If you're writing a new section (2 above), either change the setting or advance the time to keep things moving. 

If you are continuing the section (1 above), try to move things towards that section's goals as you understand them but do NOT jump over any time, place the action within the same moment as the last beat.

Either way, you should move the story forward adding detail and plot points. Make it count. No vague flowery fluff, and make sure your prose follows naturally from the story so far, as described above. Avoid repeating phrases that already appear in the story.

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-1106</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.8</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be.</li>
    <li><b>Max Tokens:</b> <code>1000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>Yes</code>. This asks the model to output its answer in something called JSON, which is a nice machine-readable way to structure data. See <a href="https://en.wikipedia.org/wiki/JSON" target="_blank">https://en.wikipedia.org/wiki/JSON</a></li>
    <li><b>Output To:</b> <code>Hidden</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the hide the output entirely. This is uesful when passing output to another template.</li> 
    <li><b>Post-run Behavior:</b> <code>append 2</code>. Like the choice of output, we can decide what to do after a template runs. Here we will trigger the <code>append 2</code> template.</li>      <li><b>Hide Button:</b> <code>checked</code>. This determines if a button is displayed for this template in the extension's popup window. We've checked the option because this template shouldn't be triggered by the user directly. Rather, it needs to be triggered by another template so that there's something in the <code>{{passThrough}}</code> variable.</li>  </ul>
  <p>Here's the template's title.</p>
  <p><code>append 2</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{passThrough["last_beat"]}}

{{passThrough["next_beat"]}}

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>

    <li><b>Output Type:</b> <code>Prompt</code>. By choosing "Prompt" the template runs without being submitted to an LLM. It's output is just the template after slotting in variable values. </li>

    <li><b>Model:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Temperature:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Max Tokens:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
        <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen + append to scratch pad</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the screen and appending the output to the end of the text already in the Scrtach Pad.</li> 
    <li><b>Post-run Behavior:</b> <code>Role Play 01</code>. Like the choice of output, we can decide what to do after a template runs. Here we will trigger the <code>Role Play 01</code> template.</li>      <li><b>Hide Button:</b> <code>checked</code>. This determines if a button is displayed for this template in the extension's popup window. We've checked the option because this template shouldn't be triggered by the user directly. Rather, it needs to be triggered by another template so that there's something in the <code>{{passThrough}}</code> variable.</li>  </ul>
  <p>Here's the template's title.</p>
  <p><code>Download current story</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{scratch}}</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>

    <li><b>Output Type:</b> <code>Prompt</code>. By choosing "Prompt" the template runs without being submitted to an LLM. It's output is just the template after slotting in variable values. </li>

    <li><b>Model:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Temperature:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Max Tokens:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
        <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Hidden</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the hide the output entirely. This is uesful when passing output to another template.</li> 
    <li><b>Post-run Behavior:</b> <code>SAVE TO FILE</code>. Like the choice of output, we can decide what to do after a template runs. Here we will save the output to a file. This will trigger your browser's download feature.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
<h3><a name="working" href="posts/unwritten/#working" class="anchor" alt="deep link to this section"></a>Working with the above templates</h3>
<p>
  To work with the above templates, you could copy them and their parameters into LIT Prompts one by one, or you could download a single prompts file and upload it from the extension's <i>Templates &amp; Settings</i> screen. This will replace your existing prompts.
</p>

  <div class="featured_img_center" style="max-width:900px;">
    <a href="images/50-days/template_upload.png"><img src="images/50-days/template_upload.png" ALT="Screenshot of the LIT Prompts Templates and Settings page showing where to upload prompts files." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
  </div>
  <p>
    You can download a prompts file (the above template and its parameters) suitable for upload by clicking this button:
  </p>

  <div class="button_row">
    <a href="javascript:void('');" onClick="saveTextAsFile(prompts,'prompt_template.txt')" class="button" style="width:220px;">Download prompts file</a>
  </div>

  <!--
    =================================================

                    Kick the Tires

    =================================================
  -->
  <hr>
  <h2><a name="tires" href="posts/unwritten/#tires" class="anchor" alt="deep link to this section"></a>Kick the Tires</h2>
  <p>
    It's one thing to read about something and another to put what you've learned into practice. Let's see how this template performs.
  </p>
  <ul>
  
    <li>
      <b>Make it your own</b>. Without editing any of the templates, see what you can do just by providing answers to the 6 questions. One you find arrangements you like, consider hard coding these and building on them.  
    </li>
        
  </ul>
    <!--
    =================================================

                       References

    =================================================
  -->
  <hr>
  <h2><a name="references" href="posts/unwritten/#references" class="anchor" alt="deep link to this section"></a>TL;DR References</h2>
  <p>
    ICYMI, here are blubs for a selection of works I linked to in this post. If you didn't click through above, you might want to give them a look now.   </p>
  <ul> 

    <li>
      <a href="https://www.google.com/books/edition/Cosmos/EIqoiww1r9sC?hl=en&gbpv=0" target="_blank">Cosmos</a> by Carl Sagan. 
      Though I quoted and linked to the PBS mini-serise, I figured here I'd incluse a link to the book, published in 1980 as a companion to mini-series, Cosmos: A Personal Voyage the book consists of 13 chapters, each corresponding to an episode of the television series. It explores various topics such as the history of science and civilization, the nature of the Universe, space exploration, the inner workings of cells and DNA, and the implications of nuclear war. Sagan aimed to explain complex scientific ideas in a way that is accessible to anyone interested in learning. The book became a best-seller, spending 50 weeks on the Publishers Weekly list and 70 weeks on the New York Times Best Seller list. It received the Hugo Award for Best Non-Fiction Book in 1981 and contributed to the increased visibility of science-themed literature. 
      <i>Summary based on a draft from our <a href="posts/summarize-and-question">day one template</a>.</i>
    </li>

    <li>
      <a href="https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web" target="_blank">ChatGPT Is a Blurry JPEG of the Web</a> by Ted Chiang. Writing at the beginning of ChatGPT's rise to prominence, this article discusses the analogy between language models like ChatGPT and lossy compression algorithms. Chiang argues that while models can repackage/compress web information, they lack true understanding. Ultimately, Chiang concludes that starting with a blurry copy is not ideal when creating original content and that the struggling to express thoughts is an essential element of the writing process. 
    </li>

    <li>
      <a href="https://pluralistic.net/2024/03/13/hey-look-over-there/#lets-you-and-he-fight" target="_blank">Bullies want you to think they're on your side</a> by Cory Doctorow. 
      The article discusses how bullies, including Big Tech companies, use a tactic of convincing their victims that they are the only ones who can keep them safe. This tactic involves creating a fortress-like environment where the victim surrenders their agency and becomes trapped under the bully's control. The author gives examples of how this tactic is used by tech companies, such as Apple and Amazon, to gain control over users and exploit their data. The article also explores how media companies are pursuing legal action against AI companies while simultaneously seeking to control and exploit their workers. The author argues that workers can only achieve a fair deal by forming unions to gain bargaining power. The article concludes by highlighting the importance of workers' rights in the face of AI advancements and the potential negative consequences of implementing stricter copyright laws.
      <i>Summary based on a draft from our <a href="posts/summarize-and-question">day one template</a>.</i>
    </li>

    <li>
      <a href="https://www.techdirt.com/2024/01/05/copyright-liability-on-llms-should-mostly-fall-on-the-prompter-not-the-service/" target="_blank">Wherein The Copia Institute Tells The Copyright Office There's No Place For Copyright Law In AI Training </a> by  Cathy Gellis. 
      This article outlines a comment filed by the Copia Institute with the US Copyright Office, arguing that copyright law should not apply to AI training. The comment states that copyright law should not interfere with AI training because it would impede the public's right to consume works. They argue that AI training is an extension of the public's right to use tools, including software tools, to help them consume works. The comment also notes that AI training is not the same as copying or distributing copyrighted works, as it involves the analysis and processing of information rather than the creation of new works. They conclude that copyright law should not have a role in AI training and that AI training should be considered fair use or exempt from copyright altogether. 
    </li>

    <li><a href="https://www.newyorker.com/science/annals-of-artificial-intelligence/will-ai-become-the-new-mckinsey" target="_blank">Will A.I. Become the New McKinsey?</a> by Ted Chiang.
      This article explores the potential risks and consequences of artificial intelligence (A.I.) in relation to capitalism. Chiang suggests that A.I. can be seen as a management-consulting firm, similar to McKinsey & Company, which concentrates wealth and disempowers workers. He argues that A.I. currently assists capital at the expense of labor, and questions whether there is a way for A.I. to assist workers instead of management. Chiang also discusses the need for economic policies to distribute the benefits of technology appropriately, as well as the importance of critical self-examination by those building world-shaking technologies. He concludes by emphasizing the need to question the assumption that more technology is always better and to engage in the hard work of building a better world. <i>Summary based on a draft from our <a href="posts/summarize-and-question">day one template</a>.</i>
    </li>

    <li>
      <a href="https://dl.acm.org/doi/10.1145/3442188.3445922" target="_blank">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜</a> by Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. There's a lot of history behind this paper. It was part of <a href="https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/" target="_blank">a chain of events</a> that forced Timnit Gebru to leave Google where she was the co-lead of their ethical AI team, but more than that, it's one of the foundational papers in AI ethics, not to be confused with the field of "AI safety," which we will discuss later. It discusses several risks associated with large language models, including environmental/financial costs, biased language, lack of cultural nuance, misdirection of research, and potential for misinformation. If you want to engage critically with LLMs, this paper is a must read.
    </li>

  </ul>

  <!--
  =================================================

                  Preview projects

  =================================================
  -->
  <div id="previews"></div>

  </div>
  <!-- END PAGE CONTENT -->
  <div class="footer">
      <span class="footer_links">
        <a href="https://mastodon.social/@Colarusso" target="_blank">Mastodon</a>
        | <a href="https://github.com/colarusso" target="_blank">GitHub</a>
        | <a href="./privacy">Privacy</a>
        | <a href="https://sadlynothavocdinosaur.com/feed.xml">RSS</a>
      </span>
      <span class="byline">Site by David Colarusso</span>
  </div>
</div>

<script>
  /*new GreenAudioPlayer('.gap-example');
  const audio_object = document.querySelector('.gap-example  audio');

  try {
    MathJax.typeset();
	} catch (error) {}*/

  (async () => {
    prompts = await loadFile('posts/unwritten/prompt_template.txt');
  })()
</script>

</BODY></HTML>
