<!DOCTYPE html>
<HTML><HEAD>

  <!-- Set base for this page equal to domain root -->
  <base href="../../">

  <!-- Page-specific metadata -->
  <title>The AI Ouroboros: Can 2024's AI Build 2014's AI?: Using LLMs to suggest features for linear and logistic regression models</title>
  <meta property="og:type" content="website"/>
  <meta property="og:publish_date" content="2024-03-27T00:00:00-0500"/>
  <meta property="og:title" content="The AI Ouroboros: Can 2024's AI Build 2014's AI?: Using LLMs to suggest features for linear and logistic regression models"/>
  <meta property="og:description" content="Today if someone mentions &quot;AI,&quot; it's a safe bet they're talking about large language models (LLMs). In 2014, however, the moniker most often referenced machine learning (ML). Below we will use today's AI (LLMs) to help make some good old-fashioned AI (ML). It's long been a joke to call statistical methods like linear and logistic regression machine learning/&quot;AI.&quot; It's funny, however, because it's true. For nearly a decade, I've used regressions as a way to introduce AI to my students. Consequently, I'm always on the lookout for better ways to teach the subject. Recently, I made a website where students can create and train their own regression models—My Toy Models—and today we'll get an LLM to build a model we can upload there for training. "/>
  <meta property="og:image" content="http://www.davidcolarusso.com/images/50-days/ymxb.png"/>
  <meta property="og:image:width" content="1024" />
  <meta property="og:image:height" content="677" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ST9X6H808L"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-ST9X6H808L');
  </script>

  <!-- Metadata for mobile -->
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
  <meta name="apple-mobile-web-app-capable" content="no" />
  <link rel="apple-touch-icon" href="images/comic.png"/>

  <!-- JS & style -->
  <link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
  <link rel="stylesheet" type="text/css" href="css/style.css?v=2024-01-30">
  <script src="js/functions.js?v=2024-01-30"></script>
  <script src="js/spin.js"></script>

  <link rel="stylesheet" href="css/prism.css" data-noprefix="">
  <script type="text/javascript" src="js/prism.js"></script>

  <!--<script id="MathJax-script" async src="js/mathjax/tex-mml-chtml.js"></script>

  <link rel="stylesheet" type="text/css" href="css/green-audio-player.css">
  <script src="js/green-audio-player.js"></script>-->

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="Sadly Not, Havoc Dinosaur" href="https://sadlynothavocdinosaur.com/feed.xml" />

</HEAD>
<BODY BGCOLOR="#ffffff" BACKGROUND="" MARGINWIDTH="0" MARGINHEIGHT="0">

<!-- Message Banner -->
<div id="msg_bar" style="display:none;"></div>

<!-- Title and search -->
<div class="title_bar">
  <div class="home">
    <a href="./" tabindex="1"><img src="images/home.png" class="home_btn"></a>
  </div>
  <div class="search">
    <a href="javascript:show_search();" tabindex="3"><img src="images/search.png" class="search_btn"></a>
    <input id="query" type="text" tabindex="2"/>
  </div>
  <span id="title"><a href="./" class="title_home">Sadly Not, Havoc Dinosaur</a></span>
</div>

<div class="content">
  <!-- START PAGE CONTENT -->

  <div id="page">
  <!--
    =================================================

                      INTRODUCTION

    =================================================
  -->
  <h1 class="post_title_01">The AI Ouroboros: Can 2024's AI Build 2014's AI?</h1>
  <div class="post_title_02">Using LLMs to suggest features for linear and logistic regression models</div>
  <div class="featured_img_right">
    <!--<div class="audio_container_container" style="display:show;">
      <div class="audio_container">
        <b>Hear the author read <i>TK</i></b>
        <div class="gap-example player-accessible">
          <audio>
              <source src="mp3s/title.mp3" type="audio/mpeg">
          </audio>
        </div>
        <span class="playback">
          Speed: <a href="javascript:void('')" onClick="set_speed(0.5)" class="playback" id="pb05">0.5x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(1)" class="playback" id="pb10" style="font-weight:900;">1x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(1.5)" class="playback" id="pb15">1.5x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(2)" class="playback" id="pb20">2x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(3)" class="playback" id="pb30">3x</a>
        </span>
      </div>
    </div>-->
    <a href="images/50-days/ymxb.png"><img src="images/50-days/ymxb.png" ALT="Tweet from Amy Hoy, “by today’s definition , y=mx + b is an artificial intelligence bot that can tell you where a line is going.” 29 Mar 2017" class="list_img_file"/></a>
  </div>
  <p class="post_p">
    <a href="https://mastodon.social/@Colarusso" target="_blank" class="body_links"><img src="images/colarusso.jpg" class="headshot_small" alt="Headshot of the author, Colarusso." style="margin-top: 7px;"/></a>
    David Colaursso<br><span class="post_date">Co-director, Suffolk's <a href="https://suffolklitlab.org/" target="_blank" class="captionlnk">Legal Innovation &amp; Tech Lab</a></span>
  </p>
  <p><i>This is <b>the 48th</b> post in my series <a href="posts/50-days-of-lit-prompts">50 Days of LIT Prompts</i></a>.</p>

  <p>
    Today if someone mentions "AI," it's a safe bet they're talking about <a href="https://en.wikipedia.org/wiki/Large_language_model" target="_blank">large language models</a> (LLMs). In 2014, however, the moniker most often referenced <a href="https://en.wikipedia.org/wiki/Machine_learning" target="_blank">machine learning</a> (ML). Below we will use today's AI (LLMs) to help make some good old-fashioned AI (ML). It's long been a joke to call statistical methods like linear and logistic regression machine learning/"AI." It's funny, however, because it's true. For nearly a decade, I've used regressions as a way to introduce AI to my students. Consequently, I'm always on the lookout for better ways to teach the subject. Recently, I made a website where students can create and train their own regression models—<a href="https://mytoymodels.org/" target="_blank">My Toy Models</a>—and today we'll get an LLM to build a model we can upload there for training. But first, let me tell you about a paper. 
  <p>
    <a href="https://www.cmu.edu/dietrich/sds/docs/dawes/the-robust-beauty-of-improper-linear-models-in-decision-making.pdf" target="_blank">The Robust Beauty of Improper Linear Models in Decision Making</a> lives rent free in my head. I think about this paper from 1979 ALL. THE. TIME! 
  </p>
  <p>
    TL;DR: Experts can make robust linear models just by picking a few salient features based on their experience. From the paper:
  </p>
  <blockquote>
    Proper linear models are those in which predictor variables are given weights in such a way that the resulting linear composite optimally predicts some criterion of interest; examples of proper linear models are standard regression analysis... proper linear models outperform clinical intuition. Improper linear models are those in which the weights of the predictor variables are obtained by some nonoptimal method; for example, they may be obtained on the basis of intuition, derived from simulating a clinical judge's predictions, or set to be equal. <b>This article presents evidence that even such improper linear models are superior to clinical intuition when predicting a numerical criterion from numerical predictors.</b> In fact, unit (i.e., equal) weighting is quite robust for making such predictions.
  </blockquote>
  <p>
    In today's parlance the TL;DR would read "feature selection is really important." You want to choose features that actually have some causal effect on the thing you're trying to predict, and clinical intuition is pretty good at this. Additionally, mathematical models tend to be a lot less noisy than clinical intuition when used on individual cases. Even if they're not perfect, their predictions bounce around a lot less. So, I got to thinking, if simply choosing the right features can get you "most of the way" to a good model, maybe LLMs could help introduce the feature selection process for those learning how to build their own models. 
  </p>
  <p>
    Getting students to think about potential features (inputs for their models) is a good place to start when building a model. The trick is to take what you know about some process and identify things you can measure that help to determine the thing you're trying to predict. 
  </p>
  <p>
    Today's template prompts the user for a question the model should answer (what they want to predict) and returns a list of features. For example, when fed "Will there be a snow day tomorrow (i.e., will they cancel school)?" it suggested the following features: predicted snowfall, temperature, and wind speed. That's not bad. When asked, "Will I get a good night's sleep?" it suggested tracking my bedtime routine (i.e., reading, meditation, watching TV, listening to music, none), exercise time, and caffeine consumption. Again, not a bad first pass. 
  </p>
  <p>
    Here's the actual template output, structured to be read by <a href="https://mytoymodels.org/" target="_blank">My Toy Models</a>.
  </p>

  <section class="line-numbers">
    <pre class="language-js" style="white-space:pre-wrap;"><code>{
  "question": "Will I get a good night's sleep?",
  "type": "categorical",
  "target": [
    "yes",
    "no"
  ],
  "features": [
    {
      "name": "bedtime routine",
      "type": "categorical",
      "categories": [
        "reading",
        "meditation",
        "watching TV",
        "listening to music",
        "none"
      ]
    },
    {
      "name": "exercise time",
      "type": "continuous",
      "units": "minutes",
      "lower": 0,
      "upper": 120,
      "mean": 30
    },
    {
      "name": "caffeine consumption",
      "type": "continuous",
      "units": "mg",
      "lower": 0,
      "upper": 500,
      "mean": 50
    }
  ],
  "training": {
    "headers": [
      "recorded_on",
      "bedtime_routine",
      "exercise_time",
      "caffeine_consumption",
      "target",
      "note"
    ],
    "observations": []
  },
  "trained_on": 0,
  "coefficients": {
    "bedtime_routine": 0,
    "exercise_time": 0,
    "caffeine_consumption": 0,
    "intercept": 0
  },
  "performance": {}
}</code></pre>
  </section>

  <p>
    Not only is this a good introduction for students to feature selection, but it also helps introduce the data structure used by <a href="https://mytoymodels.org/" target="_blank">My Toy Models</a>. I'm not convinced that the template produces the perfect feature set, but it's certainly good enough to get the idea across. So, in answer to the question, "Can 2024's AI Build 2014's AI?" the answer seems to be "in part." Of course, if we want to move beyond improper models to their proper counterparts we'll need to collect data and train the models. Luckily, that's what <a href="https://mytoymodels.org/" target="_blank">My Toy Models</a> is for. 
  </p>
  <p>
    If you'd like to take this tool for a spin and you haven't installed the LIT Prompts extension, you can access a standalone version of the tool <a href="ai/toy-models/" target="_blank">here</a>. 
  </p>

  <!-- END INTRO -->

  <h3><a name="build" href="posts/data-science/#build" class="anchor" alt="deep link to this section"></a>Let's build something!</h3>
  <p>
    We'll do our building in the LIT Prompts extension. If you aren't familiar with the LIT Prompts extension, don't worry. We'll walk you through setting things up before we start building. If you have used the LIT Prompts extension before, skip to <a href="posts/data-science/#template">The Prompt Pattern (Template)</a>.
  </p>
  <h3><a name="upnext" href="posts/data-science/#upnext" class="anchor" alt="deep link to this section"></a>Up Next</h3>
  <ul>
    <li><a href="posts/data-science/#setup" onClick="expand_setup();">Setup LIT Prompts</a></li>
    <ul>
      <li><a href="posts/data-science/#install" onClick="expand_setup();">Install the extension</a></li>
      <li><a href="posts/data-science/#point" onClick="expand_setup();">Point it at an API</a></li>
    </ul>
    <li><a href="posts/data-science/#template">The Prompt Pattern (Template)</a></li>
    <li><a href="posts/data-science/#tires">Kick the Tires</a></li>
    <li><a href="posts/data-science/#export">Export and Share </a></li>
    <li><a href="posts/data-science/#references">TL;DR References</a></li>
  </ul>
  <p>
    <b>Questions or comments?</b> I'm on Mastodon <a href="https://mastodon.social/@Colarusso" target="_blank">@Colarusso@mastodon.social</a>
  </p>
  <!--
    =================================================

                   Setup LIT Prompts

    =================================================
  -->
  <hr>
  <h2><a name="setup" href="posts/data-science/#setup" onClick="expand_setup();" class="anchor" alt="deep link to this section"></a>Setup LIT Prompts </h2>
  <div id="expand_setup" style="text-align: left;display:none;font-size: small;">
    <a href="javascript:expand_setup();" style="text-decoration: none;">&#9658; Expand</a>
  </div>
  <div id="collapse_setup" style="text-align: left;font-size: small;">
    <a href="javascript:collapse_setup();" style="text-decoration: none;">&#9660; Collapse</a>
  </div>
  <div id="setup_extension">
    <div class="list_vid">
      <iframe class="embed_vid" src="https://www.youtube-nocookie.com/embed/Ql8aXGvLBGU" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
      <div class="caption">
        7 min intro video
      </div>
    </div>
    <p>
      <i><b>LIT Prompts</b></i> is a browser extension built at Suffolk University Law School's <a href="https://suffolklitlab.org/" target="_blank">Legal Innovation and Technology Lab</a> to help folks explore the use of <a href="https://en.wikipedia.org/wiki/Large_language_model" target="_blank">Large Language Models</a> (LLMs) and <a href="https://en.wikipedia.org/wiki/Prompt_engineering" target="_blank">prompt engineering</a>. LLMs are sentence completion machines, and prompts are the text upon which they build. Feed an LLM a prompt, and it will return a plausible-sounding follow-up (e.g., "Four score and seven..." might return "years ago our fathers brought forth..."). LIT Prompts lets users create and save prompt templates based on data from an active browser window (e.g., selected text or the whole text of a webpage) along with text from a user. Below we'll walk through a specific example.
    </p>
    <p>
      To get started, follow <b>the first four minutes</b> of the intro video or the steps outlined below. <i>Note: The video only shows Firefox, but once you've installed the extension, the steps are the same.</i>
    </p>
    <h3><a name="install" href="posts/data-science/#install" class="anchor" alt="deep link to this section"></a>Install the extension</h3>
    <p>Follow the links for your browser.</p>
    <ul>
      <li>
        <b>Firefox:</b> (1) visit the extension's <a href="https://addons.mozilla.org/en-US/firefox/addon/lit-prompts/" target="_blank">add-ons page</a>; (2) click "Add to Firefox;" and (3) grant permissions.
      </li>
      <li>
        <b>Chrome:</b>  (1) visit the extension's <a href="https://chromewebstore.google.com/detail/lit-prompts/hfeojjmldhebkeknfapoghcohkhffcmp" target="_blank">web store page</a>; (2) click "Add to Chrome;" and (3) review permissions / "Add extension."
      </li>
    </ul>
    <p>
      If you don't have Firefox, you can <a href="https://www.mozilla.org/en-US/firefox/new/" target="_blank">download it here</a>. Would you rather use Chrome? <a href="https://www.google.com/chrome/" target="_blank">Download it here</a>.
    </p>
    <h3><a name="point" href="posts/data-science/#point" class="anchor" alt="deep link to this section"></a>Point it at an API</h3>
    <p>
      Here we'll walk through how to use an LLM provided by OpenAI, but you don't have to use their offering. If you're interested in alternatives, you can find them <a href="https://github.com/SuffolkLITLab/prompts/tree/main#openai-compatible-api-integration" target="_blank">here</a>. You can even run your LLM locally, avoiding the need to share your prompts with a third-party. If you need an OpenAI account, you can <a href="https://platform.openai.com/signup" target="_blank">create one here</a>. Note: when you create a new OpenAI account you are given a limited amount of free API credits. If you created an account some time ago, however, these may have expired. If your credits have expired, you will need to enter a <a href="https://platform.openai.com/account/billing/overview" target="_blank">billing method</a> before you can use the API. You can check the state of any credits <a href="https://platform.openai.com/usage" target="_blank">here</a>.
    </p>
    <p>
      Login to <a href="https://openai.com/" target="_blank">OpenAI</a>, and navigate to the <a href="https://platform.openai.com/docs/" target="_blank">API documentation</a>.
    </p>
    <div class="featured_img_center">
      <a href="images/50-days/OpenAI_keys.png"><img src="images/50-days/OpenAI_keys.png" ALT="Screenshot of the OpenAI API Keys page showing where to click to create a new key." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
    </div>
    <p>Once you are looking at the API docs, follow the steps outlined in the image above. That is:
    <ol>
      <li>Select "API keys" from the left menu</li>
      <li>Click "+ Create new secret key"</li>
    </ol>
    <hr>
    <p>
      On LIT Prompt's <i>Templates & Settings</i> screen, set your API Base to <code>https://api.openai.com/v1/chat/completions</code> and your API Key equal to the value you got above after clicking "+ Create new secret key".  You get there by clicking the <i>Templates & Settings</i> button in the extension's popup:
    </p>
    <ol>
      <li>open the extension</li>
      <li>click on  <i>Templates & Settings</i></li>
      <li>enter the API Base and Key (under the section <i>OpenAI-Compatible API Integration</i>)</li>
    </ol>
    <div class="featured_img_center">
      <a href="images/50-days/popup.png"><img src="images/50-days/popup.png" ALT="Screenshot of the LIT Prompts extension popup." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
    </div>
    <p>
      Once those two bits of information (the API Base and Key) are in place, you're good to go. Now you can edit, create, and run prompt templates. Just open the LIT Prompts extension, and click one of the options. I suggest, however, that you read through the <i>Templates and Settings</i> screen to get oriented. You might even try out a few of the preloaded prompt templates. This will let you jump right in and get your hands dirty in the next section.
    </p>
    <div class="featured_img_center">
      <a href="images/50-days/credentials.png"><img src="images/50-days/credentials.png" ALT="Screenshot of the LIT Prompts extension popup." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
    </div>
    <p>
      <span style="background:yellow;">If you receive an error when trying to run a template after entering your Base and Key, and you are using OpenAI, make sure to check the state of any credits <a href="https://platform.openai.com/usage" target="_blank">here</a>. If you don't have any credits, you will need a billing method on file.</span>
    </p>
    <p>
      <i>If you found this hard to follow, consider following along with the first four minutes of the video <a href="posts/data-science/#setup">above</a>. It covers the same content. It focuses on Firefox, but once you've installed the extension, the steps are the same.</i>
    </p>
  </div>

  <!--
    =================================================

                   Write Your Template

    =================================================
  -->
  <hr>
  <h2><a name="template" href="posts/data-science/#template" class="anchor" alt="deep link to this section"></a>The Prompt Patterns (Templates)</h2>

  <div class="featured_img_right">
    <a href="images/boxquote.png"><img src="images/boxquote.png" ALT="A slide showing the George Box quote: All models are wrong, but some models are useful." class="list_img_file"/></a>
    <div class="caption">
      Maps are models; they don't show everything. That's okay as long as you don't confuse the map for the territory.
    </div>
  </div>

  <p>
    When crafting a LIT Prompts template, we use a mix of plain language and variable placeholders. Specifically, you can use double curly brackets to encase predefined variables. If the text between the brackets matches one of our predefined variable names, that section of text will be replaced with the variable's value. Today we'll be using <code>{{scratch}}</code>. See the extension's <a href="https://github.com/SuffolkLITLab/prompts#prompt-templates" target="_blank">documentation</a>.
  </p>

  <p>
    If the text within brackets is not the name of a predefined variable, like <code>{{What question should your model answer?}}</code>, it will trigger a prompt for your user that echo's the placeholder (e.g., a text bubble containing, "What question should your model answer?"). After the user answers, their reply will replace this placeholder. A list of predefined variables can be found in the extension's <a href="https://github.com/SuffolkLITLab/prompts#prompt-templates" target="_blank">documentation</a>. 
  </p>
  <p>
    There are two templates below. The first suggests features in the <a href="https://mytoymodels.org/" target="_blank">My Toy Models</a> file format, placing them in the Scrtach Pad, and the second downloads the file. 
  </p>

  <p>Here's the first template's title.</p>
  <p><code>Create a toy model</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are a data scientist building a mathematical model to answer the following question:

Q: {{What question should your model answer?}}

State whether the model's answer/target is a continuous or categorical. Now, provide the shortest list of ex-ante features a group of experts would agree to include as inputs for your model formatted as a JSON objects of the following form:

```model = { 
             "question":"the question the model is trying to answer",
             "type":"continuous",
             "features" :[
               { "name":"mode of travel", "type":"categorical", "categories":["on foot","car","train","plane","boat"] },
               { "name":"height", "type":"continuous", "units":"inches", "lower":0, "upper":100, "mean":70 }
             ],
             "training": {
                           "headers": [
                             "recorded_on",
                             "mode_of_travel",
                             "height",
                             "target",
                             "note"
                           ],
                           "observations": []
                         },
             "trained_on": 0,
             "coefficients": {
                               "mode_of_travel": 0,
                               "height": 0,
                               "intercept": 0
                             },
             "performance": {}
           }```

OR

```model = { 
             "question":"the question the model is trying to answer",
             "type":"categorical",
             "target": { ["yes","no"] },
             "features" :[
               { "name":"mode of travel", "type":"categorical", "categories":["on foot","car","train","plane","boat"] },
               { "name":"height", "type":"continuous", "units":"inches", "lower":0, "upper":100, "mean":70 }
             ],
             "training": {
                           "headers": [
                             "recorded_on",
                             "mode_of_travel",
                             "height",
                             "target",
                             "note"
                           ],
                           "observations": []
                         },
             "trained_on": 0,
             "coefficients": {
                               "mode_of_travel": 0,
                               "height": 0,
                               "intercept": 0
                             },
             "performance": {}
        }```

Be sure to use appropriate units and ranges. That is, make sure any units used are those in which the feature is commonly measured and make sure that the range is large enough to capture the expected variability. Also, choose a sensible value for the mean. Under "headers" always include "recorded_on", "target", and "note". Under "coefficients" always include "intercept" and set all coefficients equal to 0 like above.

If the question asks for an answer other than a continuous or categorical value, respond with the output that looks like this:

```model = {"question":"the question the model is trying to answer", "type":"uncomputable"}```

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>1000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>Yes</code>. This asks the model to output its answer in something called JSON, which is a nice machine-readable way to structure data. See <a href="https://en.wikipedia.org/wiki/JSON" target="_blank">https://en.wikipedia.org/wiki/JSON</a></li>
    <li><b>Output To:</b> <code>Screen + replace scratch pad</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the screen and replacing the the current text of the Scrtach Pad with this output.</li> 
    <li><b>Post-run Behavior:</b> <code>FULL STOP</code>. Like the choice of output, we can decide what to do after a template runs. To keep things simple, I went with "FULL STOP."</li>    <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  
  
  <p>Here's the second template's title.</p>
  <p><code>Download MyToyModel file</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{scratch}}</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>

    <li><b>Output Type:</b> <code>Prompt</code>. By choosing "Prompt" the template runs without being submitted to an LLM. It's output is just the template after slotting in variable values. </li>

    <li><b>Model:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Temperature:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
    <li><b>Max Tokens:</b> <code>n/a</code>. Since Output Type is set to Prompt, we don't have to set LLM-specific parameters. </li>
        <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>SAVE TO FILE</code>. Like the choice of output, we can decide what to do after a template runs. Here we will save the output to a file. This will trigger your browser's download feature.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
<h3><a name="working" href="posts/data-science/#working" class="anchor" alt="deep link to this section"></a>Working with the above templates</h3>
<p>
  To work with the above templates, you could copy them and their parameters into LIT Prompts one by one, or you could download a single prompts file and upload it from the extension's <i>Templates &amp; Settings</i> screen. This will replace your existing prompts.
</p>

  <div class="featured_img_center" style="max-width:900px;">
    <a href="images/50-days/template_upload.png"><img src="images/50-days/template_upload.png" ALT="Screenshot of the LIT Prompts Templates and Settings page showing where to upload prompts files." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
  </div>
  <p>
    You can download a prompts file (the above template and its parameters) suitable for upload by clicking this button:
  </p>

  <div class="button_row">
    <a href="javascript:void('');" onClick="saveTextAsFile(prompts,'prompt_template.txt')" class="button" style="width:220px;">Download prompts file</a>
  </div>
  <!--
    =================================================

                    Kick the Tires

    =================================================
  -->
  <hr>
  <h2><a name="tires" href="posts/data-science/#tires" class="anchor" alt="deep link to this section"></a>Kick the Tires</h2>
  <p>
    It's one thing to read about something and another to put what you've learned into practice. Let's see how this template performs.
  </p>
  <ul>
  
    <li>
      <b>You're the expert</b>. Use the template to construct a model for something you know pretty well. See how it's suggested features stack up. 
    </li>
    
  </ul>
    <!--
    =================================================

                     Export & Share

    =================================================
  -->
  <hr>
  <h2><a name="export" href="posts/data-science/#export" class="anchor" alt="deep link to this section"></a>Export and Share</h2>

  <p>
    After you've made the templates your own and them behaving the way you like, you can export and share them with others. This will produce an HTML file you can share. This file should work on any internet connected device. To create your file, click the <i>Export Scratch Pad &amp; Interactions Page</i> button. The contents of the textarea above the button will be appended to the top of your exported file. Importantly, if you don't want to share your API key, you should temporarily remove it from your settings before exporting.
  </p>
  <div class="featured_img_center" style="max-width:900px;">
    <a href="images/50-days/export_html.png"><img src="images/50-days/export_html.png" ALT="Screenshot of the LIT Prompts Templates and Settings page showing where to export a file." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
  </div>

  <p>
    If you want to see what an exported file looks like without having to make one yourself. You can use the buttons below. <i>View export in browser</i> will open the file in your browser, and <i>Download export</i> will download a file. In either case the following custom header will be inserted into your file. It will NOT include an API key. So, you'll have to enter one when asked if you want to see things work. <i>This information is saved in your browser. If you've provided it before, you won't be asked again. It is not shared with me. To remove this information for this site (and only this site, not individual files), you can follow the instructions found on my <a href="privacy">privacy page</a>.</i> Remember, when you export your own file, whether or not it contains and API key depends on if you have one defined at the time of output.
  </p>

  <p>Custom header:</p>
    <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>&lt;h2>Get Help Building a Toy Model&lt;/h2>
&lt;p>
  Use this tool to suggest features for use in a linear or logistic regression model in the form of a &lt;a href="https://mytoymodels.org/" target="_blank">My Toy Models&lt;/a> data file. For context check out &lt;a href="https://sadlynothavocdinosaur.com/posts/data-science" target="_blank">The AI Ouroboros: Can 2024's AI Build 2014's AI? Using LLMs to suggest features for linear and logistic regression models&lt;/a>.
&lt;/p>
&lt;hr style="border: solid 0px; border-bottom: solid 1px #555;margin: 5px 0 15px 0"/>
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>
    Not sure what's up with all those greater than and less than signs? Looking for tips on how to style your HTML? Check out this <a href="https://www.w3schools.com/html/default.asp" target="_blank">general HTML tutorial</a>.
  </p>
  <div class="button_row">
    <a href="posts/data-science/interactions.html" target="_blank" class="button" style="width:230px;margin-right: 15px;">View export in browser</a>
    <a href="javascript:void('');" onClick="saveTextAsFile(exported,'interactions.html')" class="button">Download export</a>
  </div>
  <!--
    =================================================

                       References

    =================================================
  -->
  <hr>
  <h2><a name="references" href="posts/data-science/#references" class="anchor" alt="deep link to this section"></a>TL;DR References</h2>
  <p>
    ICYMI, here are blubs for a selection of works I linked to in this post. If you didn't click through above, you might want to give them a look now.   </p>
  <ul> 
    <li>
      <a href="" target="_blank">The Robust Beauty of Improper Linear Models in Decision Making</a> by Robyn M. Dawes. 
      <br> 

      ABSTRACT: Proper linear models are those in which
predictor variables are given weights in such a way
that the resulting linear composite optimally predicts
some criterion of interest; examples of proper linear
models are standard regression analysis, discriminant
function analysis, and ridge regression analysis. Research summarized in Paul Meehl's book on clinical
versus statistical prediction—and a plethora of research stimulated in part by that book—all indicates
that when a numerical criterion variable (e.g., graduate
grade point average) is to be predicted from numerical
predictor variables, proper linear models outperform
clinical intuition. Improper linear models are those in
which the weights of the predictor variables are obtained by some nonoptimal method; for example, they
may be obtained on the basis of intuition, derived
from simulating a clinical judge's predictions, or set to
be equal. This article presents evidence that even
such improper linear models are superior to clinical intuition when predicting a numerical criterion from
numerical predictors. In fact, unit (i.e., equal) weighting is quite robust for making such predictions. The
article discusses, in some detail, the application of unit
weights to decide what bullet the Denver Police Department should use. Finally, the article considers
commonly raised technical, psychological, and ethical
resistances to using linear models to make important
social decisions and presents arguments that could
weaken these resistances.
    </li>

  </ul>

  <!--
  =================================================

                  Preview projects

  =================================================
  -->
  <div id="previews"></div>

  </div>
  <!-- END PAGE CONTENT -->
  <div class="footer">
      <span class="footer_links">
        <a href="https://mastodon.social/@Colarusso" target="_blank">Mastodon</a>
        | <a href="https://github.com/colarusso" target="_blank">GitHub</a>
        | <a href="./privacy">Privacy</a>
        | <a href="https://sadlynothavocdinosaur.com/feed.xml">RSS</a>
      </span>
      <span class="byline">Site by David Colarusso</span>
  </div>
</div>

<script>
  /*new GreenAudioPlayer('.gap-example');
  const audio_object = document.querySelector('.gap-example  audio');

  try {
    MathJax.typeset();
	} catch (error) {}*/

  (async () => {
    prompts = await loadFile('posts/data-science/prompt_template.txt');
    exported = await loadFile('posts/data-science/interactions.html');
  })()
</script>

</BODY></HTML>
