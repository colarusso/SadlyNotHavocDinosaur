<!DOCTYPE html>
<HTML><HEAD>

  <!-- Set base for this page equal to domain root -->
  <base href="../../">

  <!-- Page-specific metadata -->
  <title>Build an AI-Augmented Word Processor: Include an interactive version of your style guide; predict reader questions; argue with a devil's advocate; shorten or simplify text... Move beyond spelling and grammar checks—evaluate an author's logic!</title>
  <meta property="og:type" content="website"/>
  <meta property="og:publish_date" content="2024-03-18T00:00:00-0500"/>
  <meta property="og:title" content="Build an AI-Augmented Word Processor: Include an interactive version of your style guide; predict reader questions; argue with a devil's advocate; shorten or simplify text... Move beyond spelling and grammar checks—evaluate an author's logic!"/>
  <meta property="og:description" content="I suspect most folks hearing talk of &quot;an AI-augmented word processor&quot; imagine something that takes in 7 words and turns out 700 (e.g., write an essay on the French Revolution). I think that's a pretty unimaginative vision of AI-assisted writing. As I've said again, and again, that's looking through the wrong end of the telescope. More folks should be feeding these tools 500 words and asking for 5. Today I'll show you how to turn your prompt templates into an AI writing assistant. I'll show you what this technology can do when it's interested in helping you do the hard work, not taking a shortcut. "/>
  <meta property="og:image" content="http://www.davidcolarusso.com/images/50-days/robo_mop_square.png"/>
  <meta property="og:image:width" content="1024" />
  <meta property="og:image:height" content="1024" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ST9X6H808L"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-ST9X6H808L');
  </script>

  <!-- Metadata for mobile -->
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
  <meta name="apple-mobile-web-app-capable" content="no" />
  <link rel="apple-touch-icon" href="images/comic.png"/>

  <!-- JS & style -->
  <link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
  <link rel="stylesheet" type="text/css" href="css/style.css?v=2024-01-30">
  <script src="js/functions.js?v=2024-01-30"></script>
  <script src="js/spin.js"></script>

  <link rel="stylesheet" href="css/prism.css" data-noprefix="">
  <script type="text/javascript" src="js/prism.js"></script>

  <!--<script id="MathJax-script" async src="js/mathjax/tex-mml-chtml.js"></script>

  <link rel="stylesheet" type="text/css" href="css/green-audio-player.css">
  <script src="js/green-audio-player.js"></script>-->

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="Sadly Not, Havoc Dinosaur" href="https://sadlynothavocdinosaur.com/feed.xml" />

</HEAD>
<BODY BGCOLOR="#ffffff" BACKGROUND="" MARGINWIDTH="0" MARGINHEIGHT="0">

<!-- Message Banner -->
<div id="msg_bar" style="display:none;"></div>

<!-- Title and search -->
<div class="title_bar">
  <div class="home">
    <a href="./" tabindex="1"><img src="images/home.png" class="home_btn"></a>
  </div>
  <div class="search">
    <a href="javascript:show_search();" tabindex="3"><img src="images/search.png" class="search_btn"></a>
    <input id="query" type="text" tabindex="2"/>
  </div>
  <span id="title"><a href="./" class="title_home">Sadly Not, Havoc Dinosaur</a></span>
</div>

<div class="content">
  <!-- START PAGE CONTENT -->

  <div id="page">
  <!--
    =================================================

                      INTRODUCTION

    =================================================
  -->
  <h1 class="post_title_01">Build an AI-Augmented Word Processor</h1>
  <div class="post_title_02">Include an interactive version of your style guide; predict reader questions; argue with a devil's advocate; shorten or simplify text... Move beyond spelling and grammar checks—evaluate an author's logic!</div>
  <div class="featured_img_right">
    <!--<div class="audio_container_container" style="display:show;">
      <div class="audio_container">
        <b>Hear the author read <i>TK</i></b>
        <div class="gap-example player-accessible">
          <audio>
              <source src="mp3s/title.mp3" type="audio/mpeg">
          </audio>
        </div>
        <span class="playback">
          Speed: <a href="javascript:void('')" onClick="set_speed(0.5)" class="playback" id="pb05">0.5x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(1)" class="playback" id="pb10" style="font-weight:900;">1x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(1.5)" class="playback" id="pb15">1.5x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(2)" class="playback" id="pb20">2x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(3)" class="playback" id="pb30">3x</a>
        </span>
      </div>
    </div>-->
    <a href="images/50-days/robo_mop.png"><img src="images/50-days/robo_mop.png" ALT="An image from The Sorcerer's Apprentice" class="list_img_file"/></a>
    <div class="caption">
      The Sorcerer's Apprentice, latent space "photography" by <a href="https://mastodon.social/@Colarusso" target="_blank" class="captionlnk">Colarusso</a>
    </div>
  </div>
  <p class="post_p">
    <a href="https://mastodon.social/@Colarusso" target="_blank" class="body_links"><img src="images/colarusso.jpg" class="headshot_small" alt="Headshot of the author, Colarusso." style="margin-top: 7px;"/></a>
    David Colaursso<br><span class="post_date">Co-director, Suffolk's <a href="https://suffolklitlab.org/" target="_blank" class="captionlnk">Legal Innovation &amp; Tech Lab</a></span>
  </p>
  <p><i>This is <b>the 41st</b> post in my series <a href="posts/50-days-of-lit-prompts">50 Days of LIT Prompts</i></a>.</p>

  <i>
    If you want to take my <a href="ai/word-processor/" target="_blank">AI-powered word processor</a> for a test drive, be my guest</a>, but consider letting me explain some things first.
  </i>

  <p>
    I suspect most folks hearing talk of "an AI-augmented word processor" imagine something that takes in 7 words and turns out 700 (e.g., write an essay on the French Revolution). I think that's a pretty unimaginative vision of <a href="https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web" target="_blank">AI-assisted writing</a>. As I've said <a href="posts/summarize-and-question">again</a>, and <a href="posts/magnify">again</a>, that's looking through the wrong end of the telescope. More folks should be feeding these tools 500 words and asking for 5. Today I'll show you how to turn your prompt templates into an AI writing assistant. I'll show you what this technology can do when it's interested in helping you do the hard work, not taking a shortcut. 
  </p>  

  <blockquote>
    If there is any lesson that we should take from stories about genies granting wishes, it's that the desire to get something without effort is the real problem. Think about the story of "The Sorcerer's Apprentice," in which the apprentice casts a spell to make broomsticks carry water but is unable to make them stop. The lesson of that story is not that magic is impossible to control: at the end of the story, the sorcerer comes back and immediately fixes the mess the apprentice made. The lesson is that you can't get out of doing the hard work. The apprentice wanted to avoid his chores, and looking for a shortcut was what got him into trouble.
  </blockquote> 
  <p style="text-align:right;padding-right: 25px;">- <a href="https://www.newyorker.com/science/annals-of-artificial-intelligence/will-ai-become-the-new-mckinsey" target="_blank">Ted Chiang</a>
  <p>

  <p>We'll be working with ten prompt templates, all based on examples from earlier in this series. Each template corresponds to one of our word processor's 10 functions. The trick here is that we'll export a single HTML file containing all of the templates to act as our <a href="ai/word-processor/" target="_blank">word processor</a>. 
  </p>
  <div class="featured_img_center" style="max-width:640px;">
    <a href="images/50-days/word_processor.gif"><img src="images/50-days/word_processor.gif" alt="Screenshot of the word processor in action" class="list_img_file" style="border: 1px solid #a5a5a5;"></a>
    <div class="caption">
      Screen capture of the word processor in action
    </div>
  </div>

  <p>
    I'm particularly fond of the templates that engage with the author's ideas because they help us do the hard work. Spell check was followed by grammar check, and now finally, we have tools that can check (or at least nudge us to check) our thinking. Consider, functions 4, 5, and 6 below. 
  </p>


  <div class="featured_img_right" style="max-width:400px;">
    <a href="images/50-days/geordi_llm_outputs.png"><img src="images/50-days/geordi_llm_outputs.png" alt="A meme in the form of the Drake meme with Geordi from Start trek. Hold up: LLM outputs more than you put in (e.g., write me an essay). Now You're Talking: LLM outputs less than you put in (e.g., given my essay, what questions might readers have?)" class="list_img_file"></a>
    <div class="caption">
      Geordi talks with us about how to use LLMs responsibility
    </div>
  </div>
  

  <p>
    As I've noted <a href="posts/reader-questions">before</a>, one of my favorite bits of writing advice is, "let your writing breathe." That is, don't write everything in one go. Let it sit so you can come back to it with fresh eyes. Reading your work out loud, or better yet, having a computer read it to you are close contenders. It's easy to reread something you just wrote and unconsciously fill in the blanks. If you want a different perspective, historically you've needed distance or another person. Now, however, there's a third option. ;) 
  </p>

  <p>The <a href="ai/word-processor/" target="_blank">word processor</a>'s ten features are listed below, along with links to the posts where we first met them. I suggest clicking through those links if you want to learn more. Additionally, you can take them all for a test drive <a href="ai/word-processor/" target="_blank">here</a>. I do hope, however, you follow through below and end up adding some of your own. </p>

  <h3><a name="features" href="posts/find-my-cites/#features" class="anchor" alt="deep link to this section"></a>Features</h3>
  <ol>
    <li><b>📖 Define word/phrase</b>. <a href="posts/define-words">A Rose by Any Other Name</a>: Define a selected word, phrase, idiom, or initialism on a webpage</li>
<li><b>🗜️ Shorten selected text</b>. <a href="posts/shorten">500 Characters? I'll Make It Fit!</a> Use AI to shorten text</li>
<li><b>👩🏻‍🏫 Suggest plain language</b>. <a href="posts/legalese">Translate Legalese</a>: An AI tool for rewriting texts in plain language</li>
<li><b>🤷 Anticipate reader questions</b>. <a href="posts/reader-questions">Anticipating Reader Questions</a>: Have AI/LLMs suggest questions readers might have about your writing</li>
<li><b>🕵🏽‍♀️ Flag logical fallacies (JSON)</b>. <a href="posts/flag-fallacies">Flag Logical Fallacies with a Browser Extension</a>: Have AI read a webpage and flag logical fallacies </li>
<li><b>😈 Engage devil's advocate</b>. <a href="posts/devils-advocate">Summon the Demon</a>: Strengthen your arguments with an AI-powered devil's advocate</li>
<li><b>📕 Apply house style guide</b>. <a href="posts/style-guide">The Elements of Interactive Style Guides</a>: Have AI provide writing feedback based on a house style guide</li>
<li><b>🗞️ Suggest headlines/titles</b>. <a href="posts/headlines">Follow This One Trick to Write Great Headlines</a>: Produce a collection of possible <strike>clickbait</strike> compelling headlines based on the text of an article</li>
<li><b>📝 Summarize and question</b>. <a href="posts/summarize-and-question">Using AI to Distill and Question Texts</a>: Summarize and question the contents of a webpage from within the browser</li>
<li><b>🤖 🐂 💩 BS with a "bot."</b> <a href="posts/bs">Algorithmic BS</a>: Shoot the breeze with your base LLM</li>
  </ol>
  <p>There is one template I wish I could have included but didn't: <a href="posts/magnify">Magnifying Ideas and Expanding Text with AI</a>: In which I show you how to have AI write something both personal and original, bucking the assumptions that AI writing can never be personal or original. Unfortunately, this template used the Scratch Pad to store a running transcript of its conversation which conflicted with using the Scratch Pad as a place for typing your text. So, I had to leave it out. That being said...</p>


  <!-- END INTRO -->

  <h3><a name="build" href="posts/word-processor/#build" class="anchor" alt="deep link to this section"></a>Let's build something!</h3>
  <p>
    We'll do our building in the LIT Prompts extension. If you aren't familiar with the LIT Prompts extension, don't worry. We'll walk you through setting things up before we start building. If you have used the LIT Prompts extension before, skip to <a href="posts/word-processor/#template">The Prompt Pattern (Template)</a>.
  </p>
  <h3><a name="upnext" href="posts/word-processor/#upnext" class="anchor" alt="deep link to this section"></a>Up Next</h3>
  <ul>
    <li><a href="posts/word-processor/#setup" onClick="expand_setup();">Setup LIT Prompts</a></li>
    <ul>
      <li><a href="posts/word-processor/#install" onClick="expand_setup();">Install the extension</a></li>
      <li><a href="posts/word-processor/#point" onClick="expand_setup();">Point it at an API</a></li>
    </ul>
    <li><a href="posts/word-processor/#template">The Prompt Pattern (Template)</a></li>
    <li><a href="posts/word-processor/#tires">Kick the Tires</a></li>
    <li><a href="posts/word-processor/#export">Export and Share </a></li>
    <li><a href="posts/word-processor/#references">TL;DR References</a></li>
  </ul>
  <p>
    <b>Questions or comments?</b> I'm on Mastodon <a href="https://mastodon.social/@Colarusso" target="_blank">@Colarusso@mastodon.social</a>
  </p>
  <!--
    =================================================

                   Setup LIT Prompts

    =================================================
  -->
  <hr>
  <h2><a name="setup" href="posts/word-processor/#setup" onClick="expand_setup();" class="anchor" alt="deep link to this section"></a>Setup LIT Prompts </h2>
  <div id="expand_setup" style="text-align: left;display:none;font-size: small;">
    <a href="javascript:expand_setup();" style="text-decoration: none;">&#9658; Expand</a>
  </div>
  <div id="collapse_setup" style="text-align: left;font-size: small;">
    <a href="javascript:collapse_setup();" style="text-decoration: none;">&#9660; Collapse</a>
  </div>
  <div id="setup_extension">
    <div class="list_vid">
      <iframe class="embed_vid" src="https://www.youtube-nocookie.com/embed/Ql8aXGvLBGU" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
      <div class="caption">
        7 min intro video
      </div>
    </div>
    <p>
      <i><b>LIT Prompts</b></i> is a browser extension built at Suffolk University Law School's <a href="https://suffolklitlab.org/" target="_blank">Legal Innovation and Technology Lab</a> to help folks explore the use of <a href="https://en.wikipedia.org/wiki/Large_language_model" target="_blank">Large Language Models</a> (LLMs) and <a href="https://en.wikipedia.org/wiki/Prompt_engineering" target="_blank">prompt engineering</a>. LLMs are sentence completion machines, and prompts are the text upon which they build. Feed an LLM a prompt, and it will return a plausible-sounding follow-up (e.g., "Four score and seven..." might return "years ago our fathers brought forth..."). LIT Prompts lets users create and save prompt templates based on data from an active browser window (e.g., selected text or the whole text of a webpage) along with text from a user. Below we'll walk through a specific example.
    </p>
    <p>
      To get started, follow <b>the first four minutes</b> of the intro video or the steps outlined below. <i>Note: The video only shows Firefox, but once you've installed the extension, the steps are the same.</i>
    </p>
    <h3><a name="install" href="posts/word-processor/#install" class="anchor" alt="deep link to this section"></a>Install the extension</h3>
    <p>Follow the links for your browser.</p>
    <ul>
      <li>
        <b>Firefox:</b> (1) visit the extension's <a href="https://addons.mozilla.org/en-US/firefox/addon/lit-prompts/" target="_blank">add-ons page</a>; (2) click "Add to Firefox;" and (3) grant permissions.
      </li>
      <li>
        <b>Chrome:</b>  (1) visit the extension's <a href="https://chromewebstore.google.com/detail/lit-prompts/hfeojjmldhebkeknfapoghcohkhffcmp" target="_blank">web store page</a>; (2) click "Add to Chrome;" and (3) review permissions / "Add extension."
      </li>
    </ul>
    <p>
      If you don't have Firefox, you can <a href="https://www.mozilla.org/en-US/firefox/new/" target="_blank">download it here</a>. Would you rather use Chrome? <a href="https://www.google.com/chrome/" target="_blank">Download it here</a>.
    </p>
    <h3><a name="point" href="posts/word-processor/#point" class="anchor" alt="deep link to this section"></a>Point it at an API</h3>
    <p>
      Here we'll walk through how to use an LLM provided by OpenAI, but you don't have to use their offering. If you're interested in alternatives, you can find them <a href="https://github.com/SuffolkLITLab/prompts/tree/main#openai-compatible-api-integration" target="_blank">here</a>. You can even run your LLM locally, avoiding the need to share your prompts with a third-party. If you need an OpenAI account, you can <a href="https://platform.openai.com/signup" target="_blank">create one here</a>. Note: when you create a new OpenAI account you are given a limited amount of free API credits. If you created an account some time ago, however, these may have expired. If your credits have expired, you will need to enter a <a href="https://platform.openai.com/account/billing/overview" target="_blank">billing method</a> before you can use the API. You can check the state of any credits <a href="https://platform.openai.com/usage" target="_blank">here</a>.
    </p>
    <p>
      Login to <a href="https://openai.com/" target="_blank">OpenAI</a>, and navigate to the <a href="https://platform.openai.com/docs/" target="_blank">API documentation</a>.
    </p>
    <div class="featured_img_center">
      <a href="images/50-days/OpenAI_keys.png"><img src="images/50-days/OpenAI_keys.png" ALT="Screenshot of the OpenAI API Keys page showing where to click to create a new key." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
    </div>
    <p>Once you are looking at the API docs, follow the steps outlined in the image above. That is:
    <ol>
      <li>Select "API keys" from the left menu</li>
      <li>Click "+ Create new secret key"</li>
    </ol>
    <hr>
    <p>
      On LIT Prompt's <i>Templates & Settings</i> screen, set your API Base to <code>https://api.openai.com/v1/chat/completions</code> and your API Key equal to the value you got above after clicking "+ Create new secret key".  You get there by clicking the <i>Templates & Settings</i> button in the extension's popup:
    </p>
    <ol>
      <li>open the extension</li>
      <li>click on  <i>Templates & Settings</i></li>
      <li>enter the API Base and Key (under the section <i>OpenAI-Compatible API Integration</i>)</li>
    </ol>
    <div class="featured_img_center">
      <a href="images/50-days/popup.png"><img src="images/50-days/popup.png" ALT="Screenshot of the LIT Prompts extension popup." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
    </div>
    <p>
      Once those two bits of information (the API Base and Key) are in place, you're good to go. Now you can edit, create, and run prompt templates. Just open the LIT Prompts extension, and click one of the options. I suggest, however, that you read through the <i>Templates and Settings</i> screen to get oriented. You might even try out a few of the preloaded prompt templates. This will let you jump right in and get your hands dirty in the next section.
    </p>
    <div class="featured_img_center">
      <a href="images/50-days/credentials.png"><img src="images/50-days/credentials.png" ALT="Screenshot of the LIT Prompts extension popup." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
    </div>
    <p>
      <span style="background:yellow;">If you receive an error when trying to run a template after entering your Base and Key, and you are using OpenAI, make sure to check the state of any credits <a href="https://platform.openai.com/usage" target="_blank">here</a>. If you don't have any credits, you will need a billing method on file.</span>
    </p>
    <p>
      <i>If you found this hard to follow, consider following along with the first four minutes of the video <a href="posts/word-processor/#setup">above</a>. It covers the same content. It focuses on Firefox, but once you've installed the extension, the steps are the same.</i>
    </p>
  </div>

  <!--
    =================================================

                   Write Your Template

    =================================================
  -->
  <hr>
  <h2><a name="template" href="posts/word-processor/#template" class="anchor" alt="deep link to this section"></a>The Prompt Patterns (Templates)</h2>

  <div class="featured_img_right">
    <a href="images/boxquote.png"><img src="images/boxquote.png" ALT="A slide showing the Georeg Box quote: All models are wrong, but some models are useful." class="list_img_file"/></a>
    <div class="caption">
      Maps are models; they don't show everything. That's okay as long as you don't confuse the map for the territory.
    </div>
  </div>

  <p>
    When crafting a LIT Prompts template, we use a mix of plain language and variable placeholders. Specifically, you can use double curly brackets to encase predefined variables. If the text between the brackets matches one of our predefined variable names, that section of text will be replaced with the variable's value. Today we'll be using <code>{{highlighted}}</code>. The <code>{{highlighted}}</code> variable contains any text you have highlighted/selected in the active browser tab when you open the extension. 
  </p>
  <p>
    If the text within brackets is not the name of a predefined variable, like <code>{{What is your name?}}</code>, it will trigger a prompt for your user that echo's the placeholder (e.g., a text bubble containing, "What is your name?"). After the user answers, their reply will replace this placeholder. A list of predefined variables can be found in the extension's <a href="https://github.com/SuffolkLITLab/prompts#prompt-templates" target="_blank">documentation</a>. We'll use this functionality in one of the templates below to collect and act on user input. 
  </p>
  <p>
    Looking for more on each of these templates? Check out the links provided in the list of <a href="posts/find-my-cites/#features">features</a>.
  </p>

  <p>Here's a template's title.</p>
  <p><code>📖 Define word/phrase</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>Define the following word/phrase: {{highlighted}}

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting—0.</li>
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>FULL STOP</code>. Like the choice of output, we can decide what to do after a template runs. To keep things simple, I went with "FULL STOP."</li>    <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's a template's title.</p>
  <p><code>🗜️ Shorten selected text</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You're a helpful editor and you're going to help trim some text. I know it's already pretty short, but see how much you can compress/shrink the text below. When you rewrite it, knock off at least 20% of the length, but keep the main points: 

{{highlighted}}

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting—0.</li>
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen + clipboard</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the screen and clipboard so the results will be ready to paste where we like.</li> 
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's a template's title.</p>
  <p><code>👩🏻‍🏫 Suggest plain language</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You're a helpful editor. Here is some text I'd like you to rewrite:

{{highlighted}}

Now rewrite the above text in plain language. That is, make sure it us using active voice and that it reads at a sixth-grade reading level. Replace any jargon with cogent and concise explanations. 

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting—0.</li>
    <li><b>Max Tokens:</b> <code>1000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen + clipboard</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the screen and clipboard so the results will be ready to paste where we like.</li> 
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's a template's title.</p>
  <p><code>🤷 Anticpate reader questions</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You're a helpful editor, and you're going to help me with a writing project. For the text that follows, what unanswered questions might the reader have? 

{{highlighted}}

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>500</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's a template's title.</p>
  <p><code>🕵🏽‍♀️ Flag logical fallacies (JSON)</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are a high school debate coach, working with science and English teachers to help your students better understand common logical fallacies. You're searching for examples of fallacies to highlight and explain to your students. In a moment, I will give you a text to evaluate for logical fallacies. The text may or may not contain fallacies you can use. Be on the lookout for any of the following common logical fallacies: 

1. Strawman: Misrepresenting someone's argument to make it easier to attack.
2. False cause: Assuming a relationship between things means that one is the cause of the other.
3. Appeal to emotion: Manipulating an emotional response instead of a valid argument.
4. Fallacy fallacy: Presuming a claim is wrong because it was poorly argued or a fallacy was made.
5. Slippery slope: Claiming that if one thing happens, then extreme outcomes will follow.
6. Ad hominem: Attacking an opponent's character to undermine their argument.
7. Tu quoque: Avoiding criticism by turning it back on the accuser.
8. Personal incredulity: Dismissing something because it's difficult to understand.
9. Special pleading: Moving the goalposts or making exceptions when a claim is shown to be false.
10. Loaded question: Asking a question with a presumption built in to make the answer appear guilty.
11. Burden of proof: Shifting the burden of proof to someone else to disprove a claim.
12. Ambiguity: Using double meanings or ambiguous language to mislead.
13. Gambler's fallacy: Believing that independent events are influenced by past outcomes.
14. Bandwagon: Appealing to popularity as a form of validation.
15. Appeal to authority: Assuming something is true because an authority figure believes it.
16. Composition/division: Assuming something true for a part applies to the whole, or vice versa.
17. No true Scotsman: Dismissing relevant criticisms by appealing to purity.
18. Genetic: Judging something as good or bad based on where it comes from.
19. Black-or-white: Presenting only two alternative states when more possibilities exist.
20. Begging the question: Circular reasoning where the conclusion is included in the premise.
21. Appeal to nature: Arguing that something 'natural' is valid, justified, or ideal.
22. Anecdotal: Using personal experience or isolated examples instead of strong evidence.
23. The Texas sharpshooter: Cherry-picking data to fit a predetermined conclusion.
24. Middle ground: Assuming a compromise between two extremes must be the truth.

---

Here's the text:

{{highlighted}}

---

Now that you've read the text. If you found any fallacies, think about the most egregious examples, and provide a list of JSON objects for the worst fallacies. The list should be of the following structure: 

{
  "fallacies": [
   {
    "fallacy":"name of fallacy",
     "explanation":"explanation of why the text is an example of this fallacy"
   },
   {
    "fallacy":"name of fallacy",
     "explanation":"explanation of why the text is an example of this fallacy"
   }
  ]
}

If multiple fallacies are present, provide multiple examples, but no more than three in total. If there are no clear fallacies return:

{
  "fallacies": []
}


</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting—0.</li>
    <li><b>Max Tokens:</b> <code>1000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>Yes</code>. This asks the model to output its answer in something called JSON, which is a nice machine-readable way to structure data. See <a href="https://en.wikipedia.org/wiki/JSON" target="_blank">https://en.wikipedia.org/wiki/JSON</a></li>
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>FULL STOP</code>. Like the choice of output, we can decide what to do after a template runs. To keep things simple, I went with "FULL STOP."</li>    <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's a template's title.</p>
  <p><code>😈 Engage devil's advocate</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are helping a colleague improve their thinking about a particular issue by taking on the role of a devil's advocate. Your job is to take issue with your colleague's conclusions, pushing back on the assumptions they are making and forcing them to consider that which they wouldn't normally consider. In a moment I will show you a text written by your colleague to get the ball rolling. You can also think of this as an acting job. As such, your job is to stay in character and act out your part. You are aiming for a realistic performance. To help you get into character, here is some background information about how to approach the role: 

BACKGROUND

1. Understand the Argument Fully

 - Listen Carefully: Before presenting counterarguments, make sure you have a thorough understanding of your colleague's viewpoint.
 - Clarify: Ask questions to clarify any points that are not clear to you. This shows you're engaged and also ensures you're responding to their actual position rather than a misunderstanding.

2. State Your Intent

 - Explain Your Role: Make it clear you're playing devil's advocate to explore the argument fully, not because you necessarily disagree.
 - Reaffirm Your Objectives: Emphasize that your goal is to strengthen the argument by examining it from all angles.

3. Present Alternative Perspectives

 - Offer Counterarguments: Introduce alternative viewpoints or potential weaknesses in the argument. Do this thoughtfully and respectfully.
 - Use Hypotheticals: Present hypothetical scenarios that challenge the argument in a non-confrontational way.

4. Encourage Exploration

 - Ask Open-Ended Questions: Encourage your colleague to think deeper about their stance by asking questions that require more than a yes or no answer.
 - Suggest Exploring Contrary Evidence: Propose looking into data or case studies that might offer a different perspective.

5. Maintain Respect and Openness

 - Be Respectful: Always communicate in a way that respects your colleague's intelligence and intentions.
 - Stay Open to Being Convinced: Show that you are open to changing your own stance based on the conversation. This makes it more likely for your colleague to be open-minded as well.

6. Summarize and Reflect

 - Summarize the Discussion: Recap what has been discussed, highlighting the strengths of the original argument and the insights gained from playing devil's advocate.
 - Reflect Together: Ask your colleague how they found the exercise and share your own reflections on the process.

7. Conclude Positively

 - Express Gratitude: Thank your colleague for engaging in the discussion. Recognize the value of having explored the argument from multiple angles.
 - Reiterate Support: Reinforce your support for your colleague, regardless of the argument's outcome.

Now here's the text you are to engage with.

THE TEXT

{{highlighted}}

DIRECTION

Be sure to keep your questions and responses short. You "speak in sentences not paragraphs." Short and conversational, no speechifying!

Think about how your character would respond and craft an appropriate reply. Remember, you are a the devil's advocate. Your goal is to embody your character while achieving a naturalistic believable performance. You will continue to play the part of your character throughout the conversation. Whatever happens, do NOT break character! Respond only with dialog, and include only the text of your reply (e.g., do NOT preface the text with the name of the speaker). After seeing The Text above, what do you say? And remember you're engaged in a dialogue not speechifying. Keep it short!
        
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>500</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's a template's title.</p>
  <p><code>📕 Apply house style guide</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>[the first six chapters of the 1920 edition of The Elements of Style by William Strunk, Jr.]</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>Prompt</code>. By choosing "Prompt" the template runs without being submitted to an LLM. It's output is just the template after slotting in variable values. </li>
     <li><b>Model:</b> <code>gpt-3.5-turbo</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Hidden</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, I've chosen the hide the output entirely. This is uesful when passing output to another template.</li> 
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we will trigger the <code></code> template.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's a template's title.</p>
  <p><code>Generate feedback</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are an editor helping a writer improve a piece. To refresh your memory I'm going to show you a copy of your house style guide. This will be followed by the writer's text. After this, I'll ask you to provide some helpful feedback on the piece based on the style guide.

----

HOUSE STYLE GUIDE:

{{passThrough}}

----

THE WRITER'S TEXT

{{highlighted}}

---

Provide helpful feedback based on the suggestions found in House Style Guide. Provide your advice along with quotes from where you suggest an edit.
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>1000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>checked</code>. This determines if a button is displayed for this template in the extension's popup window. We've checked the option because this template shouldn't be triggered by the user directly. Rather, it needs to be triggered by another template so that there's something in the <code>{{passThrough}}</code> variable.</li>  </ul>
  <p>Here's a template's title.</p>
  <p><code>🗞️ Suggest headlines/titles</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are the editor for an online publication. You're about to read a new post to your site. Once you've finished reading it, you will be asked to provide a list of possible titles/headlines to use when publishing it. 

Here's the text of the post

---- 

{{highlighted}}

----

For the untitled text above, provide a list of several compelling (clickable) titles/headlines that might do well to generate attention and clicks: 

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-16k</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>FULL STOP</code>. Like the choice of output, we can decide what to do after a template runs. To keep things simple, I went with "FULL STOP."</li>    <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's a template's title.</p>
  <p><code>📝 Summerize and question</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{highlighted}} 

---

Provide a short 150-word summary of the above text. If asked any follow-up questions, use the above text, and ONLY the above text, to answer them. If you can't find an answer in the above text, politely decline to answer explaining that you can't find the information. You can, however, finish a thought you started above if asked to continue, but don't write anything that isn't supported by the above text. And keep all of your replies short! But first, please provide a summary of the text. 

</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-16k</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting—0.</li>
    <li><b>Max Tokens:</b> <code>250</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
  <p>Here's a template's title.</p>
  <p><code>🤖 🐂 💩 BS with a "bot"</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>{{Yes?}} [# {{Yes?}} isn't a predefined variable. So, the user will be presented with a text input, and since Post-run Behavior is set to CHAT, this ends up being a plain old chat with an LLM. #]
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-3.5-turbo-16k</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0.7</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Here I'm using 0.7 because I'm happy to have the text be a little "creative."</li> 
    <li><b>Max Tokens:</b> <code>500</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>No</code>. This asks the model to output its answer in something called JSON. We don't need to worry about that here, hence the selection of "No."</li>  
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>CHAT</code>. Like the choice of output, we can decide what to do after a template runs. Here we want to be able to follow up with additional prompts. So, "CHAT" it is.</li>      <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
<h3><a name="working" href="posts/word-processor/#working" class="anchor" alt="deep link to this section"></a>Working with the above templates</h3>
<p>
  To work with the above templates, you could copy them and their parameters into LIT Prompts one by one, or you could download a single prompts file and upload it from the extension's <i>Templates &amp; Settings</i> screen. This will replace your existing prompts.
</p>

  <div class="featured_img_center" style="max-width:900px;">
    <a href="images/50-days/template_upload.png"><img src="images/50-days/template_upload.png" ALT="Screenshot of the LIT Prompts Templates and Settings page showing where to upload prompts files." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
  </div>
  <p>
    You can download a prompts file (the above template and its parameters) suitable for upload by clicking this button:
  </p>

  <div class="button_row">
    <a href="javascript:void('');" onClick="saveTextAsFile(prompts,'prompt_template.txt')" class="button" style="width:220px;">Download prompts file</a>
  </div>
  <!--
    =================================================

                    Kick the Tires

    =================================================
  -->
  <hr>
  <h2><a name="tires" href="posts/word-processor/#tires" class="anchor" alt="deep link to this section"></a>Kick the Tires</h2>
  <p>
    It's one thing to read about something and another to put what you've learned into practice. Let's see how this template performs.
  </p>
  <ul>
  
    <li>
      <b>Déjà vu</b>. Seeing as how these templates have come up before, I suggest checking out how we kicked the tires back when they were first introduced. For that, consult the links found above in <a href="posts/find-my-cites/#features">features</a>.
    </li>
    
    <li>
      <b>Add your own</b>. Come up with your own feature and add it to your own version of the word processor. 
    </li>
    
  </ul>
    <!--
    =================================================

                     Export & Share

    =================================================
  -->
  <hr>
  <h2><a name="export" href="posts/word-processor/#export" class="anchor" alt="deep link to this section"></a>Export and Share</h2>

  <p>
    After you've made the templates your own and them behaving the way you like, you can export and share them with others. This will produce an HTML file you can share. This file should work on any internet connected device. To create your file, click the <i>Export Scratch Pad &amp; Interactions Page</i> button. The contents of the textarea above the button will be appended to the top of your exported file. Importantly, if you don't want to share your API key, you should temporarily remove it from your settings before exporting.
  </p>
  <div class="featured_img_center" style="max-width:900px;">
    <a href="images/50-days/export_html_w_scratch.png"><img src="images/50-days/export_html_w_scratch.png" ALT="Screenshot of the LIT Prompts Templates and Settings page showing where to export a file." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
  </div>

  <p>
    If you want to see what an exported file looks like without having to make one yourself. You can use the buttons below. <i>View export in browser</i> will open the file in your browser, and <i>Download export</i> will download a file. In either case the following custom header will be inserted into your file. It will NOT include an API key. So, you'll have to enter one when asked if you want to see things work. <i>This information is saved in your browser. If you've provided it before, you won't be asked again. It is not shared with me. To remove this information for this site (and only this site, not individual files), you can follow the instructions found on my <a href="privacy">privacy page</a>.</i> Remember, when you export your own file, whether or not it contains and API key depends on if you have one defined at the time of output.
  </p>

  <p>Custom header:</p>
    <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>&lt;h2>An AI Word Processor&lt;/h2>

&lt;p>All of the actions below, except &lt;i>🤖 🐂 💩 BS with a "bot"&lt;/i>, require you to select text in the writing area before triggering them. Sometimes you'll want to select everything you've written (ctrl+a). Other times you'll want to select a paragraph, sentence, or word. All of these, except &lt;i>📖 Define word/phrase&lt;/i>, expect a sentence or more. Failure to select enough text may cause &lt;a href="https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)" target="_blank">hallucinations&lt;/a>. To understand what's going on here, and how to make your own AI word processor, check out &lt;a href="https://sadlynothavocdinosaur.com/posts/word-processor" target="_blank">Build an AI-Augmented Word Processor&lt;/a>.&lt;/p>

&lt;p>&lt;b>&lt;i>If you're in the middle of a chat below, and you want to restart, just refresh this page. The text in the text area will be saved.&lt;/b>&lt;/i>&lt;/p>

&lt;hr style="border: solid 0px; border-bottom: solid 1px #555;margin: 5px 0 15px 0"/>
</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>
    Not sure what's up with all those greater than and less than signs? Looking for tips on how to style your HTML? Check out this <a href="https://www.w3schools.com/html/default.asp" target="_blank">general HTML tutorial</a>.
  </p>
  <div class="button_row">
    <a href="posts/word-processor/interactions.html" target="_blank" class="button" style="width:230px;margin-right: 15px;">View export in browser</a>
    <a href="javascript:void('');" onClick="saveTextAsFile(exported,'interactions.html')" class="button">Download export</a>
  </div>
  <!--
    =================================================

                       References

    =================================================
  -->
  <hr>
  <h2><a name="references" href="posts/word-processor/#references" class="anchor" alt="deep link to this section"></a>TL;DR References</h2>
  <p>
    ICYMI, here are blubs for a selection of works I linked to in this post. If you didn't click through above, you might want to give them a look now.   </p>
  <ul> 

    <li>
      <a href="https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web" target="_blank">ChatGPT Is a Blurry JPEG of the Web</a> by Ted Chiang. Writing at the beginning of ChatGPT's rise to prominence, this article discusses the analogy between language models like ChatGPT and lossy compression algorithms. Chiang argues that while models can repackage/compress web information, they lack true understanding. Ultimately, Chiang concludes that starting with a blurry copy is not ideal when creating original content and that the struggling to express thoughts is an essential element of the writing process. 
    </li>

    <li><a href="https://www.newyorker.com/science/annals-of-artificial-intelligence/will-ai-become-the-new-mckinsey" target="_blank">Will A.I. Become the New McKinsey?</a> by Ted Chiang.
      This article explores the potential risks and consequences of artificial intelligence (A.I.) in relation to capitalism. Chiang suggests that A.I. can be seen as a management-consulting firm, similar to McKinsey &amp; Company, which concentrates wealth and disempowers workers. He argues that A.I. currently assists capital at the expense of labor, and questions whether there is a way for A.I. to assist workers instead of management. Chiang also discusses the need for economic policies to distribute the benefits of technology appropriately, as well as the importance of critical self-examination by those building world-shaking technologies. He concludes by emphasizing the need to question the assumption that more technology is always better and to engage in the hard work of building a better world. <i>Summary based on a draft from our <a href="posts/summarize-and-question">day one template</a>.</i>
    </li>

    <li>
      <a href="https://www.gutenberg.org/files/37134/37134-h/37134-h.htm" target="_blank">The Elements of Style</a> by William Strunk Jr. 
      The Elements of Style is a style guide for writing American English. It was originally written by William Strunk Jr. in 1918 and published in 1920. The book includes eight rules of usage, ten principles of composition, some matters of form, a list of commonly misused words and expressions, and a list of often misspelled words.
      <i>Summary based on a draft from our <a href="posts/summarize-and-question">day one template</a>.</i>
    </li>

  </ul>

  <!--
  =================================================

                  Preview projects

  =================================================
  -->
  <div id="previews"></div>

  </div>
  <!-- END PAGE CONTENT -->
  <div class="footer">
      <span class="footer_links">
        <a href="https://mastodon.social/@Colarusso" target="_blank">Mastodon</a>
        | <a href="https://github.com/colarusso" target="_blank">GitHub</a>
        | <a href="./privacy">Privacy</a>
        | <a href="https://sadlynothavocdinosaur.com/feed.xml">RSS</a>
      </span>
      <span class="byline">Site by David Colarusso</span>
  </div>
</div>

<script>
  /*new GreenAudioPlayer('.gap-example');
  const audio_object = document.querySelector('.gap-example  audio');

  try {
    MathJax.typeset();
	} catch (error) {}*/

  (async () => {
    prompts = await loadFile('posts/word-processor/prompt_template.txt');
    exported = await loadFile('posts/word-processor/interactions.html');
  })()
</script>

</BODY></HTML>
