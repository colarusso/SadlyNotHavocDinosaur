{
  "Summarize & question paper": {
    "prompt": "{{passThrough}} \n\n-------\n  \nProvide a short easy to understand 150 word summary of the above scholarly text. Present it and any subsequent answers using plain language, making it understandable for a general audience. If asked any follow-up questions, use the above text, and ONLY the above text, to answer them. If you can't find an answer in the above text, politely decline to answer explaining that you can't find the information. You can, however, finish a thought you started above if asked to continue, but don't write anything that isn't supported by the above text. And keep all of your replies short! \n",
    "model": "gpt-4o-mini",
    "temperature": 0,
    "max_tokens": 250,
    "output": 1,
    "json_mode": 0,
    "output_to": 0,
    "behavior": "chat",
    "hide_button": true
  },
  "Unsupervised Machine Scoring of Free Response Answers (Colarusso, 2022)": {
    "prompt": "Unsupervised Machine Scoring of Free Response Answers—Validated Against Law School Final Exams\nby David Colarusso\n\nAbstract\n\nThis paper presents a novel\n method for unsupervised machine scoring of short answer and essay question responses, relying solely on a sufficiently large set of responses to a common prompt, absent the need for pre-labeled sample answers—given said prompt is of a particular character. That is, for questions where “good” answers look similar, “wrong” answers are likely to be “wrong” in different ways. Consequently, when a collection of text embeddings for responses to a common prompt are placed in an appropriate feature space, the centroid of their placements can stand in for a model answer, providing a lodestar against which to measure individual responses. This paper examines the efficacy of this method and discusses potential applications.\n\n    Keywords: Natural Language Processing, NLP, Unsupervised Machine Learning, Machine Learning, ML, Automated Grading, Grading, Free Response Questions, Essays, Law School Exams\n\nIntroduction\n\nMany contemporary systems for the automated scoring of essays and short answer questions rely heavily on the application of supervised machine learning, requiring a set of pre-scored model answers (Burrows et al., 2015). When used, unsupervised methods have focused largely on the discovery of features for use in the supervised training of scoring models (e.g., the discovery of latent topics). The use of unsupervised methods has been used to group similar answers, allowing for more efficient scoring by human graders (Basu et al., 2013). However, the unsupervised component of this approach fails to produce insights into the correctness of answers and so cannot qualify as scoring. Rather, scoring is left to human graders or makes use of machine comparisons to pre-labeled sample answers (supervised learning). Consequently, current methods for the automated scoring of short answer and essay questions are poorly suited to spontaneous and idiosyncratic assessments. That is, the time saved in grading must be balanced against the time required for the training of a model. This includes tasks such as the creation of pre-labeled sample answers. This limits the utility of machine grading for novel or spontaneous assessments, such as those one might deploy in an individual section of a larger course in response to an open-ended class discussion.\n\nThis paper presents a novel method for unsupervised machine scoring of short answer and essay questions given only a sufficiently large set of responses to a properly-formed common prompt. This eliminates the need for the preparation of pre-labeled sample answers. It is the author’s hope that such a method may be leveraged to reduce the time needed to grade free response questions, promoting the increased adoption of formative assessment.\n\nThis paper will describe the particulars of this method, discuss a potential application, and examine the method’s efficacy.\nMethods and Application\n\n    “Happy families are all alike; every unhappy family is unhappy in its own way.” - Leo Tolstoy\n\nThe method described here depends on the insight gained from the Anna Karenina Principle. Drawing its name from the first line of Tolstoy’s Anna Karenina, this principle observes that success is often more constrained than failure (Diamond, 2005). That is, there are many more ways to fail at something than there are to succeed. Applied to the grading of free response questions, this can be restated as follows. For questions where “good” answers look similar, “wrong” answers are likely to be “wrong” in different ways.\n\nConsequently, there may be information about the correct answer to a question contained in the relative similarities between answers to that question. That is, those answers most like the average of the answers are likely to be “better” answers. If we are able to map answers into an appropriate feature space, we might expect that a ranking of answers based on their distance from the average of the answers would correspond to a ranking based on their quality.\n\nThe ability to gain actionable information from a population of answers by measuring this distance, however, is dependent on the nature of the question and its accompanying population of answers. The question must be one where “good” answers look similar, and the population must be both sufficiently large and sufficiently diverse for there to be a variety of wrong answers alongside a sizable number of correct answers.\n\nThe ability to gain actionable information from a population of answers in this way is also dependent on our ability to encode, in a computable fashion, the relevant content contained within an answer.\n\nAt present, a prompt asking respondents to “write an evocative short story” is poorly suited for the method described here. For example, “good” responses are not likely to be similar in a manner amenable to current methods for the automatic encoding of texts as numbers. Methods such as word2vec though able to capture some shadow of the responses’ semantic content will fail to capture affective content, and “good” responses are likely to differ a great deal in the former even if they are similar when it comes to the latter.\n\nUnderstanding these constraints, one can construct a method for the automated unsupervised scoring of free response answers. Such a method must begin with an appropriately framed question. That is a question where all of the “good” responses look alike as measured across some category or categories amenable to numerical encoding. From here the method can be described generally as:\n\n    1. Produce an embedding for each answer that captures as much of the relevant information as possible.\n\n    2. Find the centroid for all of the embeddings in your population of answers and impute the location of a “correct” model answer.\n\n    3. Measure the distance between each answer’s embedding and the “correct” answer (e.g., the answers’ centroid or medoid).\n\nAssuming the constraints above, the distance of an answer’s embedding from the centroid serves as a proxy for its quality. The closer an answer is to the centroid, the “better” it is. It may, however, be desirable to measure the distance from a nearby location. For example, if it is decided that the “best answer” should be the best answer to appear in an actual answer. In this case, the embedding nearest the centroid (the medoid) could be used as the point against which others were measured.\n\nIf one wishes, they can expand upon this simple ordering to produce traditional grade markings. To do this:\n\n    4. Convert the answers’ distances from the model answer into z-scores for the population of answers.\n\n    5. Translate these z-scores into some known grading scale.\n\nThough there is no generally agreed upon translation between z-scores and traditional letter grades, it is customary for individual graders or institutions to settle upon particular grading curves. That is, the mean score may be set to a B-, corresponding to a z-score of X, and so on. Translations could also be made to other grading scales, such as pass-fail, where a pass is a z-score over some value Y.\n\nThe code for the above method, including sample translations between z-scores and traditional letter grades can be found at https://github.com/colarusso/free-response-sc oring. This implementation stands upon a foundation of prior work, including the Python programming language, various mathematics, statistics, analytics, and NLP tools—Pandas, NumPy, SciPy, scikit-learn, Pingouin, and SpaCy (Van Rossum & Drake, 1995; McKinney et al., 2010; Harris et al., 2020; Virtanen et al., 2020; Pedregosa et al., 2011; Vallat 2018; and Honnibal & Montani, 2017).\n\nThe implementation encoded at the above link allows for the production of embeddings based on a selection of popular language models, including word2vec, DistilBERT, and RoBERTa. The feature space used to represent an answer, and against which one calculates distance, could include additional features such as reading level and other easily computed measures. The inclusion of any features, however, should be dependent on their ability to capture information relevant to an answer’s quality. Additionally, their contribution should be weighted based on their relative importance.\n\nAs discussed above, the ability to encode information relevant to an answer’s quality is necessary for the successful application of this method. An inability to capture the entirety of this information will lead to predictable errors. For example, a language model such as word2vec may fail to register the similarity between two texts expressing the same idea when one of the two makes use of idioms (Mikolov, 2013). One might call this class of problems the elephant in the room. Consequently, we can expect that output of the described method will predictably fail in such situations. This calls to mind the George Box quote observing that “all models are wrong, but some are useful.” From this observation, we can draw two actionable insights.\n\n    ● Because models are wrong, their output should start, not end, discussion.\n\n    ● To determine if a model is useful, one must ask “compared to what?”\n\nThese insights suggest features one should apply when using the method as well as a means for evaluating its efficacy.\n\nThough the method described above is capable of producing scorings for individual answers in a population of answers, the known limitations of such suggest that these markings should not be the final word. Rather, while such limitations persist, it is prudent to present these markings for evaluation by a human grader before they are finalized. Therefore, one can expand upon the method by adding the following steps.\n\n    6. Order the answers according to their markings.\n\n    7. Present this ordered list to a human grader or graders for review.\n\nThe nature of the presentation should be designed to provide the human grader with a grading experience superior to current practice. One such presentation may be one where the graders are able to easily reorder the ordered list of machine-scored answers. Ideally, this reordering would be accompanied by an automatic rescoring of the relevant text(s). For example, a grader could be presented with a screen consisting of multiple columns corresponding to grade markings (e.g., A, B, C, D, and F) with answers occupying the columns based on their machine score. Graders could then move answers up or down within a column or across columns such that their order represented their relative quality. A text’s position in this ordering would then determine its score. Alternatively, graders could enter a score directly, absent the need to manipulate an answer’s position, prompting an automated reordering of the list as needed.\n\nAnswers for which the feature space fails to capture relevant content would likely present as outliers (e.g., those subject to the elephant in the room problem discussed above). That is, they would likely receive poor marks and exist at the far end of the grade distribution. This suggests that outliers be watched carefully so that they may be moved manually by a person capable of evaluating the relevant content.\n\nAdditionally, there is no hard constraint on the grouping of answers into traditional letter grades. The ordering presented to a grader could divide answers into alternative groupings, e.g., pass and fail. Such a division would likely mark the overwhelming majority of answers as pass, allowing graders to quickly review and assess the scorings for the minority of answers marked fail.\n\nGiven that the method described takes an answer to be part of a population of answers and that it is within the context of this population that final human grading takes place, it seems appropriate to evaluate the method’s efficacy at this level. Consequently, the method will be considered effective to the extent that it is capable of producing an ordered list of answers which resembles the ordered list a human grader would produce.\n\nTo test the efficacy of the above method, more than one thousand student answers to a set of thirteen free response questions, drawn from six Suffolk University Law School final exams, taught by five instructors, were run through the algorithm described above, producing ordered lists of machine-marked answers. This corresponds to steps one through four above as these answers were graded by human instructors before being run through the method described. That is, the grader could not have been influenced by the machine scores. These responses and their associated grade data were acquired and this research conducted after approval of Suffolk University’s Office of Research and Sponsored Programs (ORSP) which oversees all human subject research at Suffolk University. These thirteen questions constitute all but one of the essay/free response questions present on the six exams obtained. The one excluded question was excluded because it was open-ended and did not ask for a single correct answer. Rather, it asked students to choose and discuss a subset of cases covered in the course. Consequently, “good” answers would not necessarily have looked similar.\n\nAgain, the code used to conduct the scoring described here is available at https://github.com/colarusso/free-response-scoring. One can also find three of the six exams at this link. The results described here made use of the code’s default parameters (i.e., a word2vec language model with the model answer set at the centroid). Each set of answers was run through the method 100,000 times and the output lists were compared to the ordering obtained by the students’ actual grades.\n\nTo avoid overfitting, the code used was originally tested and tuned on free response answers from one of the author’s classes, and these texts were not included in the set of evaluation texts discussed below.\n\nFree response answers were chosen for inclusion based on their character. That is, they had to ask for “good” answers that looked similar. Open-ended questions designed to elicit a constellation of dissimilar “good” answers (analogous to the short story example above) were excluded. This resulted in the exclusion of the question asking students to choose and discuss a subset of cases.\nResults\n\nTo determine how similar a list’s order was to that produced by a human grader, the lowest number of neighbor swaps needed to transform the ordering of a list into that of the human ordering was calculated (Gupta, 2021). That is, when two adjacent exams in an ordered list change positions this movement is called a swap, and it is only through swaps that exams can be reordered. Consequently, the minimum number of swaps needed to reorder a list so that it matches that produced by the human grader provides information on the quality of the initial ordering.\n\nThis measure was chosen primarily for its similarity to the task of reordering asked of human graders in the application envisioned above. That is, a perfect machine ordering would not require a human grader to rearrange any of the answers while a lower quality ordering would require many swaps as a grader moved texts up or down the ordering.\n\nAfter converting both human and machine markings into z-scores, the average number of swaps needed to transform an ordering into that of the human grader were found for the machine ordering and the pseudo-random shuffles across 100,000 runs of each exam question. The averages of the swaps needed were then compared. A summary of the results can be seen in Table 1.\n\nFor the data presented in Table 1, the Cohen’s d for the number of swaps needed between the average pseudo-random ordering and the average machine ordering is 1.03, which is large (Cohen, 1988). The p-value for a paired t-test of the two populations’ swaps, with the pseudo-random group acting as the untreated group and the machine-grader acting as treatment, came to 0.000000334, allowing us to reject the null hypothesis that the machine's ordering is equivalent to a random shuffle.\n\nConsequently, these results suggest that the method described is capable of conducting an unsupervised scoring of free response answers across a variety of course sizes and instructors. It is by no means a perfect scoring, but it is hoped that it may be sufficient to improve the process of grading free response questions through application of the methodology described above, additional feature engineering, and further parameter tuning.\n\nFor readers interested in how well the method agreed with human graders, the mean intraclass correlation, ICC(3,1), for the method paired with a human grader was 0.38, and the median was 0.41. See Table 1. The mean and median quadratically weighted Cohen’s kappa for rounded z-scores came to 0.40 and 0.45 respectively. To address the fact Cohen’s kappa expects ordanal markings, z-scores were multiplied by 10 and rounded to the nearest integer to produce ten categories per standard deviation. See Table 1. The observed performance straddles the boundary between fair and moderate agreement (Landis & Koch, 1977). It is, however, arguably within the range of agreements found for human graders of essay questions (see e.g., Gugiu et al. 2012, summarizing past reported human agreements). It is worth noting that the literature on agreement between human graders of free response questions shows a good deal of variability, with both high and low agreements. Consequently, it is not difficult for our method to find itself in this range. Consider, for example, the conclusion reached by G.C. Bull who found that the random assignment of grades would be almost as helpful as those applied by human graders (Bull, 1965).\n\nThe performance of the method was examined across a number of different parameters, including the means of vectorization (i.e., word2Vec, DistilBERT, and RoBERTa) and scoring scales with varying granularity (e.g., continuous and categorical). Results from such runs can be found along with code and data at: https://github.com/colarusso/free-response-sc oring. These permutations resulted in statistically significant differences between the number of swaps needed to transform pseudo-random and machine orderings into that of the human graders.\nFuture Work\n\nMore work is needed to see how such a method’s performance varies given different population sizes and feature spaces. There is likely a good deal of performance to be gained through additional feature selection and parameter tuning. The application of the method as an aid in the human grading process should be evaluated explicitly. That is, answers should be run through the entirety of the seven steps described above. This evaluation should look to see to what extent the method described reduces the time needed for grading, to what extent the presentation of pre-ordered lists biases graders, and under what conditions such bias would be acceptable. That is, there is the possibility that graders, presented with pre-ordering, will simply adopt the machine’s ordering, failing to act as an independent check. Depending on the use case, including the sensitivity required of markings and the cost of miscategorogrization, this may or may not be acceptable.\n\nGiven that the current incarnation of this method achieves performance approaching that of the worst human graders, the author believes it would be irresponsible for it to replace human graders at this time. Rather, they suggest its use as a cognitive exoskeleton to help balance the load of grading, allowing human graders to do more (e.g., the adoption of more formative assessment).\nAcknowledgments\n\nI would like to thank the Editor and the Chief, Kathleen and Charles Colarusso, for a life-long reservoir of support and feedback, as well as Jessica Colarusso for her partnership in all things.\n\nThanks is also owed to professors Sarah Boonin, Erin Braatz, John Infranca, Jeffrey Lipshaw, and Sharmila Murthy for providing access to historic exam data.\n\nReferences\n\nBasu, Sumit & Jacobs, Chuck & Vanderwende, Lucy. (2013). Powergrading: a Clustering Approach to Amplify Human Effort for Short Answer Grading. TACL. 1. 391-402. 10.1162/tacl_a_00236.\n\nBurrows, S., Gurevych, I. & Stein, B. The Eras and Trends of Automatic Short Answer Grading. Int J Artif Intell Educ 25, 60–117 (2015). https://doi.org/10.1007/s40593-014-0026-8\n\nBull, G. M. (1956). An Examination of the Final Examination in Medicine. The Lancet, 271, 368-372.\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences. Routledge. ISBN: 9780805802832 (2nd ed., pp. 26027)\n\nDiamond, Jared M. (2005). Guns, germs, and steel : the fates of human societies. New York :Norton,\n\nGugiu, M. R., Gugiu, P. C., & Baldus, R. (2012). Utilizing Generalizability Theory to Investigate the Reliability of Grades Assigned to Undergraduate Research Papers. Journal of MultiDisciplinary Evaluation, 8(19), 26–40.\n\nGupta, Shivam (2016, November 25). Number of swaps to sort when only adjacent swapping allowed. GeeksforGeeks. https://www.geeksforgeeks.org/number-swa ps-sort-adjacent-swapping-allowed/\n\nHarris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Cournapeau, Wieser, E., Taylor, J., Berg, S., Smith, N., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M., Brett, M., Haldane, A., Fernández del Río, J., Wiebe, M., Peterson, P., Gérard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., & Oliphant, T. (2020). Array programming with NumPy. Nature, 585(7825), 357–362. https://doi.org/10.1038/s41586-020-2649-2\n\nHonnibal, Matthew and Montani, Ines. spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing. To appear, 7(1), 2017.\n\nKluyver, T., Ragan-Kelley, B., Pérez, F., Granger, B., Bussonnier, M., Frederic, J., Kelley, K., Hamrick, J., Grout, J., Corlay, S., Ivanov, P., Avila, D., Abdalla, S., Willing, C., & Jupyter development team. (2016). Jupyter Notebooks – a publishing format for reproducible computational workflows (F. Loizides & B. Scmidt, Eds.; pp. 87–90). IOS Press. https://doi.org/10.3233/978-1-61499-649-1- 87\n\nLandis, J. R., & Koch, G. G. (1977). The Measurement of Observer Agreement for Categorical Data. Biometrics, 33(1), 159-174.\n\nMcKinney, W. (2010). Data Structures for Statistical Computing in Python. Proceedings of the 9th Python in Science Conference, 56–61. https://doi.org/10.25080/Majora-92bf1922-00 a\n\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. Advances in Neural Information Processing Systems, 26. https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html\n\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesna, E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12(Oct), 2825–2830.\n\nVallat, R. (2018). Pingouin: Statistics in Python. Journal of Open Source Software, 3(31), 1026. https://doi.org/10.21105/joss.01026\n\nVan Rossum, G., & Drake Jr, F. L. (1995). Python reference manual. Centrum voor Wiskunde en Informatica Amsterdam.\n\nVirtanen, P., Gommers, R., Oliphant, T., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., van der Walt, S., Brett, M., Wilson, J., Millman, K., Mayorov, N., Nelson, A., Jones, E., Kern, R., Larson, E., Carey, CJ, Polat, I., Feng, Y., Moore, E., VanderPlas, J., Laxalde, D., Perktold, J., Cimrman, R., Henriksen, I., Quintero, E.A., Harris, C., Archibald, A., Ribeiro, A., Pedregosa, F., van Mulbregt, P., SciPy 1.0 Contributors. (2020). SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17(3), 261-272. https://doi.org/10.1038/s41592-019-0686-2\nCompeting Interest\n\nThe methods described here are the subject of USPTO Patent Application Serial No. 17/246,563 which the author has assigned to Suffolk University under an agreement to share any potential revenue.\nSupplemental Materials\n\nAn implementation of the above methods, a more detailed description of data cleaning, and questions from three of the six exams can be found on GitHub at: https://github.com/colarusso/free-response-scoring\n\nTable 1. Summary data for graded texts showing the: (1) number of answers; (2) average size of texts in words; (3) average number of swaps for 100,000 pseudo-random orderings; (4) the average number of swaps for 100,000 machine orderings; (5) the percent difference between the two average swaps; (6) ICC(3,1) between the human and machine z-scores; and (7) quadratically weighted Cohen’s kappa between the human and machine scores where the z-score were first multiplied by ten and rounded to the nearest integer to convert continuous into ordinal data.\n",
    "model": "gpt-4o-mini",
    "temperature": 0.7,
    "max_tokens": 250,
    "output": 0,
    "json_mode": 0,
    "output_to": 4,
    "behavior": "Summarize & question paper",
    "hide_button": false
  },
  "Digital Curb Cuts (Steenhuis & Colarusso, 2021)": {
    "prompt": "Digital Curb Cuts: Towards an Inclusive Open Forms Ecosystem\n\nQuinten Steenhuis  & David Colarusso\n\nABSTRACT\n\nIn this paper, we focus on digital curb cuts created during the pandemic: improvements designed to increase accessibility that benefit people beyond the population that they are intended to help. As much as 86% of civil legal needs are unmet, according to a 2017 study by the Legal Services Corporation. Courts and third parties designed many innovations to meet the emergency needs of the pandemic: we argue that these innovations should be extended and enhanced to address this ongoing access to justice crisis. Specifically, we use the Suffolk University Law School's Document Assembly Line as a case study. The Document Assembly Line rapidly automated more than two dozen court processes, providing pro se litigants remote, user-friendly, step-by-step guidance in areas such as domestic violence protection orders and emergency housing needs and made them available at courtformsonline.org. The successes of this project can extend beyond the pandemic with the adoption of an open-source, open-standards ecosystem centered on document and form automation. We give special attention to the value of integrated electronic filing in serving the needs of litigants, a tool that has been underutilized in the non-profit form automation space because of complexities and the difficulty in obtaining court cooperation\n\nI. INTRODUCTION\n\nOur country has an access to justice crisis. As much as 86% of civil legal needs are unmet by the legal system.  In this paper, we explore (1) the adoption of digital curb cuts, spurred by the COVID-19 pandemic; (2) how such interventions can mitigate the access to justice crisis; and (3) the effects the broad adoption of such interventions could have on legal practice post-pandemic. The online equivalent of their physical cousin, digital curb cuts are interventions that appear aimed to improve access for a specific population, but upon further examination, clearly have broader benefits.  Cities install physical curb cuts for wheelchair users, but they benefit parents pushing strollers and pedestrians of all abilities.  In the same way, digital curb cuts have proliferated during the pandemic to allow remote participation in court but would remain valuable post-pandemic, especially for those with disabilities, childcare needs, or inflexible work schedules. When intentional, these curb cuts are the product of universal design, an approach that aims to produce built environments and systems that provide access to the greatest number of individuals regardless of their ability, disability, age, income, and other unique needs.  For many, court accessibility has always been a barrier. COVID-19 has, however, exposed for many the universal benefit of addressing this need.  The legal community should take heed. In this paper, we describe a vision for a more inclusive ecosystem of technology solutions aimed at improving access to the courts and justice.\n\nWe will focus on a case study of Suffolk Law School’s Document Assembly Line project and its Massachusetts implementation—Court Forms Online.  The Document Assembly Line has rapidly developed over two dozen mobile-friendly interactive legal applications, providing step-by-step guidance that can be accessed from any internet-connected device at any time of day or night. The future of law practice must embrace this and similar tools that facilitate direct communication between courts, clients, and advocates, and the law will require practitioners who understand and are ready to leverage their operation.\n\nThe Assembly Line Project succeeded because it embraced the tools and processes at the center of the information technology revolution.  We recognize that the work of attorneys, legal aid providers, and courts is the work of knowledge workers. The Assembly Line Project is staffed primarily by a volunteer team of attorneys, designers, software engineers, advocates, and law students from five continents.  To date, these volunteers have donated thousands of hours of labor. This global collaboration and the rapid deployment of complex tools were made possible largely through open-source software and modern agile project management techniques.\n\nWe will discuss how open-source technology has accelerated private-sector innovations and how courts can play a role in expanding access to justice by embracing many of these lessons—chiefly, a commitment to open interoperability for e-filing and data standards more broadly. We will also explore the role project management and aspects of the Agile method can play in facilitating the implementation of new technologies.\n\nDigital forms projects are not new. We find ourselves expressing the same optimism and embracing part of the same vision of the future as our predecessors 10, 20, and 30 years ago.  The vast majority, including those with robust question-and-answer driven digital forms, however, require litigants to print and deliver the completed form by hand.  While these projects often fail to remedy this last mile problem, there is a history of forms projects that have tried to use e-filing to cross this divide.  Courts, however, have failed to embrace such projects, presumably because of constrained resources and their perceived complexity.\n\nLessons from the information revolution and the experience of court operation during the pandemic suggest actions the court can take to better foster the creation of such projects, often with relatively little effort by courts.  Courts that adopt open standards for direct integration with interactive legal applications could accelerate the creation of a truly accessible online-forms ecosystem and lead to a world where equal access to courts is no longer the main barrier to justly resolving civil disputes.\n\nUsing the Assembly Line Project as a case study, we will explore how the broader adoption of digital curb cuts could play out for access to justice, the practice of law, and the role of attorneys in such a connected world. Section II will briefly summarize past attempts to improve access to justice, with a focus on digital solutions—primarily interactive applications that help litigants complete court forms. Section III will describe the Assembly Line Project and its Massachusetts implementation, Court Forms Online, along with brief discussions of other partnerships. Section IV will describe our vision for an open-source, open-standards form ecosystem and how implementing it could decrease the access to justice gap. Section V concludes.\n\nII. CLOSING THE ACCESS TO JUSTICE GAP\n\nThe access to justice gap describes the gap between the minority of individuals who receive adequate legal assistance and the majority who do not.  The focus is on areas of civil legal need because, unlike representation in criminal matters where defendants are provided counsel as a matter of right,  there is no federal right to counsel in civil matters.  The best survey on this topic, performed by the Legal Services Corporation in 2017, found that as much as 86% of civil legal needs go unmet.  The largest areas of need include housing, consumer debt, family law, and immigration.  The need for civil legal help for the poor has been recognized for more than a century, though modern attention to this problem is tied closely to Lyndon Johnson’s Great Society and the foundation of the Legal Services Corporation.   Since then, strategies to close the access-to-justice gap have focused primarily on increasing the number of lawyers available to help low-income litigants. This has included direct funding of civil legal aid and a movement towards so-called Civil Gideon, as well as efforts to experiment with alternative legal representation such as limited assistance representation and mandatory or highly encouraged pro bono service. It is worth noting that some states mandate representation for a subset of civil matters.\n\nGiven the immense scale of unmet legal needs, it is not even clear that providing representation to all of those facing legal issues is possible.  We also need to think carefully about whether most consumers want one-on-one assistance from a lawyer for all their legal needs. Just as Americans have turned away from tailors and cobblers, some Americans may be happier and better served with less expensive do-it-yourself alternatives to full representation.\n\nAccess to justice need not mean access to an attorney, particularly via traditional full-service retainers.  To stretch the resources of understaffed legal aid organizations, a number have relied on telephone advice hotlines  and brief service clinics.   Legal aid organizations, court service centers, and community organizations have a long history of producing written materials, including books and websites, that provide legal information and guidance.  For a long time, these have also included simple paper forms with instructions. While legal information on its own might solve only a few legal problems, more recently, legal aid organizations have innovated by creating a wide range of interactive tools that deliver legal information and assistance while helping litigants avoid mistakes.\n\nOver the past two decades, the Legal Services Corporation has helped fund a constellation of technology projects aimed at addressing the access to justice gap through its Technology Initiative Grant Program.  A requirement of this funding was the mandate that grantees attend an annual conference where they could share the results of their work.  As early as the year 2000, this conference began to build a community of technologists and practitioners focused on solving the needs of low-income civil litigants. This community predates the civil tech movement that swept executive branch agencies in the 2010s.\n\nIt is worth comparing the nature of these executive actions with the approach and resources available. Executive branch agencies are nimbler than the judiciary, the latter lacking a chief executive able to quickly steer the ship of state.  Additionally, executive branch civic tech has embraced the digital services model,  bringing technologists into government both to implement technology projects and to help coordinate vendor interactions, whereas courts have continued to rely largely on outsourced technical expertise.\n\nThe civic technology world, growing up in the age of technology giants like Google, Facebook, and Amazon, has drawn more heavily on a professionalized technology corps than the legal technology access-to-justice space.  On the legal side, it has been more common to draw on lawyer-coders than professionally trained technologists.\n\nA comprehensive history of civil legal tech is beyond the scope of this paper although several illustrative accounts exist.  We will restrict our exploration here to direct antecedents of the Assembly Line Project and a sample of adjacent offerings in the private sector.\n\nA. Technological solutions: online forms, guided interviews, and the birth of the interactive legal app\n\nLauritsen and Steenhuis  use the term “interactive legal application” or “app” to describe a wide range of tools that: (1) ask a series of simple fact-based questions; (2) ask relevant follow-up questions and skip irrelevant questions; (3) use the factual information provided by the user to correctly complete a legal form, provide tailored information, or provide another legal service; and (4) provide a just-in-time learning component to enable a user to make informed choices while completing a legal form. These tools are also called wizards, guided interviews, or sometimes simply document automation tools. American readers are usually familiar with TurboTax, a document automation tool that assists with correctly filling in United States income tax forms.  While there are many platforms to author such apps that have appeared and disappeared over time, the most prominent in the access-to-justice space are HotDocs, A2J Author, and Docassemble.\n\nThese tools have been highly effective at serving many self-represented litigants. Law Help Interactive, which hosts both A2J Author and HotDocs forms, serves as the largest host for these interactive legal applications, assembling half a million forms for self-represented litigants each year.\n\nB. Platforms for automating justice\n\nThe world of interactive legal apps is popular enough that several dedicated software applications help software developers build legal-focused question-and-answer-driven applications. There are too many platforms to enumerate here.  Below we briefly summarize some of the most popular apps, both for-profit and non-profit.\n\n1. HotDocs\n\nHotDocs, one of the earliest tools for automating legal forms, is still widely used today.   It was originally designed as a desktop application. In the most widely used version in the access to justice space, HotDocs Server version 11, has been ported to the Internet, but it is challenging to use on a mobile device. Text is cramped and buttons are difficult to interact with. It offers a page-based user interface, with each screen consisting of several questions and providing free navigation among each question, akin to flipping through the pages of a multi-page form.\n\nThe traditional HotDocs program is designed for use by litigators and paralegals. It includes the ability of an author to provide contextual help and graphics. Visually, it still looks like its 1990s desktop original and does not offer the ease of use that modern consumers expect, but it has the benefit of a wide base of experienced developers.\n\nInterviews that are authored in HotDocs can be run for free on the LawHelpInteractive platform,  but all developers who want to author new HotDocs applications, including non-profit developers, must purchase a monthly subscription.\n\n2. A2J Author\n\nA2J Author is a user-friendly alternative to HotDocs. It grew out of a project to produce an automated tool to assist litigants filing for divorce in Cook County, Illinois.  The first standalone version of the tool cost “hundreds of thousands of dollars.”  The desire to produce thousands of these tools with much less effort encouraged the authors to develop a general-purpose platform. One of its prominent features is a friendly digital avatar and road metaphor. Instead of presenting a user with several questions at once, it asks one question at a time. It now includes a mobile interface.\n\nA2J Author has succeeded in its goal of producing many tools.  A2J Author classes have been taught in law schools, and frequent free training is available to help legal aid lawyers learn the tool.  The first version of A2J Author relied on Macromedia Flash, a convenient tool for slick, user-friendly application development, but one that preceded mobile phones. In 2017, A2J Author lost some momentum in the effort to become mobile-friendly but has caught up, and in its most recent iteration, it works smoothly on mobile devices using HTML5 technology.\n\nA2J Author is a product of Chicago Kent Law School and is actively developed by CALI, the Center for Computer-Assisted Legal Instruction.  Starting in 2021 A2J Author was made available under a GNU AGPL v3.0 license.  Developers can also host their interviews for free at A2J.org or on LawHelpInteractive.org.\n\n3. Docassemble\n\nDocassemble is a tool conceived of and primarily maintained by a solo developer, Jonathan Pyle, at Philadelphia Legal Assistance.   Unlike the graphical tools HotDocs and A2J Author, Docassemble is a highly flexible web framework that is heavily optimized for building interactive legal applications. It allows both rapid development and the ability to add custom code.\n\nOf the major platforms, Docassemble has by far the most permissive license. The famous MIT License allows any entity, commercial or non-profit, to use and modify the software if they include an acknowledgment of the original author.\n\nIt is common for Docassemble apps to be released in a reusable library form. To achieve this, Docassemble integrates with GitHub, a popular platform for software developers to collaborate and share code.  This allows a Docassemble author to view and integrate code by other authors that use a compatible software license. In turn, Docassemble programs frequently make use of the wider constellation of open-source libraries that are part of the Python software ecosystem.\n\nExample Python libraries integrated into Docassemble apps include: a decades-long research project that can conjugate verbs, make nouns plural, and do other advanced natural language processing in four languages;  track (and therefore adjust court dates) based on holidays around the world;  extract text from images and add Bates numbering to uploaded exhibits; and autocomplete addresses as one types based on the device's location.\n\nA Docassemble app, especially one that uses external Python features, can be thought of as a long collaboration with potentially hundreds or thousands of researchers and developers around the world. All of them releasing their work to the world for free.\n\n4. Afterpattern and Documate\n\nDocassemble's permissive license has led to the creation of two commercial platforms that provide a user-friendly overlay to Docassemble: Afterpattern, formerly Community Lawyer,  and Documate.\n\nThe added value for both products is the ease of use of a graphical environment for rapid automation by non-technical users, at the cost of waiting for the platform author to add new features. The two platforms are both similar: they provide a subset of Docassemble's features for automation of legal forms in an easy to use drag-and-drop authoring environment, together with a marketplace and some bundled integrations, including connections to payment systems, case management systems, and other tools already used by small and solo law firms.\n\nUnique to one of the two platforms, Afterpattern promoted remix culture by requiring users of its free tier to make their work accessible to others. If the author allowed, another author could import and modify the code, without losing the simplified graphical interface. This has allowed for a culture of sharing to develop. For example, during the pandemic, the Centers for Disease Control and Prevention issued an eviction moratorium that requires tenants to sign a simple declaration of their right to halt an eviction.  Kentucky Equal Justice Center, a legal aid provider, built a simple app to solve this problem on Afterpattern. That app was then remixed by legal aid providers in other states who wanted to add local resources.\n\nDocumate does not explicitly encourage remixing, but it has taken the approach of allowing advanced users of the platform to edit their code in the native Docassemble environment. That means a sophisticated author still can make use of custom Python engineering and import Docassemble packages created by third parties.\n\n5. Examples of interactive legal apps\n\nInteractive legal apps are appealing to legal aid programs and non-profits because they allow them to serve many more people than they can through direct representation. Creating a legal app is a way for a private attorney to build recurring revenue and reduce costs. While most of the private legal market still relies on the billable hour, for-profit law firms have also entered this space in innovative ways. Forward-thinking law firms of all sizes have begun to build their own legal apps.\n\nThe following is a brief list of notable interactive legal apps, including both for-profit and not-for-profit.\n\na. LegalZoom\n\nLegalZoom is an early commercial pioneer in building legal apps.  LegalZoom offers a wide range of question-and-answer-driven legal forms for simple situations, from incorporation to wills and trusts. The cost of tools starts at around $100.  Their primary product includes the review of completed forms by attorneys. Attorney review is also available, either bundled or as an add-on service.\n\nb. Hello, Divorce\n\nOwner Erin Levine ran a small family firm in California before joining Duke Law School's legal startup accelerator program.  The Hello, Divorce tool complements her existing family law practice by offering a monthly subscription that includes basic forms to file for divorce. The subscription price ranges from about $100 each month for unlimited form use up to a flat fee of $4,500 for forms over the lifetime of the case with added assistance.  Compared to the average cost of a divorce ($15,000),  this is an affordable option for many families.\n\nThe Hello, Divorce website focuses on a great deal more than the interactive legal app that assembles the basic forms, which they brand the Divorce Navigator. It includes numerous articles and videos aimed at helping litigants navigate the divorce process, many of which are not behind a paywall. These serve both as a value add for subscribers and content marketing.\n\nHello, Divorce was built primarily on the Documate platform, which as we have noted is itself built atop Docassemble.\n\nc. Upsolve\n\nUpsolve, a tool that helps people with low incomes file for bankruptcy, is an exemplar of a non-profit built around an interactive legal app. It was recently named one of the top inventions of 2020 by Time Magazine.  Like Hello, Divorce, Upsolve goes beyond legal forms to the building of an ongoing relationship centered around a product. Among Upsolve's simple innovations: an active Facebook group; use of video testimonials; a referral program that allows applicants who do not qualify based on income to help fund service for those who do; and integration with credit reporting agencies to help bankruptcy applicants save time in listing debts.\n\nUpsolve was built on the Docassemble platform, with some of the development provided by Docassemble's author, Jonathan Pyle.\n\nC. Why the existing approaches are not enough\n\nWhile the existing legal app ecosystem is astonishing, there is still a veritable mountain of unmet legal need. A lot of inefficient work is done from scratch with each new forms project. Specifically:\n\n• Existing tools do not promote code reuse. There is a lot of redundant and repetitive work:\n\n  o Variable names are reinvented by each interview author, even if the concepts are the same.\n\n  o Questions are often written from scratch too because popular platforms do not offer a way to select from standard questions.\n\n• While there is a great deal of information sharing inside the access to justice world, the work is not heavily standardized. Conventions are ad hoc and vary from developer to developer.\n\n• Work is not easily divided into discrete units. Traditional forms projects use one or a small team of automators. It is challenging for two authors to work simultaneously, even in unrelated parts of the tool.\n\n• Interactive legal apps collect a great deal of structured data, but deliver the data to the court on paper or as a PDF.\n\n• Self-represented litigants find it challenging to deliver work to the court or to perfect service.\n\nBy working together with open source tools and techniques, we can reduce inefficiencies and accelerate work to close the access to justice gap.\n\nIII. SUFFOLK LAW SCHOOL’S DOCUMENT ASSEMBLY LINE PROJECT & COURT FORMS ONLINE\n\nThe authors of this paper have worked together on a project to reduce the access to justice gap that fully embraces open source and agile development techniques during the pandemic: Court Forms Online. Court Forms Online is a website that provides residents of Massachusetts interactive access to court forms.  The forms provide step-by-step guidance for several legal processes. These tools provide more than just form-filling. The experience is closer to sitting across the table from an experienced attorney than filling in a blank form on paper.\n\nThe most complex tool is a domestic violence petition for a protection order, which generates eight forms addressing custody, child support, and related issues in addition to the core protection order.  Perhaps the simplest of the tools allowed a tenant anywhere in the United States to determine if they qualifed for protection by the Centers for Disease Control and Prevention’s September 2020 order creating a moratorium on evictions and helps the tenant send a declaration protecting their rights to their landlord.\n\nThe website was built as a part of the Assembly Line Project, an international collaboration led by the Legal Innovation and Technology Lab at Suffolk University Law School, in March of 2020 in partnership with the Massachusetts Access to Justice Commission. About 200 individuals from five continents participated in the project, including volunteers from Suffolk Law School, Greater Boston Legal Services, Massachusetts Law Reform Institute, Code for Boston, Northeast Legal Aid, Theory and Principle, the NuLawLab at Northeastern University, AppGeo, Bentley University Court UX Team, Documate, and AfterPattern.\n\nA. A call to action\n\nCourt Forms Online began in response to a call to action from the late Chief Justice of the Supreme Judicial Court of Massachusetts, Ralph Gants.  “Thou shall not ration justice,” the letter begins,  quoting Judge Learned Hand. Justice Gants urged the bar to rise to the challenge to find ways to keep serving the public when physical buildings were closed.  “We know many are being hurt: defendants in custody awaiting trial, crime victims awaiting closure to their ordeal, and children in limbo awaiting custody or adoption decisions, to name only a few. And we know we must do what we can to limit their pain.”\n\nThe urgent need caused by the pandemic was clear. Courts would remain open for emergency business, but the actual buildings would not. Would-be litigants trying to get protection against abusers or abusive property owners would be stuck outside in the cold. With survivors of domestic abuse, this was literal. An early story from survivor support agencies was of a woman who waited for hours on the courthouse steps in late winter of 2020 before receiving a blank set of forms and no additional assistance to fill out an abuse protection petition.\n\nIn response to the Chief’s call, Suffolk Law Dean, Andrew Perlman, asked members of the law school’s Legal Innovation and Technology community to brainstorm how we could help. Given the physical closing of the courts, we immediately drew inspiration from the success of the online software tool MADE, a robust step-by-step process to help tenants respond to an eviction case, created by co-author Quinten Steenhuis during his time at Greater Boston Legal Services.  MADE was built over the course of about a year and leveraging about $100,000 in staff time. That would be too slow to help during the pandemic. If we helped, it would be necessary for us to find partners and processes that could multiply and speed up our work. Given our experience working with teams of students and on distributed collaborations such as open-source software, we came to believe the school’s Legal Innovation and Technology (LIT) Lab, which we staff, and later the Massachusetts Access to Justice Commission’s COVID-19 taskforce, could serve as conveners and guides for a volunteer effort.\n\nImmediately following the Chief’s call, the COVID task force contacted us with an ask for work similar to the results of our brainstorming. The Lab’s director, David Colarusso, was named a co-chair of the taskforce’s access to courts committee, and the Lab partnered with the Commission to create the Document Assembly Line Project. Our goal became to automate about 30 core processes, identified by the Trial Court as emergencies, and allow for their e-filing with the court.  Later in the pandemic, the Appeals Court collaborated with us on more processes, resulting in an administrative order naming the Assembly Line’s work as an official means of filing.  Throughout the pandemic, the Assembly Line has also created a collection of pro se materials independent of our agreements with the courts not intended for e-filing.\n\nB.  An assembly line for automated legal help\n\nTo speed up development, we thought carefully about what tools and skills would be needed for the project to succeed. We coalesced around a simple concept: we would divide the process of automating a form into several small steps that could be done independently, and then put out a call for volunteers. We chose a few tools early on. First, we chose the Docassemble platform.  LIT Lab staff were already familiar with the platform and had seen its success with Upsolve and MADE. Docassemble is built with the needs of software developers in mind. That means a steeper learning curve. However, it also makes Docassemble easier to automate. The fact that Docassemble is released under an open-source license also weighed heavily in the decision. The LIT Lab, which often works on software projects, defaults to releasing work under permissive terms, preferring the MIT license where appropriate.\n\nWe used Trello, a popular web-based implementation of a Kanban board, to organize and divide work.  Kanban is one of the principal tools in agile software development.  Part of Toyota Automotive’s Kaizen approach to process improvement, it is now widely adopted in the software industry.  The concept is simple: each unit of work is assigned a card. The developer creates multiple lists or columns that represent distinct stages of production. Cards move from one list to the next as each step in the manufacturing process is completed. The benefit of a tool like Kanban is that backlogs and delays in the work are very visible. It is easy to see what parts of your workflow require improvement or optimization as cards pile up on one list without moving to a new list. These cards can be as simple as sticky notes on a whiteboard, but several software tools allow remote, digital collaboration using Kanban.\n\nWe also relied heavily on regular meetings over Zoom and used Slack as an asynchronous, quasi-real-time collaboration platform. We recorded and continue to record the majority of our Zoom meetings. As of this writing, over one hundred such meetings can be found on the Lab’s YouTube Channel.\n\n When possible, we made use of tools that were either free or freely available to us. Our use of Trello and Slack leveraged their free tier offerings, and we made use of university-issued tools like Zoom. We also expressed a preference for open source (free) tools such as Docassemble.\n\nThis focus on tooling extended to a preference for modern technology stacks and flexible and low-cost hosting environments such as Amazon’s LightSail.  In many ways, our approach paralleled that suggested by the Digital Services Playbook.\n\nC.  Dividing up work\n\nThe basic process of automating a legal form on the Document Assembly Line consists of the following steps:\n\n1. Review and categorize the form. Understand who uses it and how it can be used.\n\n2. Label the fields (blank spaces) on the form so that the computer can recognize them. Developers are provided detailed guidance and pre-determined labels for most fields.\n\n3. Create a simple draft of a step-by-step interview to provide answers to all the fields. Our major innovation is that this process is automated if the fields have been correctly labeled.\n\n4. Work with an expert to review and improve clarity and understand the logical flow of questions for the final form.\n\n5. Review language and usability of the form based on our style guide and with advice from a plain language expert.\n\n6. Further improve and customize the form based on feedback. Rinse and repeat.\n\nOne innovation we stuck with to varying degrees throughout the project was the use of standardized questions. We worked closely with experienced plain language experts  to write questions that were used again and again on our forms: questions related to basic demographic information, such as names and addresses; questions about the court and the litigant's children; and questions about potentially tricky legal concepts, such as the litigant's role in the case as either a plaintiff or defendant. When a form was automated in our process, it would automatically make use of these standardized questions. There were limits to this approach, but each new form gave us a chance to explore and find variations in our standard questions that could cover more use cases. This saved work for developers and helped us have a consistent voice across the full form library.\n\nHistorically, most of these roles would be performed by a single person or team that would continue to work on the form from start to finish. Because we used an assembly-line style approach, the work was able to better use the efforts of volunteers who were available for an uncertain amount of time or had expertise in just one part of the process.\n\nOur volunteers included members of legal aid; usability experts; employees of the court; private attorneys from around the world; members of the academic community in Australia, Ireland, and the United States; and students from law schools all over the country. Some had ties to Boston and some just wanted an opportunity to engage in meaningful work during pandemic lockdowns. Some volunteers participated for months, and some just ran a few tests for a few hours. The assembly line model helped facilitate this range of participation levels.\n\nD. How the interactive legal apps work\n\nFrom the litigant's perspective, using an app has three basic steps:\n\n1. Answer interactive questions.\n\n2. Download the completed document.\n\n3. In some cases, click a button to email the form directly to the court.\n\nTo locate the correct form, the litigant visits a website,  on either their smartphone or a desktop computer. They can type in a description of their legal problem or navigate a menu of form categories to choose the form they need.  From there, they click a link to start the interactive question-and-answer process. As they click from screen to screen, the computer choses the proper follow-up questions. For example: if the litigant said that they are the plaintiff, the follow-up question would ask for the name of the defendant. The computer also handles any background calculations, such as determining a litigant's total income and comparing it to the federal poverty level if the litigant is applying for a fee waiver. These features can increase accuracy and save the litigant a great deal of time.\n\nThe forms include other critically helpful features that were built or improved along the way. For example, we worked with a company that specializes in geographic information systems to build a robust court locator tool.  We have found that understanding what court to file in can be difficult, but knowing the litigant's address is usually enough to narrow down the choices. Our locator uses the geocoded address and returns the geographically appropriate courts, considering any rules about jurisdiction. This removes a paralyzing step from the self-represented litigant's process.\n\nAnother important feature of the tool is the ability to add a digital signature. This enables remote collaboration. If an advocate or attorney fills out the form by talking to a client over the phone, they can click a button inside the tool to send it to their client to sign.  That process can also work in reverse, allowing a litigant to send a partially completed form to their advocate to review before it is filed in court.\n\nWhen a litigant uses one of the tools that we created, they are also provided with information in-context that explains the choices they can make and the law that relates to their legal problem. This includes simple help bubbles and links to videos, diagrams, and external websites where appropriate. The tools also include basic accessibility features, such as a built-in screen reader for those with limited vision or literacy.\n\nAt the end of the question-and-answer process, the litigant reviews and confirms the information on the form is correct, adds their signature, and then downloads or emails themself a completed document. Many of the forms include instructions that specify the next steps. In some cases, if the court approved the form for emergency use, the litigant could click a button to email the form directly to the court clerk.\n\nE.  How email-based filing works—and does not work\n\nA critical component of our project was incorporating a way for litigants to deliver completed forms to the court when the court was physically closed. We knew from past projects that while an interactive legal app can help someone complete a form correctly, it is usually up to the litigant to read, understand, and apply any information that you provide about filing, service, and filing deadlines. This aspect of using an interactive form is the likeliest to cause problems for pro se litigants. During a pandemic, with courts either closed for in-person filing or with unusually restricted hours, we felt this burden was critical for us to reduce.\n\nAfter discussing the feasible options, we reached an agreement with the Massachusetts Trial Court to deliver certain critical forms by secure TLS-encrypted email. We later added an additional agreement with the Massachusetts Appeals Court for email filing. From the perspective of the courts, email required no training. From the perspective of our project, email delivery was a remarkably simple feature for us to add. However, it soon became clear that email delivery placed a burden on clerks, who would need to open and manually file each email that came in. This limited the ability of our project to expand electronic delivery beyond emergency forms to heavily used forms that have a larger impact on pro se litigants. Many of the apps on our site, therefore, allow litigants to complete and download a form but do not deliver it by email.\n\nAt this point, we began discussions centered around integrated electronic filing that would not only produce PDF forms but add a digital entry into the court's electronic docket. The Commonwealth has an existing e-filing provider, but it requires litigants to create a separate account and navigate a process that often is difficult or impossible on a smartphone.  We proposed not to replace this, but to supplement it. Our e-filing integration would swap the email button with an e-filing button, and litigants would never need to leave our app to use it. This process is well-known and adopted commercially by third parties in states around the country, although only rarely by non-profit interactive legal app vendors.  In our agreement with the Massachusetts Appeals Court, we have agreed to produce this integration as part of a later phase.\n\nPew Charitable Trusts has provided Suffolk with funding to build this electronic filing integration, which will start with a replication project in at least one other U.S. state.\n\nF. A maturity model for interactive legal apps\n\nEarly on in our project, we realized the biggest barrier for releasing timely work was our own exacting standards. We knew that our goal was to reach minimum viable product level,  but we found it difficult to scope the work and reach release when we could clearly see improvements that would help litigants have a better experience. To solve this problem, we developed a capability maturity model for legal apps.  We realized that a minimum viable product is a slippery concept. Some forms, such as the domestic violence restraining order petition, required a very complete product to be safe for users. Others, such as an affidavit of compliance with the Servicemembers Civil Relief Act, 50 U.S.C. § 3931 (2018), require much less judgment from the user and therefore could be safely automated with fewer features.\n\nOur capability maturity model addresses the difference between complexity and minimum level of safety by focusing on a set of features. We identified four levels of maturity, with a fifth quasi-level inserted between Levels 1 and 2. Importantly, we decided that most forms were ready for release at Level 1. Forms that require additional safety can be released at the appropriate maturity level, with internal releases at earlier levels helping the author gain feedback.  A form that merits it could then receive continual improvements, focusing on a release schedule that mirrors the subsequent levels. For example, a form might be released at Level 1, again at Levels 1.5, 2, and then finally stop receiving updates when it reaches maturity Level 3.\n\nThe levels are:\n\nLevel 1: Equivalent to the existing paper process, with some automation and elimination of irrelevant questions based on the user’s responses.\n\nLevel 1.5: Language is reviewed and appropriate for a 6th grade reading level.\n\nLevel 2:Equivalent to how a newly trained person would conduct a face-to-face interview.\n\nLevel 3:Equivalent to how a highly trained person would conduct a face-to-face interview.\n\nLevel 4: Equivalent to an interview by a highly trained person and includes robust features that go beyond the paper process. For example, training videos, automated reminders, and with extensive usability testing.\n\nThe Level 4 form was included in our process as a mental benchmark. A good example of such an app might be Upsolve—a tool that took hundreds of thousands of dollars and a large team to develop. Keeping this scope in mind helped us stay grounded in choosing whether to add new features or automate a new form.\n\nThe maturity model in our project became a mental model that freed us to release work at a regular cadence. We do not share this label with litigants; it is a framing mechanism that drives the prioritization of fixes and allows us to complete many forms rather than polishing and improving a small number of forms forever. This common language around the desired capabilities and sophistication of our forms was especially useful with students and new developers on our team.\n\nThe maturity model also became a useful communication tool with partners inside the court. We found that with our first court partner, it was easy to receive scattered and inconsistent feedback. By talking directly about the maturity model early on with our second partner, we improved the quality of feedback and our ability to act on feedback. Each page of the interactive app allows the user to click a button to provide feedback. This tool describes the maturity model and asks the partner to assign the feedback to the correct maturity level.\n\nG. How the project has changed the lab and strengthened our pedagogy\n\nSince its founding in 2017, the Legal Innovation and Technology Lab has worked within the context of experiential education. Housed both in the law school’s Clinical Programs and its Legal Innovation and Technology Concentration, the Lab has offered a mix of traditional classroom and experiential instruction. Students work on multiple projects, playing the role of a legal technology and innovation consultant for real world clients. Besides traditional classes taught by the Lab’s staff, the Lab's two programs incorporate students: a lab class and the LIT Fellows program. LIT fellows are embedded inside a traditional law school clinic. Alongside their traditional case load, the fellows act as internal innovation officers, identifying areas where they can improve the clinic's ability to serve clients. Students in the lab course provide a similar service to external clients such as local non-profits, public defenders, or district attorney’s offices.\n\nThe Lab also helps steer work in experiential learning courses in other parts of the law school. Students in project-based learning courses may work on a lab project as part of their course work. This work has included assisting clients with knowledge management, the creation of online apps, data analytics, and document automation.\n\nThe Lab has never sought to turn law students into software engineers. What it has aimed to do is acquaint students with the realm of the possible, to provide them with an understanding of the technology that they and their clients will encounter. To accomplish this, students work with that technology, often as parts of interdisciplinary teams.\n\nWith the advent of the Document Assembly Line Project, however, the Lab’s work shifted from that of a consultancy to something more akin to a startup. At the peak of our efforts in the summer of 2020, the Lab was working with hundreds of students and volunteers across five continents and eleven time zones. Many of these volunteers contributed only a few hours here or there, but there was a core group of a few dozen full-time team members, including three volunteer project managers. The scope of our work was no longer confined to what we thought would be appropriate for a semester’s engagement. Instead, it was dictated by the demands of a pandemic response. We no longer worked together on a single project. Instead, we managed dozens of interrelated projects. On more than one occasion we compared this task to laying track ahead of a moving train. Working in iterative sprints, students learned how to solve problems we could never have anticipated.\n\nThe pandemic disrupted the summer plans of law students as internships evaporated, and we worked to provide meaningful experiences not just for our students, but for students from law schools around the country, including Northeastern, Boston College, Boston University, Drexel, Cardozo, the University of South Carolina, and Brooklyn Law School. We found the sweet spot for student involvement often straddled the line between law and technology. We adapted materials from our classes to bring students with no prior coding experience up to speed on our tools and paired them with subject matter experts to make the interactive legal apps legally accurate and more closely match the natural interview style of an experienced legal aid worker. We joked that they were learning the law by teaching it to computers, but ultimately their audience was the user on the other end. What they learned to do was to listen, to empathize with those seeking justice, and to use the levers at their disposal to assist them on their journey.\n\nThis project offered an opportunity to implement some of the lessons we have always aimed to teach our students but on an unprecedented scale. A traditional clinical experience does not only offer you the opportunity to work with clients on real world cases. It embeds you in a firm with its own culture and which is itself a member of a larger community. Learning how you fit into that culture and the larger community of lawyers is one of the formative experiences of a young attorney’s career and perhaps responsible for the fact that so many law students discover their love for an area of practice while working in a clinic.\n\nFor the first time since its founding, the Lab really began to feel like it was part of something bigger than itself. It was finding its community, its place in the world, and its own culture. Students worked alongside attorneys, designers, engineers, and advocates as part of truly interdisciplinary teams, and the results were humbling. Despite never meeting in person, the team created work product with market value easily in excess of $1 million and made it available to the world for free. They came to our aid in our time of need, and they learned the value of service in a time of tumult. They showed us what is possible when you marry the public service mission of a clinical program with the drive of innovation. They showed us what a 21st-century clinic can be, and they did it all remotely.\n\nWhen classes resumed in the fall of 2020, the Lab pivoted to take advantage of this new understanding. Alongside existing opportunities for project work, we began to find ways to incorporate the Assembly Line. Leaning heavily on collaborative tools, we sought to promote and foster the growth of the community that had grown out of the Assembly Line. Students working on projects across classes could for the first time contribute to a larger project as part of a single team.\n\nH. The impact of the work\n\nThe CourtFormsOnline.org website has seen wide usage during the pandemic. Just a few thousand people visited in the first months when our early forms were rolled out, but between July of 2020 and January of 2021, the guided forms on the website saw more than 190,000 unique visits. One national firm that helped tenants understand if they qualify for the Centers for Disease Control and Prevention's eviction moratorium was successfully used to prepare and email a declaration more than 4,000 times and in all 50 states. Forms on the website helped domestic violence survivors get protection orders. They helped tenants get emergency actions against neglectful property owners. A dozen forms touch on different parts of the ongoing housing crisis—from motions to dismiss to claims for affirmative relief; from security deposit claims to defending against eviction.  In at least one case, a form was used to stop a physical eviction. The constable was present, and the user’s property was being removed from their home. During this process they used one of our forms to ask for a stay and it was granted, stopping the evection and returning their property to their home.   And other forms just helped attorneys with the time and ability to file limited appearances in court.\n\nBeyond the numbers and the specific relief, the forms give access to, the availability of remote any-time assistance for key court forms is hard to overstate. One tenant, using a tool to help defend against an eviction action, submitted an intake to legal aid at 12:30 a.m. No legal aid office or telephone hotline is available at 12:30 a.m. Pandemic relief is important. These needs will continue after the pandemic has ended and existed before the beginning of the pandemic.\n\nLong-term sustainability is an important goal for the Lab. Massachusetts Law Reform Institute, which maintains the state-wide Massachusetts legal information website, was among our most important partners, working to help author and link to our forms. When we chose forms to automate beyond the initial emergency forms identified by the court, we worked closely with MLRI to select forms solving problems that were among the most visited pages on the legal information website and that fit into their long-term mission.  The work that we did to embed this work into our course and lab work was also part of our sustainability plan. A legal tool requires regular updates to accurately reflect the law, and we believe the process behind this maintenance has pedagogical value for our students.\n\nThe pandemic was an accelerant for our work and our success. It provided access to a large pool of committed volunteers and a court that recognized the need for urgent action. When projects like this require accessing new funding or convincing a skeptical court, success may not be as rapid. The challenge of waiting for an at times slow-moving court led us to work on projects that could be successful without the court's cooperation and to identify community groups independent of the court that would both adopt and champion our tools. We believe that success can come for a project with even limited resources by leveraging both the tools that Suffolk built and learning from the lessons we gained through hard practice.\n\nIV. A VISION FOR AN OPEN-SOURCE OPEN-STANDARDS FORMS ECOSYSTEM\n\nIf the successes of the Document Assembly Line project were to spread, what would that success look like? What would help it succeed? The last 40 years have been full of predictions for the transformation of the legal industry through the adoption of technology. Pilot projects have succeeded without yet causing widespread lasting change.  In this section, we describe a vision for an open-source forms ecosystem. Specifically, we propose three key lessons of the Document Assembly Line project that can be replicated and expanded to increase access to justice:\n\n1. The adoption of open source, or at least permissive open-source style, remixing of both code and common shared elements.\n\n2. The use of standardized work processes, including assembly-line style development.\n\n3. The widespread adoption of open standards, including those for electronic filing or email delivery.\n\nThese three suggestions are both modest and radical. Modest because not only do these suggestions save time, but they are free to implement. Radical because a model based on shared code has been out of reach for the last 20 years even as individual form projects have been quite successful.  We have seen these lessons applied to the private sector and to an extent in the civic\n\ntech movement. Now we need to extend those lessons to how we address the access to justice problem.\n\nAn organization that wants to adopt our recommendations need not start from scratch. As with a franchise, interested “partners” may adopt our core technologies and operating procedures, allowing them to learn from our work and pick up where we left off in another jurisdiction. Unlike a franchise, however, our work is available for free and comes with an unrestrictive license to modify and adapt it.\n\nA. How standards are born\n\nFigure 1: XKCD #927, Copyright 2021 Randall Munroe (Available online at https://xkcd.com/927/ [https://perma.cc/N7DT-WE7E]).\n\nStandards and conventions facilitate communication.   There are many paths to standards. Some standards are created by official entities, which may include committees, such as the International Standards Organization (ISO)  or governments. Others are created by fiat: they become standards because a dominant market player has adopted them and other participants in the market wish to maintain compatibility with the market leader. Rarely, standards are developed without a committee and a single market leader.\n\nUnless they are mandated by a government entity with the power of force, standards gain adoption when they are sufficiently useful. Often, they are useful only when they are widely adopted.\n\nThe Document Assembly Line project lacks the force of either an official committee or a government agency. Yet we believe that our project's demonstrated value and the free tools that we are building can lead to a successful replication. Two jurisdictions are working directly with Suffolk to replicate our work: Louisiana and Illinois. As part of our onboarding of those two jurisdictions, Suffolk hosted an open four-week “bootcamp” in February of 2021 that drew more than 50 developers from around the world. These developers represent legal aid programs, private developers, and nonprofits across the United States and three continents. The project offered training in the tools that we built and the processes that we used to launch the Assembly Line. We hope that this is just the beginning.\n\nB. On the shoulders of giants\n\nFigure 2: XKCD #353, Copyright 2021 Randall Munroe (Available online at https://xkcd.com/353/).\n\nIn Figure 2 above, Cueball's excitement at the world of possibilities created by Python's open-source libraries is palpable. We confess having the same excitement when exposed to the Docassemble platform after having programmed in HotDocs and other graphical tools. Each completed Python library becomes like a Lego building block that you can use to build a bigger project. To benefit from its power, you can just add it to your project. You do not need to know how to inject plastic to make a perfect little block. In some ways, open-source  coding is also like the common law.  Once the precedent is set, we can instead work on addressing just the novel aspects of whatever we are building.\n\nOpen source is about both give and take. Borrow open-source libraries, and the norm is that you contribute some of your work back to the collective commons. This makes open source an amazing tool for accelerating development. The more developers who use an open-source tool, the better it can become with less work by each individual contributor. Among the best-known open-source projects in the world, Linux, the operating system with the most users worldwide,  is contributed to by a wide range of authors, from hobbyists scratching a personal itch to Fortune 500 companies.\n\nThe open-source culture is not only about sharing code.  A vision for an open-source forms ecosystem could also share things like prewritten translated, 6th-grade reading level questions to cover the basics, such as the litigant's name and addresses; the variable name schema, which controls both how information is collected, and the internal names assigned to data that the app collects; and conventions like the structure and order of questions. The same economies of scale that helped the Assembly Line project succeed in rapid automation within one organization could spread across the entire ecosystem.\n\nAnother way that the sharing of ideas can help is in developing a common look and feel for interactive legal applications. The wider website and app space are both seeing an explosion of different visual metaphors, particularly when compared to the relative sameness of desktop applications in the 1990s and early 2000s. This wide variety, while visually exciting, is not helpful for people forced to use multiple platforms that each have their own look and feel.\n\nWhile question-driven apps that assemble forms are not new, litigants, especially in the target population for access to justice initiatives, will benefit when they can see a common set of conventions across the many independent interviews they may need to use. These conventions might include a common visual language and structure, like the placement of opening screens, menus, labels for buttons, terms of use, and conventions for downloading and sending completed documents electronically. The New York State Bar Association has started to develop a list of standards for these tools, which they call Best Practice Guidelines.\n\nA mutually beneficial open-source forms ecosystem does not need to be proscriptive. Sharing of variables, structure, and pre-written questions, so long as they save developers time and are useful, will naturally help tame the wild landscape of interactive legal apps. This is especially true when the examples that gain popularity have been vetted by usability experts and end-user testing. Developers will benefit from reduced cost and time, and litigants will benefit from reduced cognitive load while switching from one platform to another. While the individual terms might be debated, this common visual language will help litigants be able to use and understand each new app more quickly.\n\nWe do not claim to be proposing something entirely new here. The Innovations in Technology Conference, for example, is one way that the access to justice community already convenes annually to share best practices and talk about projects, and their support has been instrumental to the success of existing tools such as A2J Author.  Another place for informal collaboration is the Legal Services National Technology Assistance Project (LSNTAP) email list.  Rather, we propose that allowing authors to directly share code and model questions may be a catalyst to turn informal idea-sharing into replication.\n\nC. The value of agile development\n\nFigure 3: XKCD #1658, Copyright 2021 Randall Munroe (Available online at https://xkcd.com/1658/ [https://perma.cc/9P9F-F7MD).\n\nIn late 2013, the Department of Health and Human Services launched an online insurance exchange—HealthCare.gov. The launch and name of the site quickly became synonymous with government failure and technical incompetence, the focus of public ridicule and late-night comedy routines.  Despite millions of visits to the site, only six people were able to register for insurance on its first day.  This spurred the Government Accountability Office to launch several reviews along with the creation of scholarly postmortems and case studies.  Yet for those familiar with the successful effort to fix the site, HealthCare.gov now represents a success story, an example of how government can do tech well.  The effort also holds a central place in the history of forward-looking government tech shops such as the US Digital Service and 18F.\n\nThere were several reasons for the site’s initial failure, many of them relating to project management. A non-exhaustive list is suggested by Anthopoulos, et al., including:\n\n(1) Unrealistic requirements—the website was extremely complex, one of the most complex ever created by the federal government; (2) Technical complexity: there were 55 contractors, 36 states, and 300 private insurers with over 4000 plans; (3) Lack of management leadership: there was difficulty with keeping the project on track, not effective leadership from HHS Secretary Sebelius and staff within [Centers for Medicare and Medicaid Services]; and (4) Inadequate testing: with reports noting that the website was only tested two weeks before the launch, when it needed much more thorough testing.\n\nThe launch was so bad that post launch the President was weighing whether to scrap the project and start over.  Yet, in roughly two months, an ad hoc team of engineers and fixers, many from outside of government, turned the site around, not only creating a site that worked, but one that was continually improving.  Much has been written about the subsequent cultural transfer from Silicon Valley to Washington sparked by this intervention,  but at the core of this new tech ethos was a commitment to agile project management and user-centered design.\n\nTo understand the benefits of agile management methods, it is helpful to contrast them to the historical alternative: the waterfall method.  In the waterfall method, a project manager or client creates a set of milestones or phases which a contractor must complete before advancing to the next phase.  Payment is provided in installments linked to each completed milestone.  But each milestone rarely looks like a completed product. When pictured as a set of descending steps with progress flowing from the top down, this arrangement evokes a waterfall.\n\nUnfortunately, the waterfall method requires an agency or client to accurately predict each step for a project’s completion before work has begun. Because software projects can be duplicated for free, most so-called software engineering solves a new problem each time. For this reason, it is much harder to predict the scope of a software project than the construction of a building or a bridge. Couple this with the fact that such projects often limit user exposure to the software until the project’s completion, and you have a recipe for disaster. If experience is an indicator, such an approach leaves much to be desired. Consider US Digital Services co-founder Haley van Dyck’s observation in 2016 that 40% of federal government IT projects were never completed and 95% were over budget and behind schedule.\n\nAgile approaches present an alternative method for managing complexity. Instead of relying on precise predictions of the distant future and commitment to preplanned milestones and a final project that succeeds or fails on delivery, they structure work around short iterative sprints that aim to produce incremental output for clients. Each sprint produces a work product that can be tested and evaluated by an end-user. At the end of each sprint, the user provides feedback, and planning for the next sprint can proceed with knowledge of what is and is not working. Central to this approach is the centering of client satisfaction, “through early and continuous delivery of valuable software.”  This places the user at the center of the design process, requiring developers to understand their clients and their needs.\n\nIn the wake of the failure and the subsequent success of HealthCare.gov, the US Digital Service was established with one of the site’s fixers at the helm.  Aimed at formalizing the approaches that helped save HealthCare.gov, the US Digital Services articulated these in two foundational documents: The Digital Services Playbook  and The TechFAR Handbook.  Respectively, they lay out a set of model methods such as “Build the service using agile and iterative practices,” and how federal agencies can navigate the Federal Acquisition Regulation (FAR) to implement these methods. It is important to note the pairing of a document on best practices with a guide for implementation within the framework of procurement. It can be tempting to see failures such as the launch of HealthCare.gov or, more recently, poorly performing sites for scheduling COVID vaccination as illustrative of incompetence without fully recognizing the challenges faced by such projects, including the need to operate within existing organizational constraints—such as procurement. Failure to see this context and the role of structural constraints can lead to the impression that problems are easier to solve than reality would allow.  It is worth noting that many of those who worked on the HealthCare.gov rescue pulled fifteen-hour workdays and experienced mental and physical health effects as a result.  There are no shortcuts, and it always takes a team effort.\n\nAs the name suggests, the promise of the Agile method is its ability to help development teams adapt to complex and constantly changing environments. Our experience with the Assembly Line, the ultimate success of HealthCare.gov, and the codification of these principles within The Digital Services Playbook presents a compelling roadmap for its adoption as a step towards a more vibrant forms ecosystem.\n\nD. How e-filing standards could change the world\n\nFigure 4: XKCD #974, Copyright 2021 Randall Munroe (Available online at https://xkcd.com/974/).\n\nKey to the success of an open-source forms ecosystem is the ability for litigants to deliver their work directly, from within the interface of an interactive legal application. Each additional step beyond completing the form itself will reduce the number of litigants able to access the court and defer access to justice. As discussed above in Section III.E, e-filing can be the bridge between the simple guided process of an interactive legal app and the intimidating and challenging-to-access world of the court.\n\nSuffolk's approach to this problem is simple: we are building software that can connect an interactive legal app to a court's case management system and releasing it for free to the world.  Any entity, whether a legal aid program, court, or commercial vendor, can make use of our work without charge.\n\nOur code will be part of the Assembly Line framework, which means a vendor who wants a complete solution can build an app rapidly with model questions and connect e-filing all in one place. A vendor who wants to use just the e-filing integration with an existing forms platform, such as A2J Author or LawHelpInteractive, can install the tool on their own server and connect to a court system with a simplified API.\n\nThe first version of our tool will address an existing standard: OASIS LegalXML ECF v4.  The largest vendor that has adopted this standard is Tyler Technologies. We will also work with at least one court system that has built its own case management system and will use the ECF standard to accept digital filings. We hope that the free availability of this reference implementation for the most popular existing standard will help more courts provide the ability for third parties to use e-filing and for more interactive legal app vendors to adopt this technology.\n\nBy releasing this software for free and providing documentation and free training, we hope to spur a new wave of legal app building. Courts around the country are now imposing mandatory e-filing.  They recognize that e-filing can save clerks time, but e-filing without forms integration can impose an additional burden on litigants.\n\nE. The attorney's role in a world of legal software\n\nFigure 5: XKCD #208, Copyright 2021 Randall Munroe (Available online at https://xkcd.com/208/ [https://perma.cc/L46A-R39J]).\n\nThe involvement of attorneys as subject matter experts in the authoring of Assembly Line interviews demonstrates their value in the automated form production process. Not all attorneys will find themselves in automation roles. They will, however, find themselves in a world where the tools of information workers are increasingly used to help individual attorneys serve many clients. For example, one day after the release of the CDC's eviction moratorium, Suffolk had automated the form.  In the next few months, over 4,000 tenants from across the country used it to assert their rights. There is no way a traditional housing clinic could have helped as many people file a notice.\n\nThe Assembly Line experience also underscores the value of operating as part of an interdisciplinary team. Teams that bring substantive knowledge together with software development expertise can deliver more work quickly than teams that need to look outside and translate software language into subject language in several round trips.\n\nWe want YOU to build the next Court Forms Online. The Assembly Line model described above is not just a methodology. Our work is now the equivalent of a franchise starter kit: a drop-in ready platform that can be customized for any jurisdiction. Partners in Illinois and Louisiana are actively engaged in reproducing our work. In order to facilitate reproduction of our work, we designed an open, free, four-week class¬¬, called the Document Assembly Line Bootcamp, that teaches interested organizations how to: automate documents from start to finish; apply the agile method; use Trello; use GitHub for version control; and add organization-specific databases of courts, logos, and instruction text.  More than 50 participants from around the world joined for at least one class, with about 30 attending all four.\n\nIn addition to detailed documentation, the starter kit allows an author to customize elements including:\n\n• The layout and content of different standardized elements of the interactive legal apps that the jurisdiction wishes to create.\n\n• The list and detailed information relating to courts in the targeted jurisdiction.\n\n• The logos, fonts, visual appearance and branding.\n\nOnce customized, these elements can give each form a consistent look and feel.\n\nV. CONCLUSION\n\nThe justice system moved quickly to expand access to the courts during the pandemic. Mobile-first document assembly with integrated filing is just one area where courts began to open up. These changes increased access to justice during the pandemic, and we can see how they will continue to in the future. Just as with physical curb cuts, these improvements have benefits that extend beyond the need for social distancing. We must not go backwards. The vision we have laid out is a way to join forces, standardize, and simplify our common work. Now is the time to take a lesson from the agile playbook and stop to evaluate what worked and what did not. We have been given a preview of a possible future. For the benefit of improved access to justice, attorneys need to find a way to participate in it.\n\nOur experience working with students, practicing attorneys, paraprofessionals, and advocates of all stripes, have made it clear that our vision for a future of an open-source open-standards forms ecosystem is attainable. Successful interactive legal apps are already in existence around the nation. We think that what will allow these successes to replicate, and scale is the adoption of standard processes and templates, user-centered agile methodologies, and assembly-line-inspired workflows. Whether this will be enough to foster the growth of the ecosystem we envision depends on a community of people working together to carry the load, a community in which practitioners of law work alongside other professionals in interdisciplinary teams to solve problems as they arise.\n\nAs we consider these communities, we look forward to seeing what our new partners in Illinois and Louisiana will do as they work with us to replicate the Assembly Line's work, producing their own interactive legal apps using our process and incorporating true e-filing beyond the simple\n\nencrypted emails of Massachusetts’s pandemic implementation. We think this is reason for optimism.\n\nFOOTNOTES:\n\nThis paper is dedicated to the memory of the Honorable Ralph Gants, Chief Justice of the Massachusetts Supreme Judicial Court. Not only was his example an inspiration, but the work described here would not have been possible without his leadership. He is sorely missed.\n\nClinical Fellow, Suffolk University Law School, J.D., Cornell Law School (2008); B.S. Logic and Computation, B.S. Political Science, Carnegie Mellon University (2004). Before joining Suffolk Law School in March 2020, Quinten Steenhuis was a legal aid attorney for 12 years at Greater Boston Legal Services.\n\nDirector, Legal Innovation and Technology Lab and Practitioner-in-Residence, Suffolk University Law School, J.D., Boston University School of Law (2011); M.Ed., Harvard Graduate School of Education (2002).\n\nThis ordering of authors was determined by coin flip.\n\nLEGAL  SERVS. CORP THE JUSTICE GAP: MEASURING THE UNMET CIVIL LEGAL NEEDS OF LOW-INCOME AMERICANS 28 (2017), https://www.lsc.gov/sites/default/files/images/TheJusticeGap-FullReport.pdf [https://perma.cc/9FC3-PS8X]. This number has been persistent across decades. Ronald W Staudt, All the Wild Possibilities: Technology  Attacks Barriers to Access to Justice, 42 L.A. L. REV. 1117 (2009\n\nElizabeth Petrick, Curb Cuts and Computers: Advocating for Design Equality in the 1980s, 35 DESIGN ISSUES 23–32 (2019) (documenting the history of the curb cut metaphor in 1980s computing).\n\nId.\n\nSheryl Burgstahler, Universal Design: Implications for Computing Education, 11 ACM TRANSACTIONS COMPUTING EDUC. 1–17 (2011).\n\n    For many it did not take a pandemic, but if this is the wakeup call people needed, then so be it. Consider B.W. Hesse, Curb Cuts in the Virtual Community: Telework and Persons with Disabilities, in PROCEEDINGS OF THE TWENTY-EIGHTH HAWAII INTERNATIONAL CONFERENCE ON SYSTEM SCIENCES 418–25 (1995).\n\n CT. FORMS ONLINE, https://courtformsonline.org [https://perma.cc/ABE5-6TJH].\n\nFor a broader discussion of how the information technology industry has impacted society, see KLAUS SCHWAB, THE FOURTH INDUSTRIAL REVOLUTION (2017). Like Tim Hwang, we agree “that the specialist should control their tools, and not the other way around.” Tim Hwang, Bestlaw: How a 3L Enhanced Westlaw, LAW TECHNOLOGY TODAY (Dec. 12, 2014), https://www.lawtechnologytoday.org/2014/12/bestlaw-westlaw/ [https://perma.cc/KLD3-7QT2].\n\nThe Document Assembly Line Project, COVID-19 Response: The Document Assembly Line Project 1, https://suffolklitlab.org/doc-assembly-line/#team [https://perma.cc/GS5X-QAX4].\n\nSee extended discussion infra section IV. C below.\n\nSee, e.g., Staudt, supra note 1.\n\nAmong the largest state-wide interactive legal app repositories are those in New York State (https://www.nycourts.gov/courthelp/DIY/index.shtml [https://perma.cc/SB9T-XPDE]), Michigan (https://michiganlegalhelp.org/  [https://perma.cc/38CF-BZVT]), and Illinois (https://www.illinoislegalaid.org/ [https://perma.cc/UZ3X-CGU2]). None currently provide integrated electronic filing that files forms directly in a court's case management system to self-represented litigants. More than one state piloted the acceptance of forms by email during the pandemic, with clerks manually opening and then filing emailed forms by hand. E-filing projects can be expensive when started from scratch and require cooperation by either courts or for-profit vendors, like Tyler Technologies, and incentives may not align with free electronic filing for pro se litigants. While there is not space in this article for a full survey of all efforts towards digital delivery of forms, we believe it is likely that some courts who have built their own case management systems have such integrations for specific forms, and it should be noted that the Tyler Guide and File platform provides end-to-end filing for a limited number of forms.\n\nWe do note three examples of quasi-integrated electronic filing that were created by legal aid agencies with the assistance of Law Help Interactive. Two are focused on advocates, not pro se litigants. A third was a pilot that has been discontinued.\n\n1.\tIn the New York State Family Law Court, since 2014, court-based domestic violence advocates have been able to use a Law Help Interactive interview to enter information about domestic violence survivors prior to a case being docketed. The information goes directly into the court's case management system, saving critical time in the completion of restraining orders. This tool, however, is not open to the general public and does not include a full pleading. See Rochelle Klempner, The Case for Court-Based Document Assembly Programs: A Review of the New York State Court System’s “DIY” Forms, 41 FORDHAM URBAN LAW JOURNAL 1189 (2016); Josh Waldman, The Family Offense Petition Project: Increased Usage for New York State Tool for Advocates, CONNECTING JUSTICE COMMUNITIES (Jan. 25, 2021), https://www.connectingjusticecommunities.com/ny-family-offense-petition/2021/01/ [https://perma.cc/6AZK-9QB2].\n\n2.\tIn Washington, DC, beginning in March 2020, court personnel began collaborating with Law Help Interactive to login to a portal called “LHI Connect” to periodically download domestic violence filings. In some DC courts, the filings are received by email rather than through the LHI portal. In either case, the clerk then enters information about the case into a case management system by hand.\n\n3.\tLaw Help Interactive had an e-filing pilot project based in Minnesota's Hennepin County between 2012–2015 that integrated directly with the court's case management system. The project allowed filing of both a domestic violence and a civil nuisance pleading, eventually reaching about 2,400 filings per year. This pilot project involved sending XML data directly to the court's case management system, predating the modern Electronic Filing Service Partner (EFSP) relationship that Tyler Technologies now has in 8 states. LEGAL SERVICES NAT'L TECH. ASSISTANCE PROJECT, THE STATE OF E-FILING 2017 (Oct. 4, 2017), https://www.slideshare.net/LSNTAP/the-state-of-efiling-2017 [https://perma.cc/A9XH-Y2SD]. This project later led to the development of a document outlining best practices for e-filing on behalf of self-represented litigants. Richard Zorza, E-Filling Best Practices Document Available — Please Help with the Evaluation, RICHARD ZORZA’S ACCESS TO JUSTICE BLOG (Jan. 8, 2014), https://accesstojustice.net/2014/01/08/e-filling-best-practices-document-available-please-help-with-the-evaluation/ [https://perma.cc/UPM4-LFAD].\n\n Klempner, supra note 11.\n\nCCJ/COSCA, Guiding Principles for Post-Pandemic Court Technology, PENNSYLVANIANS FOR MODERN CTS. (July 16, 2020), https://www.pmconline.org/resources/guiding-principles-post-pandemic-court-technology [https://perma.cc/FU34-BQLJ ].\n\nSee David Anthony Colarusso & Erika J. Rickard, Speaking the Same Language: Data Standards and Disruptive Technologies in the Administration of Justice, 50 SUFFOLK U. L. REV. 387 (2017).\n\n LEGAL SERVS. CORP., supra note 1 at 6.\n\nSee Gideon v. Wainwright, 372 U.S. 335 (1963).\n\nRight to counsel in a narrow subset of civil matters does, however, exist as a matter of state law, although these areas of law usually share features with criminal law in that liberty interests are at stake. For example, Massachusetts provides counsel by right for civil commitment and child requiring cases. See MASS. GEN. LAWS ch. 123, § 12(b), ch. 119 § 39F (2019) respectively.\n\nLEGAL SERVS. CORP., supra note 1.\n\nLEGAL SERVS. CORP., supra note 1 at 23.\n\nWarren E. George, Development of the Legal Services Corporation, 61 CORNELL L. REV. 681 (1975).\n\nSee Gideon v. Wainwright, 372 U.S. 335 (1963).\n\n“Even if lawyers became more willing to work for free, U.S. lawyers would have to in- crease their pro bono work from an annual average of thirty hours each to over nine hundred hours each to provide some measure of assistance to all households with legal needs.” Gillian K. Hadfield, Innovating to Improve Access: Changing the Way Courts Regulate Legal Markets, 143 DAEDALUS 83, 87 (2014).\n\nMary Juetten, Unauthorized Practice of Law Claims Threaten Access to Justice, FORBES (May 8, 2018), https://www.forbes.com/sites/maryjuetten/2018/05/08/unauthorized-practice-of-law-claims-threaten-access-to-justice/ [https://perma.cc/GBK5-PTQR].\n\nJames Sandman, Rethinking Access to Justice, 68 VIRGINIA LAWYER 28 (2014).\n\nThe UK has a special history of brief advice hotlines—the Citizens Advice Bureaux—as a method of closing the access to justice gap. Dolores Korman Sloviter, Let’s Look at Citizens Advice Bureaux, 65 A.B.A. J. 567 (1979).\n\nAbout the Statewide Website Assessment Public Report, LEGAL SERVS. CORP., https://www.lsc.gov/node/4534 [https://perma.cc/2D4X-C844].\n\nTechnology Initiative Grant Program, LSC - LEGAL SERVICES CORPORATION, https://www.lsc.gov/node/456 [https://perma.cc/B8G5-AXHX].\n\nSee, e.g., 2020 Technology Initiative Grant (TIG) Special Grant Terms and Conditions (2020), LEGAL SERVS. CORP., https://www.lsc.gov/sites/default/files/attach/2020/11/2020-TIG-Terms-and-Conditions.pdf [https://perma.cc/A82M-6EPG].\n\nMatt Stempeck, A Timeline of Civic Tech Tells a Data-Driven Story of the Field, CIVIC HALL (May 28, 2019), https://civichall.org/civicist/how-civic-tech-has-evolved-over-the-last-25-years/ [https://perma.cc/XE5L-2L9B].\n\nDIGITAL GOVERNMENT: BUILDING A 21ST CENTURY PLATFORM TO BETTER SERVE THE AMERICAN PEOPLE, https://obamawhitehouse.archives.gov/sites/default/files/omb/egov/digital-government/digital-government.html [https://perma.cc/LT8V-FR6K].\n\nId.\n\nSee e.g., Todd Wallack, Call it Big Data’s Big Dig — $75m, 19 Years, Still Not Done, THE BOS. GLOBE (April 11, 2015, 9:01 p.m.) https://www.bostonglobe.com/metro/2015/04/11/massachusetts-courts-long-delayed-computer-system-may-leave-public-out/S7tZcbvBDFd3nho7XvEZPO/story.html [https://perma.cc/D6VV-AZHY].\n\nSee infra Section IV.C.\n\nSee, e.g., Staudt, supra note 1.\n\nMarc Lauritsen & Quinten Steenhuis, Substantive Legal Software Quality: A Gathering Storm?, in ICAIL '19 PROCEEDINGS OF THE SEVENTEENTH INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND LAW 52 (2019).\n\nAustan Goolsbee, The Turbo Tax Revolution: Can Technology Solve Tax Complexity?, THE CRISIS IN TAX ADMINISTRATION 126 (Henry Aaron & Joel Slemrod eds., 2004). Note that the use of Turbo Tax as a generic descriptor is problematic given the underpinnings of the business model behind the product. See Jason Tashea, Stop Comparing Your Startup to TurboTax, ABA JOURNAL (Mar. 25, 2019, 6:30 AM CDT), https://www.abajournal.com/lawscribbler/article/stop-comparing-your-startup-to-turbotax [https://perma.cc/HE4D-2B3C].\n\nThe Legal Services Corporation has specifically called out each of these three technologies as preferred for its Technology Initiative Grants projects. See Technology Initiative Grant Program, LEGAL SERVS. CORP. (Mar. 21, 2019), https://web.archive.org/web/20190321190234/https://www.lsc.gov/grants-grantee-resources/our-grant-programs/tig [https://perma.cc/56NX-GX4H].\n\nTina Rosenberg, Opinion, Legal Aid With a Digital Twist, N.Y. TIMES (June 1, 2016), https://www.nytimes.com/2016/06/01/opinion/legal-aid-with-a-digital-twist.html [https://perma.cc/5B7X-83F6].\n\nFor a closer look at a few others, see Lauritsen & Steenhuis, supra note 35.\n\nHotDocs Classic, HOTDOCS (2021), https://www.hotdocs.com/products/hotdocs-classic/ [https://perma.cc/VK8H-Y5DR].\n\nSee LAW HELP INTERACTIVE, https://lawhelpinteractive.org/ [https://perma.cc/JNH9-VRMB].\n\nStaudt, supra note 1.\n\nStaudt, supra note 1 at 116.\n\nAs of February 2021, CALI's website announces that A2J Author interviews have been used more than 4 million times. See What is A2J Author?, CALI, https://www.cali.org/faq/15690 [https://perma.cc/CY3R-6HNP].\n\nMarc Lauritsen, Roads, Towers, and Online Legal Help, MEDIUM (Jan. 10, 2020), https://marclauritsen01.medium.com/roads-towers-and-online-legal-help-57957a25767 [https://perma.cc/K499-ZFKX].\n\nSarah Glassmeyer, We’re Rolling out A2J Author Version 5.0!!, CALI (Jul. 31, 2014, 1:20 PM), https://www.cali.org/blog/2014/07/31/we%E2%80%99re-rolling-out-a2j-author-version-50 [https://perma.cc/6MVD-S3MB].\n\nStaudt, supra note 1.\n\nFree Software Foundation, Inc., GNU Affero General Public License Version 3, OPEN SOURCE INITIATIVE (2007), https://opensource.org/licenses/AGPL-3.0 [https://perma.cc/VH3T-ZX8A]; Mike Mitchel, Update License to GNU AGP v3.0 and Link in Readme.md · CCALI/a2jdat@acbfb9a, GITHUB (2021), https://github.com/CCALI/a2jdat/commit/acbfb9af281a7dbd2f1b5a11280bd30b651917c3#diff-4673a3aba01813b595de187a7a6e9e63a3491d55821606fecd9f13a10c188a1d [https://perma.cc/PSS3-3JTQ].\n\nFor a history of LawHelpInteractive and its hosting of HotDocs and A2J Author interviews, see Lauritsen, supra note 45.\n\nJonathan Pyle, Docassemble, DOCASSEMBLE, http://docassemble.org/ [https://perma.cc/325C-H3X8].\n\nThe MIT License, OPEN SOURCE INITIATIVE, https://opensource.org/licenses/MIT [https://perma.cc/EX8J-93HC].\n\nGitHub: Where the World Builds Software, GITHUB (2021), https://github.com [https://perma.cc/SA8U-JSFG].\n\nRick Hoving, Gabriel Slot, & Slinger Jansen, Python: Characteristics Identification of a Free Open Source Software Ecosystem, in 2013 7TH IEEE INTERNATIONAL CONFERENCE ON DIGITAL ECOSYSTEMS AND TECHNOLOGIES (DEST) 13 (2013); Jonathan Pyle, External Data, DOCASSEMBLE (2021), http://docassemble.org/docs/external.html [https://perma.cc/K8PA-WJGY].\n\nTom De Smedt & Walter Daelemans, Pattern for Python, 13 J. MACHINE LEARNING RES. 2063 (2012).\n\nMaurizio Montel, Python-Holidays, GITHUB (2021), https://github.com/dr-prodigy/python-holidays [https://perma.cc/P9F2-XSJA].\n\nJonathan Pyle, Setting Variables (and Doing Other Things) with Questions, DOCASSEMBLE (2021), http://docassemble.org/docs/fields.html [https://perma.cc/K24X-KLDJ].\n\nAfterPattern no longer uses Docassemble as the underlying engine. Still, its design and structure remain heavily influenced by its original iteration as a Docassemble front-end. In November 2021, Affterpatter was acquired by NetDocuments and is slated for integration with the NetDocuments platform. NetDocuments Inspires Productivity with Acquisition of Afterpattern Document Automation, NETDOCUMENTS (2021) https://www.netdocuments.com/press-releases/netdocuments-inspires-productivity-with-acquisition-of-afterpattern-document-automation\n\nSee AFTERPATTERN, https://afterpattern.com/ [https://perma.cc/78ZQ-XPNB]; see also DOCUMATE (2021), https://www.documate.org/ [https://perma.cc/9WYT-7CQK] respectively.\n\nBen Carter, Helping Renters, AFTERPATTERN (2020), https://info.afterpattern.com/case-studies/helping-renters [https://perma.cc/BTJ7-WA7S].\n\nBen Carter, author of the app, reports that it was duplicated by legal aid providers in Cleveland and Indiana.\n\nFor a longer, but still incomplete, list of non-profit apps in this space, see an inventory conducted here: Summer 2020 Automated Forms Audit, A2J AUTHOR, https://www.a2jauthor.org/content/summer-2020-automated-forms-audit [https://perma.cc/W344-3SQH]. See also Rebecca L. Sandefur, Legal Tech for Non-Lawyers: Report of the Survey of US Legal Technologies, AM. BAR FOUND. (2019), http://www.americanbarfoundation.org/ uploads/cms/documents/report_us_digital_legal_tech_for_nonlawyers.pdf [https://perma.cc/E5DW-KYZX].\n\nAlthough outside of the scope of this article, it is worth noting that LegalZoom is also a pioneer in establishing that these tools are not the unauthorized practice of law. For an extensive discussion, see Emily McClure, LegalZoom and Online Legal Service Providers: Is the Development and Sale of Interactive Questionnaires that Generate Legal Documents the Unauthorized Practice of Law?, 105 KY. L.J. 563 (2017).\n\nLegalZoom: Start a Business, Protect Your Family: LLC, Incorporate, Wills, Trademark, Legal Advice, LEGALZOOM (2021), https://www.legalzoom.com/ [https://perma.cc/DX8P-RQ4D].\n\nThemis Solutions, Inc., How an Attorney-Turned-Entrepreneur is Using Legal Technology to Reshape Divorce Law, CLIO (2020), https://www.clio.com/customers/how-an-attorney-turned-entrepreneur-is-using-legal-technology-to-reshape-divorce-law/ [https://perma.cc/JRL3-8VQZ].\n\nHello Divorce: Plans and Pricing, HELLO DIVORCE, https://hellodivorce.com/membership-options/ (2022).\n\nErin McDowell, The Average Cost of Getting Divorced Is $15,000 in the US, BUS.  INSIDER (Aug. 1, 2019, 10:58 AM), https://www.businessinsider.com/average-cost-divorce-getting-divorced-us-2019-7 [https://perma.cc/7H85-UVBN].\n\nUpsolve: The Best Inventions of 2020, TIME (Nov. 19, 2020, 8:56 AM), https://time.com/collection/best-inventions-2020/5911367/upsolve/ [https://perma.cc/J4UB-K7R4].\n\nDanny Crichton, YC-Backed Upsolve Is Automating Bankruptcy for Everyone, TECHCRUNCH (Jan. 16, 2019, 9:01 AM), https://social.techcrunch.com/2019/01/16/upsolve-bankruptcy/ [https://perma.cc/K5TE-AQGD] ; Upsolve Bankruptcy Community, https://www.facebook.com/groups/upsolveusers/; Rohan Pavuluri, Why Does Upsolve Believe in Earning Revenue ? (Aug. 1, 2020), https ://upsolve.org/learn/being-sustainable/ ; Quinten Steenhuis, 7 Lessons for Building User Friendly Guided Interviews, NONPROFIT TECHY (Jun. 27, 2018), https://www.nonprofittechy.com/2018/06/27/7-lessons-for-building-user-friendly-guided-interviews/.\n\nThis functionality, however, is offered in other similar tools such as Survey Monkey. See Question Bank, SURV. MONKEY (2021), https://help.surveymonkey.com/articles/en_US/kb/What-is-Question-Bank [https://perma.cc/9TJ8-M67E].\n\nJason Tashea wrote about this concept recently. Jason Tashea, The Justice System as a Digital Platform, THE COMMONS (Sept. 30, 2020), https://wearecommons.us/2020/09/30/the-justice-system-as-a-digital-platform/ [https://perma.cc/S35U-LD2B].\n\nStaudt, supra note 1, elegantly summarizes the challenges caused by a lack of e-filing integration.\n\nThis insight is not unique insight and echoes Cyd Harrell’s description of open source benefits in the context of civic technology. CYD HARRELL, A CIVIC TECHNOLOGIST’S PRACTICE GUIDE 23 (2020).\n\nDocument Assembly Line Project, Court Forms Online (MassAccess), CT. FORMS ONLINE MASSACCESS https://courtformsonline.org/ [https://perma.cc/QLR9-QJXZ].\n\nQuinten Steenhuis, Creating a Clinic in a Box: Why I Fell in Love with Building Online Legal Apps, MEDIUM (Sept. 8, 2020), https://medium.com/legal-design-and-innovation/creating-a-clinic-in-a-box-why-i-fell-in-love-with-building-online-legal-apps-8b56a6e0ed1c [https://perma.cc/63Z8-5YSH].\n\nSee Document Assembly Project, Domestic Violence, CT. FORMS ONLINE MASSACESS, https://courtformsonline.org/dv/#209A [https://perma.cc/J6V7-9CAZ].\n\nSee Document Assembly Project, Housing, CT. FORMS ONLINE MASSACCESS, https://courtformsonline.org/housing/#CDC [https://perma.cc/E3AT-N6BE].\n\nNotable individual contributors included Maeve MacGlinchey, a volunteer from South Africa who worked as a full-time project manager six time zones away. Other volunteers participated from The Hague, The Netherlands; Australia; India; Ireland; and cities across the United States.\n\nChief Justice Ralph D. Gants, Letter to the Bar from Supreme Judicial Court, MASS.GOV (Mar. 19, 2020), https://www.mass.gov/news/letter-to-the-bar-from-supreme-judicial-court-chief-justice-ralph-d-gants [https://perma.cc/TW8Y-PFSM].\n\nId.\n\nId.\n\nId.\n\nWe learned of this powerful incident, and several similar ones, through oral reports from multiple legal aid attorneys, although the name of the client was not disclosed to protect privacy. It is a good illustration of the delays and inconsistent procedures that the sudden closing of courts to physical access caused at the beginning of the pandemic.\n\nSee Massachusetts Defense for Eviction (MADE): Self-Guided Eviction Help, GREATER BOSTON LEGAL SERVS. (2021), https://gbls.org/MADE [https://perma.cc/92VG-CMEL]; Quinten Steenhuis, MADE in Boston; Replicable Around the World?, LAW, TECH. & ACCESS JUST. (May 16, 2019), https://law-tech-a2j.org/odr/made-in-boston-replicable-around-the-world/ [https://perma.cc/GLQ8-XPW5].\n\nIt is worth noting, that the term e-filing as used here includes the email submission of filings. Unlike true e-filing, email provides for electronic delivery, but a court clerk still needs to manually enter information about the case in the court's docket. See section III.E. In what is likely a record, the Court drafted a Memorandum of Understanding spelling out the relationship between Suffolk and the Court within two days of the topic being raised. It would be six weeks before the MOU was eventually signed, but the Court’s interest in taking action was palpable.\n\nSee Massachusetts Appeals Court, Appeals Court Administrative Order 20-5, MASS.GOV (2021), https://www.mass.gov/info-details/appeals-court-administrative-order-20-5 [https://perma.cc/ZK2F-6PB5]. Such an order was not required for the Trial Court work as they did not have a single mandated method for e-filing. Id.\n\nTo provide some measure of how quickly the decision to use Docassemble was reached, the project’s name—The Document Assembly Line—was originally coined as an homage to Docassemble . . . DocAssembly.\n\nAtlassian, Trello, TRELLO (2021), https://trello.com [https://perma.cc/E8CW-DXUA].\n\nSee Bogdan-Alexandru Andrei, Andrei-Cosmin Casu-Pop, Sorin-Catalin Gheorghe, & Costin-Anton Boiangiu, A Study on Using Waterfall and Agile Methods in Software Project Management, 13 J. INFO. SYS. & OPERATIONS MGMT. 125 (2019).\n\nId..; see Jagdeep Singh & Harwinder Singh, Kaizen Philosophy: A Review of Literature 24 (2009).\n\nSuffolk Legal Innovation and Technology Lab, Document Assembly Line Project, YOUTUBE, https://www.youtube.com/playlist?list=PLy6i9GFGw5GzcPqGyZQ06lPp35v6S5-YF (last updated Mar. 11, 2021) [https://perma.cc/5V5T-GVT6].\n\nIn fact, we were able to spin up servers and integrations like Twilio for less than $100 a month.\n\nThe Digital Services Playbook, DIGITAL SERVS. PLAYBOOK, https://playbook.cio.gov/ [https://perma.cc/6R76-KRPZ].\n\nFor example: one expert member of our team, Caroline Robinson, has worked for 17 years as an editor of MassLegalHelp.org to write legal content for untrained members of the general public.\n\nTo experiment with the apps described here, the reader can visit https://courtformsonline.org.\n\nThe ability to match a user’s description of their problem with a legal issue and relevant resources is made possible through the integration of Suffolk A-powered issue spotter—Spot. See The Legal Innovation & Technology Lab's Spot API Beta, LEGAL INNOVATION & TECH. LAB, https://spot.suffolklitlab.org/ [https://perma.cc/826B-UMGD].\n\nCalvin Metcalf, AppGeo Data Detectives: The Case of Boston Courthouse Jurisdiction, APPGEO (Apr. 20, 2020), https://www.appgeo.com/appgeo-data-detectives-the-case-of-boston-courthouse-jurisdiction/ [https://perma.cc/K76J-M53K].\n\nDespite Massachusetts already allowing electronic signatures, a number of standing orders were issued during the early days of lockdown to reaffirm their validity. Supreme Judicial Court Updated Order Authorizing Use of Electronic Signatures by Attorneys and Self-Represented Parties, MASS.GOV, (June 10, 2020), https://www.mass.gov/supreme-judicial-court-rules/supreme-judicial-court-updated-order-authorizing-use-of-electronic [https://perma.cc/PH3S-YDPL].\n\nE-filing is mandatory in some Massachusetts Courts, including Housing and Appeals. Housing Court Standing Order 1-20: Implementation of Mandatory Electronic Filing for Attorneys in Summary Process and Small Claims Cases in the Housing Court Department, MASS.GOV (Jan. 27, 2020), https://www.mass.gov/housing-court-rules/housing-court-standing-order-1-20-implementation-of-mandatory-electronic-filing [https://perma.cc/DDP7-TL5S]; Appeals Court Rule 13.0: Electronic Filing, MASS.GOV (Mar. 5, 2020), https://www.mass.gov/appeals-court-rules/appeals-court-rule-130-electronic-filing [https://perma.cc/HP4W-MT6X].\n\nSee supra fn. 11 for extensive discussion.\n\nThis work is focusing on integration with both bespoke court-created in-house e-filing and national vendors like Tyler Technologies, the plurality vendor of electronic filing platforms for United States court systems, which has third-party integration agreements with eight different U.S. states: Texas, California, Illinois, Maryland, Minnesota, Indiana, New Hampshire, Georgia.\n\nHenrik Kniberg, Making Sense of MVP (Minimum Viable Product) - and Why I Prefer Earliest Testable/Usable/Lovable, CRISP’S BLOG, https://blog.crisp.se/2016/01/25/henrikkniberg/making-sense-of-mvp [https://perma.cc/Y6V8-SVNL].\n\nThe concept of a maturity model originated at Carnegie Mellon University in 1986 for the Department of Defense. Mark Paulk, Capability Maturity Model for Software, ENCYCLOPEDIA OF SOFTWARE ENGINEERING sof589 (John J. Marciniak ed., 2002).\n\nFor a discussion of iterative design, see J. Nielsen, Iterative User-interface Design, 26 COMPUTER 32–41 (1993).\n\nId.\n\nA Maturity Model for Legal Apps, SUFFOLK LAW SCHOOL LEGAL INNOVATION AND TECHNOLOGY LAB, 2021, https://suffolklitlab.org/legal-tech-class/docs/legal-tech-overview/maturity-model/ [https://perma.cc/3G2P-WFJZ].\n\n¬¬Suffolk Law School’s Legal Innovation and Technology Lab, Lab Portfolio, https://suffolklitlab.org/portfolio/ [https://perma.cc/FMD9-C3MF].\n\nWhen possible, we have included undergraduate computer science and business school students as part of our lab course to provide just such a team experience.\n\nWe learned this story from a clerk at the Appeals Court who was responsible for getting this emergency request to the attention of the presiding justice. To protect the privacy interests of the tenant involved, we have chosen not to include the docket number or other identifying details here.\n\nSee Massachusetts Legal Help, https://masslegalhelp.org [https://perma.cc/U3DB-CFL6].\n\nWe discuss some successful projects earlier in this paper supra note 11. See also Klempner, supra note 11. The individual successes do not vitiate our point that there still exists a vast mountain of court processes that can only be completed by completing a form on paper.\n\nThe Creative Commons license under which the XKCD web comic is licensed is an example of such permissive sharing. It is also what allowed us to make use of multiple XKCD comics below. See XKCD, https://xkcd.com/license.html [https://perma.cc/5FCN-S7MH].\n\nSee supra note 106.\n\nColarusso and Rickard, supra note 14 at 390–92.\n\nInternational Organization For Standardization, International Standards Organization, ISO - About us, ISO (2021), https://www.iso.org/about-us.html [https://perma.cc/WK26-8XX7].\n\nEric S. Raymond is the author of the best-known text explaining the open source movement. See ERIC S. RAYMOND, THE CATHEDRAL & THE BAZAAR: MUSINGS ON LINUX AND OPEN SOURCE BY AN ACCIDENTAL REVOLUTIONARY (2001).\n\nToby Grytafey has compellingly made this connection, most recently at Docacon 2019. See Toby Grytafey, Speaking the Same Language (Jun. 25, 2019),  https://www.youtube.com/watch?v=Ism2qR3e9Yc [https://perma.cc/K6F7-WCB3].\n\nShahron Williams van Rooij, Adopting Open-Source Software Applications in U.S. Higher Education: A Cross-Disciplinary Review of the Literature, 79 REVIEW OF EDUCATIONAL RESEARCH 682–701, 685 (2009).\n\nIt is worth noting that we are hardly unique in advocating the power of building an open source ecosystem to accelerate software development, especially in an academic or non-profit context. See, e.g., Katie Malone & Rich Wolski, Doing Data Science on the Shoulders of Giants: The Value of Open Source Software for the Data Science Community, HARVARD DATA SCIENCE REVIEW (2020), https://hdsr.mitpress.mit.edu/pub/xsrt4zs2 [https://perma.cc/PBW9-RLDW]. In higher education, see van Rooij, supra note 113.\n\nRAYMONDE GUINDON, COGNITIVE SCIENCE AND ITS APPLICATIONS FOR HUMAN-COMPUTER INTERACTION 59 (2013).\n\nAmerican Bar Association, ABA Best Practice Guidelines for Online Legal Document Providers, https://www.americanbar.org/content/dam/aba/images/news/2019/08/am-hod-resolutions/10a.pdf [https://perma.cc/Y3L3-CXD2].\n\nSee Staudt, supra note 1.\n\nThe LSNTAP mailing list is a place for largely informal information sharing and question and answer gathering, funded by the Legal Services Corporation.\n\nSee  Leonidas Anthopoulos, Christopher G. Reddick, Irene Giannakidou, & Nikolaos Mavridis, Why e-Government Projects Fail? An Analysis of the Healthcare.gov Website, 33 GOVERNMENT INFORMATION QUARTERLY 161–173 (2016). See also Saturday Night Live, Obamacare Website Tips (2013), https://www.youtube.com/watch?v=_rzQeDOGDxI [https://perma.cc/RV9Y-U7DQ].\n\nAdrianne Jeffries, After Healthcare.gov, Can the Government Make its Technology Suck Less?, THE VERGE (2014), https://www.theverge.com/2014/4/2/5573594/are-we-serious-about-making-government-tech-suck-less [https://perma.cc/U7GX-K9DF].\n\nSee e.g., U. S. Government Accountability Office, Healthcare.gov: Ineffective Planning and Oversight Practices Underscore the Need for Improved Contract Management (2014), https://www.gao.gov/products/GAO-14-694 [https://perma.cc/E95U-3FTK]. See also Anthopoulos et al., supra note 119 and Janis L Gogan, Elizabeth J Davidson & Jeffrey Proudfoot, The HealthCare.gov project, 6 J. INFO. TECH. TEACHING CASES 99–110 (2016).\n\nJeffries, supra note 120.\n\nMeyer Robenson, The Secret Startup That Saved the Worst Website in America, THE ATLANTIC (Jul. 9, 2015), https://www.theatlantic.com/technology/archive/2015/07/the-secret-startup-saved-healthcare-gov-the-worst-website-in-america/397784/ [https://perma.cc/2HXW-NMS3].\n\nAnthopoulos et al., supra note 119 at 166.\n\nSteven Brill, Obama’s Trauma Team, TIME, https://time.com/10228/obamas-trauma-team/ [https://perma.cc/MJJ8-YWUC].\n\nId.\n\nJack Moore, Invasion of the Innovators, GOVERNMENT EXECUTIVE (2015), https://www.govexec.com/feature/invasion-innovators/ [https://perma.cc/A73A-35KJ]..\n\nInes Mergel, Agile Innovation Management in Government: A Research Agenda, 33 GOVERNMENT INFORMATION QUARTERLY 516–23 (2016).\n\nAndrei et al., supra note 86 at 126.\n\nId.\n\nLindsay Young, 18F: Digital Service Delivery: Using Agile and DevOps to Get Better Results Than a Change Control Board (2021), https://18f.gsa.gov/2021/03/02/using-agile-and-devops-to-get-better-results-than-a-change-control-board/ [https://perma.cc/MV38-CBPD].\n\nKate Torgovnick, What Happens When You Disrupt the White House: Haley Van Dyck at TED2016, TED BLOG (Feb. 16, 2016, 12:05 PM), https://blog.ted.com/what-happens-when-you-disrupt-the-white-house-haley-van-dyck-speaks-at-ted2016/ [https://perma.cc/CXG3-8MPL].\n\nPrinciples Behind the Agile Manifesto, AGILE SOFTWARE, https://agilemanifesto.org/principles.html [https://perma.cc/7TXD-ZZ4G].\n\nNancy Scola, White House Launches ‘U.S. Digital Service,’ with HealthCare.gov Fixer at the Helm, WASHINGTON POST (Aug. 11, 2014, 1:33 PM), https://www.washingtonpost.com/news/the-switch/wp/2014/08/11/white-house-launches-u-s-digital-service-with-healthcare-gov-fixer-at-the-helm/ [https://perma.cc/F7A6-4KTH].\n\nU.S. Digital Service, supra note 90.\n\nU.S. Digital Service, The TechFAR Handbook, https://techfarhub.cio.gov/handbook/ [https://perma.cc/7CD9-5MCN].\n\nThough they bear a resemblance to the story of software engineers swooping in to save HealthCare.gov, narratives that elevate individual actors and simple solutions risk minimizing this broader context. See Sharon Otterman, N.Y.’s Vaccine Websites Weren’t Working. He Built a New One for $50., THE NEW YORK TIMES (Feb. 9, 2021), https://www.nytimes.com/2021/02/09/nyregion/vaccine-website-appointment-nyc.html [https://perma.cc/T4BC-R9YE]. See also Dan Hon, The $50 Vaccine Website We Don’t Need, NEW AMERICA (Feb. 10, 2021), http://newamerica.org/pit/blog/the-50-vaccine-website-we-dont-need/ [https://perma.cc/524E-YLR3]. Narratives that hold up “$50” solutions differ from those trumpeting the eventual success of HealthCare.gov in that they often fail to consider or address the causes of system failure. As Cyd Harrell has noted in her advice to those seeking to enter the civic tech space, “But our job as a civic technologist isn’t to be the hero of the stories we stumble into halfway through; it’s to understand the people who have already been in place doing the work, and who want to use tech to make improvements.” HARRELL, supra note 71 at 30.\n\nHARRELL, supra note 71 at 138.\n\nOur work will be released under the same, permissive, MIT license that Docassemble uses.\n\nOASIS, Electronic Court Filing Version 4.01 (2013), http://docs.oasis-open.org/legalxml-courtfiling/specs/ecf/v4.01/ecf-v4.01-spec/os/ecf-v4.01-spec-os.html [https://perma.cc/ZX74-VQUH].\n\nRecent states to impose mandatory e-filing for at least some case types include Massachusetts, Illinois, and New Hampshire. Housing Court Standing Order 1-20, supra note 95; E-Business for the Courts - Electronic Filing Initiatives, http://illinoiscourts.gov/EBusiness/Electronic_Filing.asp [https://perma.cc/LY84-T26T]; New Hampshire Circuit Court - Electronic Filing Pilot Rules, https://www.courts.state.nh.us/rules/dmcr/dmcr-sup-efile.htm [https://perma.cc/6LW8-WWXA].\n\nBob Ambrogi, Suffolk’s LIT Lab Releases Eviction Moratorium Tool to Help Tenants Exercise Their Rights, LAWSITES (Sept. 2, 2020), https://www.lawsitesblog.com/2020/09/suffolks-lit-lab-releases-eviction-moratorium-tool-to-help-tenants-exercise-their-rights.html [https://perma.cc/E84A-VMXJ].\n\nSuffolk Legal Innovation and Technology Lab, Suffolk LIT Lab’s Document Assembly Line Bootcamp (2021), https://suffolklitlab.org/docassemble-AssemblyLine-documentation/docs/bootcamp/ [https://perma.cc/9969-XLNE].\n",
    "model": "gpt-4o-mini",
    "temperature": 0.7,
    "max_tokens": 250,
    "output": 0,
    "json_mode": 0,
    "output_to": 4,
    "behavior": "Summarize & question paper",
    "hide_button": false
  },
  "Speaking the Same Language (Colarusso & Rickard, 2017)": {
    "prompt": "Speaking the Same Language: Data Standards and Disruptive Technologies in the Administration of Justice\n\nby David Colarusso * & Erika J. Rickard **\n\nI.  Introduction\n\nWhile the legal profession is coming to grips with technological disruption, practitioners serving the needs of those with low- and moderate-incomes find themselves struggling to keep up.   Insufficient resources clearly impede large-scale technological improvements.  Yet, the rise of civic coding and the growing legal technology sector suggest an untapped pool of civic and private resources ready to help address this shortfall.   We argue that state trial courts are best positioned to leverage these resources for the benefit of low- and moderate-income individuals by addressing a key structural impediment to innovation: the lack of clearly-defined judicial data standards.\n\nIn private practice and in legal education, innovative technologies have fueled competition from companies providing document automation to the general public and leveraging machine intelligence to remove the work of repetitive tasks, including matters involving rudimentary questions of judgment.   While lawyers can (and do) debate the relative merits and drawbacks of this emerging paradigm shift, the adoption of modern information technology is undeniably a gamechanger for all parties involved.  Meanwhile, such technology has begun to deliver only a fraction of the disruption long predicted by scholars and practitioners in the access to justice world.\n\nWe argue that the primary obstacles to utilizing technology to increase access to justice lie in the state trial courts.  Court systems, like legal aid programs and the private bar, have begun to adopt new technologies, both internally and externally.  However, tech tools are often added to the existing structures and schema of court administration, with little consideration for how the parts fit into the whole or how such changes provide opportunities for reforming existing practices.  This paper calls for a reimagining of current practice.  While we are not the first to suggest some kind of redesign, we propose a catalyst:  open data standards.\n\nWe see data standards as both a critical part of well-functioning court systems and a fulcrum for leveraging technology to drive system change.  Section II provides an overview of open data standards, including an historical look at their use in the law and examples of successful 21st century implementations by executive branch agencies.  Section III provides context for the access to justice movement and its current efforts to harness technology for the benefit of those with legal needs.  Section IV focuses on how establishing data standards for electronically sharing information across the justice system would propel existing technological innovations to greater prominence and effectiveness.  Section V takes the idea one step further, posing data standards as the missing ingredient to broader thinking about improving access to justice in the 21st century through state courts. \n\nII.  The Catalyst:  Data Standards\n\nIf an argument over the use of one or two spaces post-punctuation has ever concluded with blows, a lawyer likely threw the first punch.   Form matters.  The legal community codifies proper form in court rules and adopts standards of citation in service of clarity and efficiency.  The effort of deciphering an author's meaning is a tax on their argument, one the legal community has done its best to abolish. Form matters in part because it should not matter.   Standards allow those engaging with a document to more easily access its content.  This understanding is at the core of legal practice.   So it is surprising that the legal community has largely failed to carry this insight into the digital age.  When the final product is a printed document with one-inch margins and the intended reader is a person, no self-respecting court would consider forcing parties to adopt a monolithic technical solution (e.g., all filings must be typed on a 1939 Underwood Universal typewriter). However, in the digital realm many courts are happy to do just that.  This is likely due to a conflation of software products with the tasks they perform.  The printed document represents a well-defined standard and parties are free to produce compliant documents however they prefer.\n\nIn the early days of English common law, those writing the Year Books (i.e., reports of pleas before the common bench) found it sufficient when citing cases to use monikers such as \"casum la dame de Gild,\" meaning \"the case of the lady of Gild.\"   Such conventions and the memories of individual lawyers, however, proved inadequate at indexing the entirety of English case law.   Today we avoid reliance on memory through the application of standard citation and reference to common court reporters.   Now imagine a different world, one in which each courthouse maintains its own library containing the entirety of all case law, painstakingly transcribed by the court clerk into idiosyncratically paginated, sequentially numbered, three-ring binders.  To cite a case under this system, one is required to reference the binder and page number of the case as recorded in that court’s library.  Instead of abolishing our decipherment tax, the courts have shifted costs to those operating between courts, and they have made cost sharing among the courts impossible.  This new cost works to erect walls around individual courts, siloing them off from larger communities.  This is the world legal technologists see today, one in which the failure to adopt clear standards concerning the formatting of data impedes the development of digital solutions at scale, while imposing real costs on those wishing to access justice.\n\nA.  Data Standards Explained\n\nIn the context of modern information technology, data standards are the rules under which one stores data, including the data’s format.  For example, if the court transmits a date to the jail requesting the transport of an inmate, it would be efficient if that date was added automatically to the prison’s calendaring system.   Before this can happen, however, the prison’s system must know how the court writes dates.  Consider that it is customary for Americans to write dates in the format month-day-year while much of the world makes use of the format day-month-year.   Dates written in these formats risk ambiguity if a day falls within the first twelve days of the month or the reader is inattentive.  Cultural conventions normally operate to avoid such ambiguity.  Yet when sharing information across cultures, a failure to explicitly discuss such norms can end badly, such as in the loss of a 125 million-dollar space mission.  \n\nData standards serve as a digital lingua franca, providing all parties involved the reassurance that they start on the same page.  To the extent that attorneys are aware of such standards, it is often when encountering their absence.  Many attorneys have noticed this absence when attempting to open a WordPerfect file in Word or vice versa.  Attorneys come away from this experience believing that unless you use tools from the same vendors, something will always be lost in translation.  This, however, is a flawed conclusion and a dangerous one to make if carried into the realm of court-approved software.  It is true that unless each tool kit makes use of the same standards, translation will suffer, but there is nothing precluding different tool kits from making use of the same standards.  In the case of WordPerfect and Word the use of proprietary internal standards leads to a breakdown in translation.  Such closed standards have long been part of a developer’s business model.  Yet, standards need not be closed.  Open standards, those that are publicly shared, explained, and open for anyone to use, can serve as the foundation for successful generally accessible software solutions.  In fact, the World Wide Web is proof of the power of open standards. Hyper Text Markup Language (HTML), the language that specifies the content of a webpage, is an open standard.  Governed by the World Wide Web Consortium, it is nothing more than a set of specifications curated and published by an international standards organization.  The entirety of the World Wide Web is built on technology that implements these standards, and so the publisher of a website can be confident in the fact that visitors from the world over will start on the same page.  Standards help facilitate innovation by removing the decipherment tax discussed above. \n\nCourts too often fail to recognize open standards as an alternative to the decipherment tax. They mistakenly believe that quality control requires the same brand of tools.  Consequently, they enter into exclusive contracts with vendors while granting them a monopoly over all aspects of service.  This is analogous to the monopoly of court reporters, which, although at one time useful in the development of standard citations, is starting to  show its age.   More importantly, courts feel there is little they can do to foster technological innovation short of developing or purchasing large-scale technology solutions.  Courts mistakenly believe they have to do it all, that they must build or buy a particularized solution for each of their many stakeholders. \n\nThese stakeholders, including the public, law enforcement, attorneys, and other governmental agencies, have unique needs regarding their interactions with court data.  Consider the options available to a court wishing to make these interactions possible over the web.  Providing each stakeholder with their own idiosyncratic web portal (including their own secure electronic access, user interface etc.) is costly and time-consuming.  Fortunately, the courts need not bear this burden.\n\nGiven a standard for court data, a state court system could implement a single access point for all parties seeking access to their data.   This access point would operate as a place for computer programs to securely exchange standardized data, including everything from requests for court information to electronic filings.  Such an exchange is often implemented through an interface designed specifically for other computer programs, known as an application programming interface (API).  APIs are often just websites that accept and output structured data.  For example, after some form of secure user authentication, a visit to https://example.com/courts/Springfield/cases/pending/ could return a list of cases pending before the Springfield court, and a visit to https://example.com/courts/Springfield/cases/?docket=123 could return detailed information for the case with docket number 123.  In both cases, the data returned would be structured data conforming to a set of well-defined data standards.   This structure could be as simple as tables with a set of predefined column names or more complex like a nested set of Extensible Markup Language (XML) entities along with an attached data file.   Likewise, one could upload data (e.g., an e-filing) in a similarly structured form.  A person, however, would never visit these URLs in their web browser.  Rather, a program would visit them on the user’s behalf and incorporate their output into a value-added service (e.g., pushing alerts to a user about upcoming court dates).  In this way, the courts could provide a foundation upon which both the courts and other parties may build.  The courts need not provide stakeholders with typewriters to receive typed pleadings.  A court is free to do so, but it is not a requirement.  Additionally, the more widely adopted a standard, the more easily products developed for one class of users can be adapted for others. Standards make solutions scalable.\n\nA court API is only one example of how data standards can produce efficiencies.  A court dead set against web-based access to data could benefit from adopting standards as well. Making sure that all internal systems are standards-compliant (i.e., capable of both reading and writing in the standard) improves internal information sharing.  When courts purchase multiple solutions from a single vendor, they achieve seamless internal information sharing due to the fact that vendor systems all operate under a common standard.  Requiring systems that are compliant with open standards avoids vendor lock-in whereby one has to keep using the same vendor because no other product can read the data.  \n\nB.  Privacy and Security Concerns\n\nSuccessful examples of open data standards are often cited in relation to the open data movement.   Notably, open standards and open data are distinct from one another in that open standards, which are often paired with open data, are system protocols that allow users to share data freely and independently between competing vendors.   Open data, on the other hand, refers to data that can be accessed and shared by the general public freely and with ease.   For example, Emoji, the whimsical iconic images used by many to communicate via text, are standardized as part of a text standard administered by the Unicode Consortium.   The fact that there is an open standard enumerating a common set of emoji—that a wide number of technology companies have seen fit to support—has no bearing on whether or not an individual's text conversations are open to the world for anyone to read.  The Unicode character set (including emoji) is an open standard, which can be used to encode data that is itself not open.   Of course, HTML falls into this category too.  The fact that a bank’s website is built on open standards in no way affects its ability to secure its clients’ data.  Likewise, the adoption of data standards need not limit a court's control over its data. \n\nIn fact, a number of law enforcement agencies already make use of a national data standard, the National Information Exchange Model (NIEM), to share sensitive information, and facilitate e-filing of criminal complaints in some jurisdictions.  As the state public defender agency in Massachusetts, the Committee for Public Counsel Services, recently argued in a public comment on new proposed court record rules,  “As the administrator of [an API implementing open standards], the court would maintain complete control over access based on whatever permissions it deemed appropriate, and could supply different data to different parties depending on need and confidentiality requirements.”   \n\nC.  Where Open Data Standards Have Worked Before \n\nFederal, state, and municipal agencies have successfully deployed APIs built on open data standards, resulting in a proliferation of user-centered applications and web tools in everything from public transit to the delivery of city government services.  If you were planning a trip in 2005, a number of online navigation services offered driving directions (e.g., Google Maps, MapQuest, and Yahoo), but aside from local government trip planners, you would have been hard-pressed to find travel suggestions including public transit.   If you were Bibiana McHugh, working as an IT manager for Portland, Oregon’s public transit organization TriMet, you saw this as a problem that needed solving.   McHugh reached out to the providers of several navigation products, asking if they would consider including Portland’s data.   This led to a meeting with some Google engineers, and within five months Portland’s data became the cornerstone of Google Transit.   The TriMet team worked with Google to provide their data in a simple comma-separated value (CSV) file, and their formatting was later codified into an open standard, what would come to be known as the General Transit Feed Specification (GTFS).   The simplicity of this format allowed other agencies to easily publish their data in the new standard, and, as of this writing, Google Transit has launched in hundreds of cities around the world thanks to the use of GTFS data.\n\nThe GTFS standard made scaling possible because what works for one transit system works for any system using GTFS data.  Remarkably, McHugh explains that “TriMet has not incurred any direct cost for this specific project, except resource time, which is a very small investment in comparison to the returns.”   The TriMet example underscores the role data standards can play in scalability along with the light lift asked of those adopting the standards, but the example does not end with transit data on Google Maps.\n\nIn the mid-2000’s, the Massachusetts Department of Transportation (MassDOT) recognized that riders would benefit from the installation of digital signs to share transit alerts and bus/train arrival times.  The installation of these displays would require major capital investment, and MassDOT projected the roll out would take years.  In 2009, MassDOT was already collecting location data on a significant fraction of its fleet.  So they began publishing the data in a slightly tweaked version of GTFS, what would become GTFS-realtime (GTFS-RT).   MassDOT invited developers to build tools based on this data under the MassDOT Developer’s Initiative. This standards-based approach offered developers the promise of portability and scalability, sparking the creation of dozens of transit apps and websites, products that serve MassDOT’s riders, products that were developed at almost no cost to MassDOT as the investment in tracking was part of a pre-existing plan to provide digital arrival signs. Again, many localities followed suit and adopted the standard, making solutions that worked in one locale easily transferable to others.  If you ever find yourself using your phone to check the location of the next bus, thank open standards. \n\nThe lessons here are not that open standards are a panacea or that their adoption will allow courts to offload large technical builds to third parties.   Rather, the act of adopting data standards and sharing those standards (not necessarily the data they encode) aids in developing scalable technical solutions, and perhaps most importantly, it is a concrete step that courts can easily take.\n\nD.  The Courts Have Room to Grow\n\nCompared to executive branch agencies, the idea of open data standards is relatively new to state courts.   The NIEM is one exception: a data standard used to share information between the judiciary and federal and state agencies to promote interoperability across multiple data sets.   Developers launched NEIM in 2005 as a collaboration between the Department of Justice and the Department of Homeland Security.    A number of courts have adopted NIEM or similar frameworks.  The NIEM, like other national models, is essentially a meta-standard: a template for how states can use standard language to define the types of information that they share across organizations.  Adopting a model like NIEM is therefore an essential first step towards comprehensive data standards.  \n\nSome jurisdictions have begun to create common databases or data exchanges with individual state agencies on an ad hoc basis, often in reaction to specific requests, legislative mandates, or other needs.   To date, fewer than a half-dozen courts across the United States have adopted a comprehensive data standard system despite efforts by entities such as the National Center for State Courts (NCSC) to help facilitate such adoption.   Consequently, there is an opportunity for courts to play a leadership role in the improvement of such standards.  \n\nThe Massachusetts Trial Court recently sought recommendations from an ad hoc Visiting Committee on Technology composed of chief information officers from Harvard University and the Massachusetts Institute of Technology.   The visiting committee was tasked with making recommendations for improving the use of information technology within the court system.  In their 2016 report, the Visiting Committee’s key recommendation to “[i]mprove data sharing and integration across systems,” suggested that the Massachusetts courts “align with external stakeholders around what data formats and standards should be adopted to make interfaces more efficient by reducing the amount of unstructured data that is transmitted.” \n\nThe lack of clear data standards impedes innovation by increasing the cost of development and limiting the population of potential users.  Standards rarely develop from whole cloth.  Rather they are often the result of custom, the outgrowth of de facto standards put in place by monopolistic parties.  The standard is what people use.  What do people use? The standard.  For example, Word is a de facto standard, as much as it hurts users of WordPerfect to admit it.  GTFS is a standard in part because of Google’s market dominance.   Eventually, robust legal data standards will arise.  The question is who will control them.  Given the courts’ traditional role as arbiter of form, it seems appropriate that they should take a leading role.  In so doing, they could likely accelerate innovation and shrink the justice gap.\n\nIII.  The Current Landscape\n\nIn addition to the courts’ roles as keeper of the records and arbiter of form, courts have a vested interest in the development of judicial data standards.  State trial courts in particular stand to benefit from setting the path by which they could link court data to other data sets and, perhaps more importantly, other resources to assist court users.  \n\nA.  Access to Justice and the Role of the State Trial Court\n\nMore and more people are coming to court without a lawyer.  In areas from family law, to housing, to consumer debt, courts across the country have documented this development for several decades, with a marked increase since the 2007-2008 economic recession.   Absent recognition of a legal right to counsel in civil matters, the primary reason that litigants navigate the civil legal system without an attorney is that they cannot afford to hire one.   Legal aid programs that serve those with low incomes are under-resourced and limited by funding restrictions that prevent them from serving entire categories of users or legal issues.   At the same time, attorneys in small and solo practice are struggling to build robust practices and connect with clients who can afford to pay for traditional representation.   Scholars commonly refer to this phenomenon as the justice gap—the dual problem of too many low- and moderate-income individuals who are unable to find or afford an attorney to assist with a legal problem, and the increasing number of attorneys who are unable to find sustainable legal work.   As Access to Justice Advisor and Massachusetts Housing Court Judge Dina Fein notes, “[t]hese trends have converged to create a crisis that impacts all elements of the civil justice system, places untenable stress on the courts, and take an enormous toll on individuals, families, and society at large.”   In the absence of sufficient funding for legal aid or an affirmative right to counsel in most civil matters, alternative forms of assistance along a continuum for those without access to traditional full representation are necessary. \n\nThe court system is a critical actor in addressing challenges facing the civil justice system, particularly for low- and moderate-income litigants.   Courts recognize their responsibility to address the needs of court users as well as would-be court users, which consist of both those who do not or cannot use the courts to address justiciable legal questions, as well as those who are not engaging in the system despite having active cases within the courts.  Leaders of state judiciaries in particular emphasize the need to work across organizations to implement common policy solutions.  Yet these state leaders face institutional and cultural barriers to some of the solutions they seek.\n\nInstitutional inertia presents perhaps the greatest challenge.   State court systems are slow to change, even when court leaders recognize that change is needed.   For example, people who enter the courts with civil legal problems and without representation often struggle with complex court procedures.   Antiquated court procedures require additional explanation and instruction, for attorneys and parties alike.   Courts have the power to simplify court processes themselves.   Civil procedure rules in most jurisdictions, for example, require service of process by publication in newspapers after failed attempts to locate the defendant, despite the well-recognized fact that a potentially interested party will rarely ever have actual notice by reading that newspaper.  While these specific procedures are largely unchanged, the tide is turning in favor of greater access and better resources provided by courts to court users.   The 2011 U.S. Supreme Court decision in Turner v. Rogers  requires courts to provide some safeguards to ensure that courts do not “risk [] erroneous deprivation of liberty.”   Armed with the language of Turner, more and more courts have adopted plain language court forms and instructions, explanations of court processes, and other written self-help tools.  \n\nHowever, self-help tools are generally developed internally by courts themselves without input or contributions from court users, be they litigants or attorneys.  Courts avoid providing legal advice or sharing information developed by advocates, in order to preserve the appearance of judicial neutrality.  The unfortunate consequence of a reliance on judicial neutrality, on the other hand, is that the court can tend to play a passive role, at times arguably acting as a rubber stamp for the most powerful actor.  This is endemic to both individual judges in individual courtrooms, as well as court administrators overall.  As a result, even where another institution has developed explanatory self-help, courts develop their own tools, which err on the side of neutrality at the expense of targeted information or connections between vulnerable populations and legal services providers who can serve their needs.\n\nB.  Technology, Access to Justice, and the Courts:  Current Practice\n\nAs the Conference of Chief Justices and Conference of State Court Administrators highlighted in its 2015 joint resolution Reaffirming the Commitment to Meaningful Access to Justice for All, “effective use of technology” is a great tool for expanding access to justice.   Private practitioners, technology entrepreneurs, and legal aid organizations have all begun to take advantage of technology to address the justice gap.  For example, the Legal Services Corporation’s Technology Innovation Grants (TIGs) incentivize legal aid organizations to use technology to increase “efficienc[y] and effective[ness]” of legal service organizations.   TIG is largely hailed as a success, but unfortunately it has no analogue in the courts. \n\nAt the same time, court systems are improving technology by leaps and bounds.  Much in the same way developing countries skipped over landline telephones entirely—going from no telephone access directly to cell phones—state court systems in many jurisdictions are “leapfrogging” over some of the technological advancements of the 1990s and early 2000s, and some literally moving directly from the typewriter to electronic completion and submission of court documents.   State court technological improvements include court websites, electronically accessible court forms and information and multimedia self-help tools, electronic filing of court pleadings, and electronic access to court records.\n\nAdding new technologies to a court system’s repertoire necessarily means some changes to court operations.  That said, one of the reasons we do not see a more significant transformation in court operations or user experience is the way that technology is adopted by the courts.  New tools are often grafted onto existing business practices, resulting in redundant and duplicative resources, information, and technologies.  Much of the effort to improve internal court technology has not been linked conceptually to the needs of outside users.  Most court technology focuses solely on the court’s needs instead of an outside user’s legal and informational needs.\n\nWeb-based information sharing is one clear example.  Courts and court-based law libraries (typically part of the state judiciary) have progressed beyond initial concerns about unauthorized practice of law when providing legal information online.  Almost all state courts have some form of court website that includes information about different courthouses and legal topics.  There are clear benefits to the advent of these sites, including a trend toward responsive mobile-friendly design and other improvements to render court information easier to find, navigate, and absorb.  While those improvements are laudable, court web tools are lacking in two key areas; first, they are typically organized from the court’s organizational perspective, rather than from the perspective of the target audience.  Second, they are largely redundant to other efforts.  Courts are generally reluctant to collaborate with perceived “advocacy” organizations, resulting in duplicative content across the government and nonprofit sphere.   At the same time, private entrepreneurs are far ahead of courts and legal services providers in developing web-based information and advertising.  Litigants searching online for information about “divorce” are far more likely to find a search engine-optimized site from an enterprising small law firm looking for new clients rather than either of the more neutral resources. \n\nWebsites, text alerts, multimedia self-help, remote assistance, and electronic records are a sample of the various technologies that could have been a lever for improving access to the courts and access to justice overall.  However, as interested groups introduced these initiatives, they were built to accommodate existing systems and not to challenge them.  Further, rather than break down the silos between justice system stakeholders, new technologies have served to strengthen and entrench those separations.\n\nC.  A Pay-to-Play Digital Future?\n\nThe belief that a common vendor, not a common data standard, must be used in order to safeguard data quality is dangerous because it leads to the conclusion that one vendor must be given a monopoly on electronic process.  Earlier we imagined a court mandating all filings be typed on a 1939 Underwood No. 5 typewriter.  Let us assume that in the end, they decided it was good enough to mandate all filings come on A1 paper with one-inch margins.  After all, attorneys were starting to buy their own typewriters.  They did, however, have lingering concerns over security.  How could they be sure that a filing would make it safely from the typewriter to the court? So the courts put out a bid asking for couriers.  They received a healthy number of responses.  However, they all insisted on exclusive contracts.  After all, common vendors are the safeguards of quality.  So the court chose the lowest bid and agreed to an exclusive contract.  The next day an attorney arrived at the court with a filing only to find one of these couriers outside the clerk’s office.  She thought nothing of this and walked into the office to make her filing.  However, the clerk informed her that she would have to have the filing delivered by the courier.  She asked why and was told it was the only way to guarantee security.  She asked if she could just hand the filing over or send it certified mail, and was told “No, it has to come via our courier service.  No one else will do.” \n\nIn the digital world, this is like allowing only one vendor to provide e-filing when it is technically possible to have a single vendor implement a standards-based, not vendor-based, system that authorized vendors could access.  You might think this absurd, and you might also assume that this is a metaphor.  However, if you attempted to make a civil filing in Montgomery County, Texas in 2010, you were actually required to file electronically.   In fact, you were required to make this filing through a single vendor and you had to pay for the privilege above and beyond standard court fees.   Thankfully, this arrangement is no longer in place in Montgomery County, though as of 2015 attorneys and possibly litigants in a few states are still locked into similar third-party vendor contracts.   Currently, the Texas e-filing service offers a selection of filing providers, including free options.  Yet it is clear that when a court mistakenly conflates the need for a common vendor with the need for common standards, it distorts incentives.   It moves the center of power towards vendors, distancing it from user-centered interests and exacerbating the justice gap by prioritizing the preferences of third-party vendors over litigants.\n\nIV.  Catalyzing Change, A New Legal Standard:  Improving on Existing Tech Innovations Through Data Standards\n\nTechnologies aiming to disrupt the legal system in the service of access to justice should share the following principles: (1) collaboration between courts and other justice system partners; (2) user-centered design; and (3) an openness to change existing practices when the need for and efficacy of change is supported by evidence.  This section addresses several court-based technological innovations that, if designed using the above principles, can leverage shared data to dramatically improve access to justice.\n\nGoing to court is no simple task.  Imagine a world where you learn you have a legal problem in one office, then have to go to a different office to explain that problem and find out who can help you with that problem.  Armed with that information, you go to a third office, explain what you have learned, and obtain some help filling out a court form.  After getting help, you take that form to court.  Then to look up what happens next in your case, you have to come back or go to a different building altogether to see what happens next.\n\nFor many low- and moderate-income litigants navigating court processes, that is an accurate description of the steps they must take, with long periods of waiting on hold sprinkled in.  From the legal technologist’s perspective, the existing technology in the courts represents a replica of that inefficient and frustrating structure.\n\nTwo forms of technological solutions on the horizon may address this problem:  tools to guide people through understanding their legal problem and connecting to resources; and, for those already in the court system, tools to share information between court users and the courts.  These technologies already exist, and some courts use them, as well as private attorneys and legal aid organizations.   Their efficacy is limited by the lack of means for those tools to connect with one another.  Data standards will be the first step towards truly leveraging this technology to revolutionize the relationship between courts, justice system partners, and court users themselves.  Through data standards and data governance, courts have an opportunity to demolish existing silos and create an integrated system with a shared data infrastructure.\n\nA.  Automated Tools:  Triage and Expert Systems\n\n1.  Triage Explained\n\nCourts and attorneys engage in legal triage:  assigning a level of service to a person based on available resources and the nature of their legal need.   One of the challenges facing triage is how to look upstream, to provide information or points of entry into a decision-making system at an earlier stage in an individual legal or quasi-legal questions.   Many non-lawyers do not recognize when they are facing legal questions, misclassifying them as the subjects of some other domain.   Triage is at its most effective for people in the “information-gathering” stage, the point when someone begins to search for answers to what attorneys would consider a legal question, whether it has yet formed in the mind of the user as a legal question or not.   In the private bar, this idea has taken the form of a “legal checkup” as a way to uncover the latent legal market.   A legal checkup would provide an opportunity for would-be court users to assess their situations and possible paths for addressing their legal needs before reaching a crisis point.  \n\nTechnology-assisted triage is essentially the idea of an automated “sorting hat” that will connect people to the appropriate resource for their legal needs, without the need for an attorney.   Automated triage is well positioned for those seeking answers.  Individuals in the information-gathering stage first start with friends and family, or others in the community with a similar legal question or problem.  After that, they take to the Internet.  Here, a web-based automated triage tool can be accessed based on a user’s expressed needs or the query that a person puts forth, reducing the burden of outreach and communication between service providers and those who would benefit from services.\n\nReaching users early on in the process is only the first step in successful triage.  The next step is assessment of a legal question, often the most difficult step for a non-lawyer.   Interactive assessment has traditionally been the purview of attorneys explaining options with their client.  The goal of technology-assisted triage is for the user to procure a similar assessment through automated tools.\n\n2.  Improving Triage Through Data Standards\n\nMost triage systems across the civil legal justice community are fragmented—at a minimum, walling off the court system from the rest of the legal community.   At the same time, multiple entities develop overlapping (and competing) websites to answer questions and direct users to specific resources.\n\n[F]or triage to be successful on a systemic level, stakeholders cannot continue independently to design and deploy triage systems for litigants.  By definition, a litigant portal requires coordination between the courts, and the legal and non-legal service providers because litigant users will want these portals to provide access to legal and practical information. \n\nIn 2016, the Legal Services Corporation (LSC) and Microsoft announced a  pilot to develop a statewide, single triage portal.   The vision for this statewide triage portal takes users from initial information gathering, through assessment and decision-making, and ultimately connects them to the applicable legal resource, be it intake for a legal services provider, application for a government service, or electronic filing of a legal pleading.   The LSC-Microsoft Request for Proposals is remarkably ambitious in scope, but even that pilot does not explicitly address integrating existing information systems.   Standards and governance structures for data sharing are a necessary foundation for technology-assisted triage.\n\nData standards themselves will improve the quality of technology-assisted triage, primarily by reducing inefficiencies.  The current experience of verbally or manually providing the same information over and over again is inefficient for everyone involved, and can re-traumatize those users in crisis.  Whether it is the court or another actor, in order to share data across entities, someone will have to set standards that parties adopt and utilize.  The absence of standards assures that technology-assisted triage serves only as a means to connect people to services, missing an opportunity to eliminate duplicate data entry on the part of users. \n\nBuilding a data-sharing infrastructure also serves the purpose of bringing the various stakeholders together.  State courts are newcomers to the world of algorithmically-mediated triage, but they wield tremendous power to enhance the success of these efforts.  By establishing a structure for sharing data, courts will have the opportunity to start the conversation across the legal system.  In order for an automated triage tool to reflect the input of legal aid, the private bar, the judiciary, and administrative and social service agencies, those entities must be able to share data with one another. The courts are well positioned to facilitate all of these conversations.\n\nTo the extent that triage portals like the one contemplated in the LSC/Microsoft pilot go beyond diagnosis of a legal problem toward “guid[ing] self‐represented litigants through the entire legal process,” courts that adopt data standards and APIs will have the added benefit of enabling controlled access to their own case management systems, allowing for electronic filing directly from the triage portal.   This reduction in transaction costs can serve as the fuel for enumerable innovations.\n\nB.  Data Sharing Through e-Filing, Case Management Systems, and Electronic Records\n\nFirst federal courts, and now state courts, have added the option—and in some cases, the requirement—of electronically filing court documents.   Many courts have added this feature to their existing workflow without ensuring that the new technologies communicate with the old. The information in electronically filed documents cannot necessarily be imported into legacy systems.  Consequently, many courts find themselves entering data manually into multiple systems.  In some jurisdictions, for example, parties must still make paper filings that are electronically scanned at a later date.  In order to facilitate information sharing among parties, some courts require that orders and other forms must be printed or typed via typewriter onto carbon (or carbonless) copy paper: the white original for the court, pink for the initiating party, yellow for the responding party, etc.  Despite the fact that computers, photocopiers, and colored copy paper have existed for decades. In several cases – including domestic violence restraining orders – courts rely on this method for copying information despite it being difficult to read and edit.  \n\nFor the purposes of this discussion, we will consider interactive tools that assist litigants with clarifying their legal issue and populating applicable court forms with relevant information as one unit.  These interactive tools include expert systems,  including guided interviews (also called guided walkthroughs),  and document assembly programs.   When combined, these tools effectively develop an interface that assists users with a range of tasks, from decision-making to completion of court forms. \n\nFar more than simply electronically fillable forms, guided interviews help individuals through the procedural steps and key decision points in a legal question.   By eliciting only responses relevant to the specific legal issue, guided interviews populate necessary forms and reduce incorrect or inaccurate information.   Guided interviews can assist litigants in navigating complex court processes through limited decision trees, without requiring in-person assistance.   They essentially act as an automated intermediary, providing information relevant to the user in order to generate responses relevant to the court.\n\nThese automated tools face the same challenge that we see in other technological interventions:  lack of uniformity and fragmentation of civil justice system stakeholder efforts.   Legal services providers and courts use their own platforms for developing these tools, which are generally standalone and do not connect with other systems.\n\nA guided interview that results in document preparation will be most effective if the data obtained can be shared directly with the court’s case management system through electronic filing.  A few courts have begun to connect guided interviews and document assembly programs to electronic filing, often through proprietary arrangements with single vendors.   This can lead to stagnant or obsolete technology platforms due to vendor lock-in and institutional inertia.  \n\nBy connecting the e-filing mechanism to a user-centered interface that includes interactive information and decision-making tools, courts can move away from thinking of e-filing as simply another mode of submission towards a recognition that it is part of how the user interacts with the court worthy of thoughtful consideration.  Open data standards permit multiple actors to develop user interfaces that can link users to the courts, similar to the public transit examples discussed earlier, fostering competition that will ultimately improve the available products and reduce the burden on the court to develop bespoke tools.\n\nDespite occupying two sides of the same coin, input/writing and output/reading, electronic filing is often disconnected from electronic access to court records.  Public availability of court records has moved online on a completely separate path from the development of electronic filing.  Adopting electronic filing has been a slow, deliberative process within jurisdictions, as they weigh the effects of changing civil procedure.   Electronic sharing of court records, on the other hand, has been largely driven by considerations of technical capacity.  It is now technically feasible to make any electronically stored court information available to the public, ranging from case numbers and party names to court dates to the pleadings themselves.  Jurisdictions across the country have found themselves in the uncomfortable position of having unintentionally made too much information available to the general public online, enabling credit reporting agencies, employers, landlords, and others to use court data for improper purposes.   At the same time, with very few exceptions, courts have not developed systems to enable litigants to see details of their own cases.  \n\nWhile electronic filing and electronic court records were not universally developed with litigants in mind, they can be adapted to meet litigants’ needs.   By establishing and committing to data standards, courts have the opportunity to build connections across entities and ultimately integrate systems so that an individual will be able to go seamlessly from information-gathering through legal guidance to electronic submission of court pleadings without waiting in line, manually or verbally re-explaining her legal situation, or falling through the cracks.\n\nWith the capacity to move through each step in the court process online, why have a physical courthouse?  The American Bar Association and other groups have embraced online dispute resolution (ODR), perhaps the most extreme example of legal technological disruption of the court system.   Leaving aside the policy discussion of whether they are viable or desirable, ODR platforms will be most effective when all entities involved can share information with one another through a common language.  \n\nV.  Building on a Culture of Data Sharing to Aid the 100% Access Movement\n\nThe previous section described tools that would generate shared information and communication between courts, legal aid organizations, other government entities, and social service organizations.  Beyond setting standards for how that data is shared, what would be required to implement those tools effectively? \n\nIn addition to technology itself, the adoption of data standards offers a paradigm shift in the role of the courts as part of a larger justice ecosystem.  This shift is long overdue.  While courts are always included in broader multi-stakeholder discussions of access to justice, they are typically at arm’s length, not subject to the oversight or critique of entities that represent court users, like the private bar or legal aid organizations.  Further, collaborations between justice system partners encounter two contradictory faces of state trial courts.  Court leaders—particularly judges—play an increasingly strong leadership role in promoting and setting priorities for collaborative access to justice initiatives.   At the same time, most local and state-level court administrators generally do not partner with legal aid providers or other similar legal advocates on court-initiated efforts, even when it concerns implementing priorities that have been set out by judicial leadership.\n\nWe are beginning to see signs of a shift in this kind of thinking.  The Public Welfare Foundation, building upon the Resolution for Meaningful Access to Justice for All passed by the Conference of Chief Justices and Conference of State Court Administrators in 2015, created a competitive grant process to encourage state-level stakeholders to develop comprehensive, collaborative approaches to foster access to justice for all.   Seven states received grants, and as of 2017, they are in the process of strategic action planning.   The Justice for All grant process is but the latest example of an approach that continues to take place in court systems, legal services providers, and access to justice commissions across the country.  These approaches take stock of the legal needs and available resources within a jurisdiction and pilot solutions to observed legal needs.   The Justice For All approach is somewhat novel in that it emphasizes collaboration and coordination across the justice system—including the state bar, legal aid providers, the state court system, and also social service providers and others not traditionally considered access to justice stakeholders.  Also unique in the Justice For All process is the characterization of the courts as partners to the rest of the civil justice ecosystem.\n\nFor initiatives like Justice For All to be sustainable, they require more than an ad hoc approach to multi-stakeholder “strategic action planning.”   They require a culture of communication and collaboration that does not yet exist.  We believe that the process of developing and implementing data standards is the missing link that could lay the groundwork for communication and collaboration in other settings.\n\nStrengthening the court system’s interoperability with others’ is but one example of how data standards can initiate a conversation about the changing role of the courts in the larger civil justice ecosystem.  Engagement with legal aid and the private bar can inform court decision-making and judicial education in ways that improve the system overall.  Similar to the judicial decision-making process itself, judicial administration and court policy must become “deeply informed about the institutions with which legal actors interact . . .” as well as informed by court users, particularly people without lawyers.   Initiating a conversation about data sharing is often overlooked as a potential first step towards understanding court operations from the perspective of those who use the courts. Such an understanding is paramount in creating true “demand side reform.” \n\nThrough a collaborative effort with other stakeholders, state courts benefit from examining court processes from a user’s perspective.  Redesign efforts  outline a process for mapping the court user experience, identifying gaps, recommending solutions to address, and experimenting with model solutions to assess their effectiveness. \n\nVI.  Conclusion\n\nData standards are inevitable.  They are a necessary ingredient for the success of current and future technological innovations.  These standards will express the priorities of those organizations active in their creation or selection.  The power to set standards, be it through creation or adoption, is the power to determine their impact, their value.\n\nAs the forum for hearing disputes and the keeper of records in such, courts have a unique role historically and today.  The courts dictate the form by which individuals and institutions interact with the court system, and by extension, with one another.\n\nBy leading a movement toward the acceptance of open data standards, courts stand to benefit by reducing the burden of creating new tools and by providing a forum for relevant stakeholders to interact.  Those who stand to benefit most are low- and moderate-income court users, the intended audience for many of the technology tools envisioned here.  Currently court users are forced to run a gauntlet of overlapping and redundant services.  Establishing governance and communication protocols in relation to standards will expand communication between institutions and legal actors, reducing the silo effect and generating more opportunities for courts to integrate feedback from users.\n\nThe advent of data standards is not the only path forward.  It is however, a low-cost, high-reward investment, a mechanism for connecting technologies and driving conversations.  Whether courts create new standards or adopt and expand existing ones, they should focus first on establishing a common language.\n\nData standards represent a foundation upon which we can build innovative technologies that will move beyond the status quo, allowing us to simplify court processes, and transform court systems to better serve the needs of their users.\n\nFOOTNOTES:\n\n* Data Scientist, Massachusetts Committee for Public Counsel Services (CPCS); J.D., Boston University School of Law, (2011); M.Ed., Harvard Graduate School of Education (2002).  The opinions expressed here are the author's own and do not reflect those of CPCS or the Commonwealth of Massachusetts.\n\n** Associate Director of Field Research, Access to Justice Lab at Harvard Law School & Commissioner, Massachusetts Access to Justice Commission; J.D., Harvard Law School (2010).\n\nPortions of this article derive from presentations made by the authors at the 2016 Suffolk Legaltech Symposium.  Additional portions were adapted from a CPCS blog post accompanying a public comment in reply to the Massachusetts Trial Court’s 2016 proposed rule change regarding access to court records. Many thanks to CPCS Chief Information Officer Daniel Saroff who assisted greatly in the drafting of the comment and blog post.  Many thanks as well to Professor Gabe Teninbaum, Judge Dina Fein, HLS Library Innovation Lab Director Adam Ziegler, the staff of the Suffolk University Law Review, and the community of writers at Lawyerist for their title suggestions, esp. Andrew Cabasso.\n\nSee CLAYTON CHRISTENSEN, THE INNOVATOR’S DILEMMA: WHEN NEW TECHNOLOGIES CAUSE GREAT FIRMS TO FAIL XIII(Harvard Bus. Rev. Press 2013 ed.) (analyzing effects of disruptive technology on well-managed companies).  See also Raymond H. Brescia et al., Embracing Disruption: How Technological Change in the Delivery of Legal Services Can Improve Access to Justice, 78 ALB. L. REV. 553, 555-66 (2015) (discussing The Innovator’s Dilemma and technological disruption in the legal profession).\n\nSee Who We Are, CODE FOR AMERICA, https://www.codeforamerica.org/who/ (last visited Feb. 16, 2017) [https://perma.cc/QCN4-ATJC] (describing the civic coding work or Code for America); See STANFORD CODEX CENTER FOR LEGAL INFORMATICS, Legaltechlist, http://techindex.law.stanford.edu/ (last visited Feb. 16, 2017) [https://perma.cc/7AYT-RJ3P]\n\nSee Gerard J. Clark, Internet Wars:  The Bar Against the Websites, 13 J. HIGH TECH. L. 247, 270 (2013) (noting LegalZoom’s “automated online checks” and reviews of documents).  Machine intelligence is disrupting the legal profession in the areas of discovery, document automation, predictive analysis, and even the generation of legal briefs and pleadings.  See John O. McGinnis & Russell G. Pearce, The Great Disruption:  How Machine Intelligence Will Transform the Role of Lawyers in the Delivery of Legal Services, 82 FORDHAM L. REV. 3041, 3046 (2014) (indicating improved predictive analytics affect all areas of legal technological change).  See also Clark, supra, at 296 & 296 n.298 (positing law firms outsourcing productions and e-discovery at “more sophisticated level”).  Id.\n\nSee generally DEBORAH L. RHODE, ACCESS TO JUSTICE 100 (2004) (illustrating court as policymaker and administrator, as well as judicial decision-maker).\n\n Compare Farhad Manjoo, Space Invaders:  Why you should never, ever use two spaces after a period, SLATE (January 12, 2011), http://www.slate.com/articles/technology/technology/2011/01/space_invaders.html [https://perma.cc/M7PV-6RMB] (arguing for one space based on typography industry-consensus)with Heraclitus, Why Two Spaces After a Period isn’t Wrong (or, The Lies Typographers Tell About History), HERACLITEAN RIVER (November 1, 2011), http://www.heracliteanriver.com/?p=324 [https://perma.cc/6L9C-C3U6] (arguing one space argument based on false historical accounts).  Tongue firmly in cheek, as a matter of probabilities, the number of people with the job title lawyer outnumbers that of other clearly delineated writing professions.  See BUREAU OF LABOR STATISTICS, A-Z Index, Occupational Outlook Handbook, available at https://www.bls.gov/ooh/a-z-index.htm (showing the number of lawyers, 778,700, to far exceed that of writers and authors, 136,500). \n\nSee generally MARSHALL MACLUHAN, THE MEDIUM IS THE MESSAGE (2005).  It has been argued that the medium is the message, and there is ample social science to support the argument that context, including form, can influence cognition.  For example, consider the havoc that ensues when asked to read the name of a color printed in a hue other than the color the word describes.  See John Ridley Stroop, Studies of Interference in Serial Verbal Reactions, 18 J. EXPERIMENTAL PSYCHOL. 643, 646-48 (1935) (detailing psychological experiment underpinning the Stroop effect).  The authors of this article intend to engage with the question of form in an idealized sense. It may be that the form of a piece can influence a reader's impression of content, especially when conventions of form are ignored.  And it is also true that the art of persuasion can lean heavy on the exploitation of cognitive biases, such that the true substance of an argument is often intentionally obscured, but it is clearly the goal of standardized formatting to minimize these extraneous influences.\n\nCf. Richard A. Posner, What is Obviously Wrong with the Federal Judiciary Yet Eminently Curable, Part I, 19 GREEN BAG 2D 187, 197 (2016) (“Clarity, not eloquence, is the only attainable, though not attained, literary goal of modern judicial writing . . .”), http://www.greenbag.org/v19n2/v19n2_articles_posner.pdf \n\nSee infra note 68 and accompanying text (explaining courts requiring electronic filing).\n\nSee Maxstoke v. Martyn, YB 2 Rich. 2, Trinity, pl. 6 (1378) (Eng.), reprinted in 1 Ames Foundation 20, 22 (1975) (citing case establishing widows’ property rights).  This instance was brought to our attention thanks to the work of Byron D. Cooper. See Cooper, infra, note 11 (discussing English Year Books).\n\nSee Byron D.Cooper, Anglo-American Legal Citation:  Historical Development and Library Implications, 75 LAW LIBR. J. 3, 6 (1982) (discussing English origins of history of legal citation).\n\nSee id. at 17-23 (discussing historical and current American legal citation systems).\n\nSee iCalendar Resources, Specification and Tools, ICALENDAR.COM, https://icalendar.org/ (last visited Mar. 12, 2017) e.g., (describing the history and specifications of a common standard for the communication of calendar events).\n\nThe ordering of four-digit year, two-digit month, two-digit day, used widely by computer programmers, benefits from the fact that a simple sorting from smallest to largest produces a chronological ordering. \n\nSee David Bixenspan, UK Media Locked Out of White House Amidst Confusion Over Date Formatting, MEDIATE (January 27, 2017), http://www.mediaite.com/online/uk-media-locked-out-of-white-house-amidst-confusion-over-date-formatting/ [https://perma.cc/9XCU-Py4Y] (discussing inability of some foreign reporters to attend White House press conferences due to formatting error).\n\nLisa Grossman, Nov. 10, 1999: Metric Math Mistake Muffed Mars Meteorology Mission, WIRED (November 10, 2010), https://www.wired.com/2010/11/1110mars-climate-observer-report/ [https://perma.cc/TQP9-62LT] (explaining how software using different measurements failed to correctly interact, leading to disaster).\n\nSee WC3 HTML, WORLD WIDE WEB CONSORTIUM, https://www.w3.org/html/ (last visited Mar. 12, 2017)\n\nSee The Year to Free California’s Case Law “for Publication by Any Person,” CITING LEGALLY (January 9, 2017), http://citeblog.access-to-law.com/?p=765 [https://perma.cc/TFU8-Q295] (illustrating move from print to electronic materials).\n\nCOMMITTEE FOR PUBLIC COUNSEL SERVICES, Supporting an Open Standards Approach for MassCourt Data, PUB. COUNS (Apr. 22 2016), https://www.publiccounsel.net/blog/2016/04/22/data-standards-api/ [https://perma.cc/HP5J-YV7D] (promoting open data standards approach for Massachusetts trial courts).\n\nSee OAuth, OAUTH, (last visited February 16, 2017) https://oauth.net/ [https://perma.cc/RC7Y-UF8W] (introducing authorization framework for third-parties to obtain access to applications). \n\nSee EXTENSIBLE MARKUP LANGUAGE (XML), WORLD WIDE WEB CONSORTIUM, https://www.w3.org/XML/ [https://perma.cc/ 5SU3-LRRH] (last modified Oct. 11, 2016). XML “is a simple, very flexible text format derived from standard generalized markup language.”  Id.\n\nSee No-Cost Improvements to Child Support Enforcement:  Hearing on H.R. 10 Before the Subcommittee on Human Resources of the House Committee on Ways and Means, 112th Cong. 27 (2012) (statement of Craig D. Burlingame, Chief Information Officer, Trial Court Information Services, Massachusetts Trial Court System, https://waysandmeans.house.gov/UploadedFiles/Craig_Burlingame_Testimony_No_Watermark.pdf [https://perma.cc/XEJ5-WUSX] (highlighting benefits attainable through implementation of technology standard).  In his statement to the Subcommittee on Human Resources, Craig Burlingame stated “[s]ound standards establish a technological vocabulary that allows parties with various perspectives to speak the same language when discussing electronic information and data exchanges.  Further, the existence of quality standards provides a level playing field for the vendors that provide software and services to the governmental entities using them.”  Id. \n\nSee e.g., McHugh, infra note 31 (namely the inclusion of an open data standards article in a book on open data) .\n\nSee Overview of Open Standards, Free Software Found. Eur. (last visited Feb. 16, 2017), https://fsfe.org/activities/os/def.en.html [https://perma.cc/MC6C-BJBF] (providing widely adopting definition of open standards). \n\nSee Open Data, Open Data Handbook (last visited Feb. 16, 2017), opendatahandbook.org/glossary/en/terms/open-data [https://perma.cc/BTR3-SDUH] (providing summary of concept of open data).\n\nSee Unicode Technical Report, UNICODE CONSORTIUM (last visited Feb. 16, 2017), http://unicode.org/reports/tr51/index.html [https://perma.cc/H8ME-FTRA] (providing technical information to ensure conformance with Unicode standard).\n\nSee Hannah Miller et al., “Blissfully Happy” or “Ready to Fight”:  Varying Interpretations of Emoji, 2016 TENTH INT’L AAAI CONF. ON WEB & SOC. MEDIA 259, 259 (describing potential ambiguity of emoji due to varying individual platforms); Jeremy Burge, Apple And The Gun Emoji, EMOJIPEDIA (Aug. 4, 2016), http://blog.emojipedia.org/apple-and-the-gun-emoji/ [https://perma.cc/L5L2-BNZP] (describing Apple’s change of pistol emoji to green watergun emoji).\n\nSee John S. Hollywood & Zev Winkelman, Improving Information-Sharing Across Law Enforcement: Why Can’t We Know?, NAT’L CRIM. JUSTICE REF. SERV. 1-2 (2015), https://www.ncjrs.gov/pdffiles1/nij/grants/249187.pdf [https://perma.cc/B6N4-DV5D] (illustrating instances of law enforcement use of NEIM system).\n\nCOMMITTEE FOR PUBLIC COUNSEL SERVICES, supra note 20.\n\nSee John Carlo Bertot et al., Big Data, Open government and e-Government: Issues, Policies and Recommendations, 19 INFO. POLITY 5, 5-6 (2014) (describing changes to data governance, open data policies under the Obama Administration).  One example of these municipal and federal initiatives is Open 3-1-1, http://www.open311.org/, and Data.gov, https://www.data.gov/. \n\nBibiana McHugh, Pioneering Open Data Standards: The GTFS Story, in BEYOND TRANSPARENCY:  OPEN DATA AND THE FUTURE OF CIVIC INNOVATION 125 (2013), http://beyondtransparency.org/pdf/BeyondTransparency.pdf [https://perma.cc/L2LQ-8TQR] (noting transit data available but rarely put out for public online access).\n\nId. at 125.\n\nId. at 125-126.\n\nId. at 126.\n\nId. at 126.\n\nSee id. at 130.\n\nSee id. at 133 (referencing the MBTA, a subsidiary of MassDOT).  GTFS-RT was also developed in collaboration with TriMet, BART, and MTS.  Id.\n\nTo be clear, this is not a suggestion that courts offload work to civic hackathons, which are often limited in their scope and available resources. \n\nCompare Data.gov, http://data.gov [https://perma.cc/KG6B-4Q26] (providing public access to government data), with CONFERENCE OF CHIEF JUSTICES (CCJ) AND CONFERENCE OF STATE COURT ADMINISTRATORS (COSCA), RESOLUTION 13 (2001), http://www.ncsc.org/Services-and-Experts/Technology-tools/Court-specific-standards/CCJ-Resolution.aspx [http://perma.cc/RU3A-Y3JM] (resolving to begin implementation of data standards).\n\nSee NIEM’s History, NAT’L INFO. EXCHANGE MODEL, https://www.niem.gov/about-niem/history (last accessed Feb. 16, 2017) [https://perma.cc/Z4V2-H5JZ].\n\nSee id. (describing history of NEIM); see also Thomas C. Carlson, The National Information Exchange Model: An Update, FUTURE TRENDS IN STATE COURTS at 26 (2007), http://www.ndcrc.org/sites/default/files/future_trends_2007.pdf [https://perma.cc/K4XW-JZFS].  NIEM was based on the Global Justice XML Data Model (GJXDM) that originated in 2003.  Id. (describing Global Justice Data XML as “seed material” for NIEM).\n\nNIEM is but one in a constellation of complementary (and competing) national data standards.  See NATIONAL STANDARDS, NAT’L CTR. ST. CTS., http://www.ncsc.org/Services-and-Experts/Technology-tools/National-standards.aspx (last visited Feb. 24, 2017) [https://perma.cc/YCA2-SV5D] (listing other national data standards).  Other national standards initiatives in the state justice system space include Legal XML, a nonprofit that sets data standards specific to legal systems, including electronic court filing. See LegalXML, http://www.legalxml.org/about/index.shtml (last visited Feb. 24, 2017) [https://perma.cc/6JLM-3ZQ9] (introducing legal consortium sector). Data standards like Legal XML can also themselves adopt NIEM’s data architecture, meaning that a state court that utilizes the Legal XML standard for electronic court filing will also conform with NIEM standards. See Adam Angion & James Cabral, 1.3.1 National Information Exchange Model (NIEM), Electronic Court Filing Version 4.01 Plus Errata 01, OASIS, at 10 (July 14, 2014), http://docs.oasis-open.org/legalxml-courtfiling/specs/ecf/v4.01/ecf-v4.01-spec/ecf-v4.01-spec.pdf [https://perma.cc/7GHD-JTDL] (explaining that LegalXML worked with NIEM when developing ECF 4.0, its model for electronic court filing). \n\n\n\nA broad framework that can be applied to everything from emergency management to immigration to justice information exchanges, the NIEM is not intended to cover every data specification that state-level justice systems may require, particularly regarding civil cases. \n\nSee Hollywood & Winkelman, supra note 30, at 19 (“existing nationwide standards, technologies, and guidance are necessary but not yet sufficient.”).  Id.  \n\n See The Massachusetts Court System, The Future of IT:  Technology Visiting Committee Issues Report, MASS.GOV (Aug. 2016), https://www.mass.gov/courts/features/july-aug-2016-it-corner.html [https://perma.cc/UD3P-8KTG] (summarizing pilot electronic data system).  In Massachusetts, for example, the pilot Electronic Application for Criminal Complaint (EACC) allows information to be shared between law enforcement agencies and the court case management system.  Id.  Data standards also permit the Massachusetts court and the state’s child support enforcement agency to share data.  Id.  Common databases that aggregate limited data sets from multiple entities are similarly narrow in scope.  See Brittany R. Fuller et al., Data Integration for the Thomas Jefferson Area Community Criminal Justice System, SYS. & INFO. ENGINEERING DESIGN SYMP. (SIEDS) 300 (2014). DOI 10.1109/SIEDS.2014.6829924 (explaining pilot project in Virginia).  The project took exported data from current systems and integrated them into a single relational database with a Microsoft Sharepoint interface.  Id. \n\nWho Uses NIEM? Success Stories, Nat’l Info. Exchange Model, NAT’L EXCHANGE INFO. MODEL, https://www.niem.gov/about-niem/success-stories (last accessed Feb. 16, 2017) [https://perma.cc/P4S9-Q3DF] (highlighting Kansas, Florida, and Colorado court systems); see also Technology Tools, NAT’L CTR. STATE CTS., http://www.ncsc.org/Services-and-Experts/Technology-tools.aspx (last visited Feb. 16, 2017) [https://perma.cc/KCW8-VJ9V] (listing a number of services by the NCSC related to use of standards). \n\nSee HARVARD UNIV. INFO. TECH., REPORT OF THE MASSACHUSETTS COURT TECHNOLOGY VISITING COMMITTEE 1 (Spring 2016), http://www.mass.gov/courts/docs/bulletins/visting-committee-report-final-6-17-16.pdf [https://perma.cc/C458-J5RE] (Comment: no need for blurb here EC). \n\nId. at 7 (referencing application programming interfaces as mechanisms for integrating data sets across organizations).\n\nSee generally McHugh, supra note 32 (describing the history of GTFS).\n\nThe authors wrestled with the appropriate term for describing non-court actors interacting with the court, and although a number of identifiers were considered, the term user was agreed to be the most inclusive.  It is important, however, to remember always that users are people, not numbers.  In fact this is a core insight of user-centered design, a philosophy we endorse here. \n\nSee LEGAL SERVS. CORP. DOCUMENTING THE JUSTICE GAP IN AMERICA: THE CURRENT UNMET CIVIL LEGAL NEEDS OF LOW-INCOME AMERICANS 5 (Sept. 2009), http://www.lsc.gov/sites/default/files/LSC/pdfs/documenting_the_justice_gap_in_america_2009.pdf [https://perma.cc/3CU8-Z3Q2] (discussing legal issues arising from financial crisis); John Greacen, Self Represented Litigants and Court and Legal Services Responses to Their Needs: What We Know, CTR. FAMILIES, CHILDREN & CTS., CA ADMIN. OFFICE OF CTS. 1 (2003), http://www.courts.ca.gov/partners/documents/SRLwhatweknow.pdf [https://perma.cc/9V6J-WV3G]; Emily Savner, Expand Legal Services Now, NAT’L L.J. (June 28, 2010) http://www.brennancenter.org/blog/expand-legal-services-now [https://perma.cc/BMZ9-T4EX] (reporting increases in demand and 75% drop in Interest on Lawyers Trust Fund Accounts (IOLTA) funds between 2007 and 2009); Jonathan D. Glater, In a Downturn, More Act as Their Own Lawyers. N.Y. TIMES (Apr. 10 2009), http://www.nytimes.com/2009/04/10/business/ 10lawyer.html [https://perma.cc/8KJN-DH36] (illustrating increase in pro se litigants).\n\nSee Jessica K. Steinberg, Demand Side Reform in the Poor People’s Court, 47 CONN L. REV. 741, 752 (2015) (addressing survey results showing increase in pro se litigation).  In examining the characteristics of pro se litigants, studies have shown that “poverty is the primary force driving individuals to represent themselves in court.”  Id.  See also AM. BAR ASS’N, CIVIL RIGHT TO COUNSEL RESOLUTION 5, (2006) www.americanbar.org/content/dam/aba/administrative/legal_aid_indigent_defendants/ls_sclaid_06A112A.authcheckdam.pdf [https://perma.cc/G6DQ-CG6K] (recognizing need for increase in counsel access for low-income litigants); JUDICIAL COUNCIL OF CALIFORNIA, FACT SHEET: PILOT PROJECTS UNDER THE SARGENT SHRIVER CIVIL COUNSEL ACT 2, (2010), www.courts.ca.gov/15583.htm [https://perma.cc/43T7-J7WJ] (explaining pilot program aimed at increasing counsel services for low-income civil litigants); John Pollock & Michael S. Greco, It’s Not Triage if the Patient Bleeds Out, 161 U. PA. L. REV. PENNUMBRA 40, 40-41 (2012) (affirming need to address civil legal aid needs); Laura K. Abel, A Right to Counsel in Civil Cases: Lessons From Gideon v. Wainwright, 15 TEMP. POL. & CIV. RTS. L. REV. 527, 530 (2006) (focusing on scope of proposed civil right to counsel); Paul Marvy & Laura K. Abel, Current Developments in Advocacy to Expand the Civil Right to Counsel, 25 TOURO L. REV. 131, 132–33 (2009) (describing current efforts to address civil legal needs). \n\nSee STATEWIDE TASK FORCE TO EXPAND CIVIL LEGAL AID IN MASS., BOS. BAR ASS’N, INVESTING IN JUSTICE:  A ROADMAP TO COST-EFFECTIVE FUNDING OF CIVIL LEGAL AID IN MASSACHUSETTS 1, n.1 (2014), http://www.bostonbar.org/docs/default-document-library/statewide-task-force-to-expand-civil-legal-aid-in-ma---investing-in-justice.pdf [https://perma.cc/4C9K-WSSB] (suggesting most needy Massachusetts litigants denied access to justice).  “In Massachusetts, civil legal aid programs turn away 64% of all eligible cases.”  Id. \n\nSee Russell Engler, Access to Justice and the Role of the Private Practitioner, 24 KAN. J.L. & PUB. POL’Y 554, 567-69 (2014) (describing how private practitioners can generate fees while also addressing justice gap); see generally Deborah L. Rhode, Access to Justice:  A Roadmap for Reform, 41 FORDHAM URB. L.J.1227 (2013).\n\nAlan Houseman, The Justice Gap, in CENTER FOR AM. PROGRESS, CLOSING THE JUSTICE GAP: HOW INNOVATION AND EVIDENCE CAN BRING LEGAL SERVICES TO MORE AMERICANS 21 (2011), http://cdn. americanprogress.org/wp-content/uploads/issues/2011/06/pdf/prose_all.pdf [https://perma.cc/QX2W-E9RT] (defining the justice gap). \n\nSee Hon. Dina Fein, First Justice, Western Division Housing Court, W. NEW ENG. L. REV. (2017) (forthcoming). (need to get source EC)\n\nSee Jennifer Smith, Rationed Justice, 49 SUFFOLK U.L. REV. 353, 365 (2016) (noting need for civil legal aid funding).  The lack of funding creates a need for creative, low-cost solutions.  See Russell Engler, Towards a Context-Based Civil Gideon Through Access to Justice Initiatives, 40 CLEARINGHOUSE REV. 196, 200 (2006) (expounding on importance of measuring efficacy of new solutions); Steve Eppler-Epstein, The Fight for Legal Aid Funding and Right to Counsel Advocacy:  An Incremental Approach and an Overarching Message, 26 MGMT. INFO. EXCHANGE J. 41, 43-44 (2012) (proposing alternative forms of assistance).\n\nSee RICHARD E. SUSSKIND, TOMORROW’S LAWYERS:  AN INTRODUCTION TO YOUR FUTURE 84-91 (2013) (emphasizing importance of access to justice initiatives in future legal landscape).  As early as 1994, court administration experts were predicting a dramatic shift in the way courts do business as a result of technological innovation.  RICHARD E. SUSSKIND, THE END OF LAWYERS?:  RETHINKING THE NATURE OF LEGAL SERVICES 270 (2008) (predicting future of legal sector); see also Sohail Inayatullah, The Future of State Court Administration, Futures Res. Q., 16 (Spring 1986), http://www.metafuture.org/library1/JudiciaryFutures/future-of-state-court-administration-frq-1994.pdf [https://perma.cc/8JYM-TVGX] (citing Dator & Bezold, Judging the Future (1981)); Hon. P.J. De Muniz, Building Tomorrow’s Courts Today, in FUTURE TRENDS IN STATE COURTS 2009, 82 (C. Flango et al. eds.); Hon. R.B. Shore et al., The ROI of Emerging Technologies, presented at the National Court Technology Conference, Denver, CO (Sept. 2009) (presenting on technological innovation in Florida courts); Jim Dator, When Courts are Overgrown with Grass:  The Role of Courts in the 21st Century, 32 FUTURES 183, 189 (2000) (describing societal change’s impact on legal services); Joseph F. Coates, Law and Technology in the Twenty-First Century, 52 TECH. FORECASTING & SOC. CHANGE 255, 259-60 (1996) (describing potential legal/technological issues).\n\nSee Russell Engler, And Justice for All—Including the Unrepresented Poor:  Revisiting the Roles of the Judges, Mediators, and Clerks, 67 FORDHAM L. REV. 1987, 2022–23 (1999) (noting changes to court rules to help promote fairness and justice).  But see John Greacen, Legal Information vs. Legal Advice:  Developments During the Last Five Years, 84 JUDICATURE 198, 203 (2001) (arguing courts can address imbalance without modifying current rules).  \n\nSee Bruce Tonna et al., Future of the Courts: Fixed, Flexible, and Improvisational Frameworks, 44 FUTURES 802, 810 (2012) (““ . . . We must anticipate and address potential resistance to changes to the court system. Traditionally, justice in the U.S. has focused on maintaining the status quo; legitimacy is the presumed outcome of this consistency.”).\n\nSee Steinberg, supra note 53, at 755-56 (discussing problematic errors made by pro se litigants and their threat to judicial process). \n\nSee id. at 754 (describing “complex, and often counter-intuitive, procedures”).  Id. \n\nSee Richard Zorza, Some First Thoughts on Court Simplification:  The Key to Civil Access and Justice Transformation, 61 DRAKE L. REV. 845, 863-64 (2012) (asserting procedural simplification will lead to substantive simplification in excessive complexity of current system).\n\nSee Instructions for Alternate Service, ALASKA COURT SYSTEM http://courts.alaska.gov/shc/family/shc-184.htm (last visited Feb. 16, 2017) [https://perma.cc/9R2V-H4KS] (permitting service by website and other alternate means after “diligent inquiry”). \n\n564 U.S. 431 (2011). \n\nId. at 477; see also MODEL CODE OF JUDICIAL CONDUCT r. 2.2, (AM. BAR ASS’N 2007).\n\nSee Laura K. Abel, Turner v. Rogers and the Right of Meaningful Access to the Courts, 89 DENV. U.L. REV. 805, 807 (2012) (describing Turner-mandated assistance); Russell Engler, Turner v. Rogers and the Essential Role of the Courts in Delivering Access to Justice, 7 HARV. L.  & POL’Y REV. 31, 36 (2013) (documenting increase in access to justice initiatives); Richard Zorza, Turner v. Rogers:  The Implications for Access to Justice Strategies, 95 JUDICATURE 255, 257-58 (2012) (exploring what programs, mandates Turner requires).  Instructions that accompany court forms are moving toward lower reading levels and increased “plain language.” PUB. L. 111–274 (2010), https://www.gpo.gov/fdsys/pkg/PLAW-111publ274/pdf/PLAW-111publ274.pdf [https://perma.cc/A3N4-K77B].  Courts are encouraged to adopt the principles of the Plain Writing Act of 2010, which requires all federal agencies to use “clear government communication that the public can understand and use.”  Id. \n\nSee Russell Engler, Ethics in Transition:  Unrepresented Litigants and the Changing Judicial Role, 22 NOTRE DAME J. L. ETHICS & PUB. POL’Y 367, 387-88 (2008) (noting criticism of passive judiciary with pro se litigants); Richard Zorza, The Disconnect Between the Requirements of Judicial Neutrality and Those of the Appearance of Neutrality When Parties Appear Pro Se:  Causes, Solutions, Recommendations, and Implications, 17 GEO. J. LEGAL ETHICS 423, 429-30 (2004) (analyzing how judges assist pro se litigants and maintain neutrality simultaneously.  See also Paula Hannaford-Agor, Helping the Pro Se Litigant: A Changing Landscape, Court Rev. (2003); Rebecca Albrecht et al., Judicial Techniques for Cases Involving Self-Represented Litigants, 42 JUDGES J. 1 16, 16-17(2003) (surveying case law for techniques for adjudicating cases with pro se litigants). \n\nSee CONF. OF CHIEF JUST. & CONF. OF ST. CT. ADMIN., RESOLUTION 5:  REAFFIRMING THE COMMITMENT TO MEANINGFUL ACCESS TO JUSTICE FOR ALL (2015), http://www.ncsc.org/~/media/Microsites/Files/CCJ/Resolutions/07252015-Reaffirming-Commitment-Meaningful-Access-to-Justice-for-All.ashx [https://perma.cc/46QC-YP7F] (noting advancements in creating access to effective assistance of counsel). The Resolution cites several new initiatives, many of which are covered in this article, including “expanded self-help services to litigants, new or modified court rules and processes that facilitate access, discrete task representation by counsel, increased pro bono assistance, effective use of technology..., and triage models to match specific needs to the appropriate level of services[.]” \n\nSee TECHNOLOGY INITIATIVE GRANT PROGRAM, LEGAL SERVS. CORP., (2017) http://www.lsc.gov/grants-grantee-resources/our-grant-programs/tig [https://perma.cc/N5KS-LPNF] (providing overview of TIG eligibility requirements and applicable law).\n\nSee TECHNOLOGY INITIATIVE GRANT HIGHLIGHTS AND IMPACT, LEGAL SERVS. CORP., (2017) http://www.lsc.gov/grants-grantee-resources/our-grant-programs/technology-initiative-grant-program/technology [https://perma.cc/6F87-S97D] (describing successful programs utilizing TIG to enhance access).\n\nSee Richard T. Watson, Africa’s Contributions to Information Systems, 5 AFRICAN J. INFO. SYS. 126, 128 (2013) (Comment: do not think we need a blurb here EC).\n\nClark, supra note 5, at 260-61 & nn.79-83 (citing multiple court-based self-help websites); see also Ursula Gorham, Facilitating Access to Legal Information by Self-Represented Litigants: An Exploratory Case Study of the People’s Law Library of Maryland, 2 J. OPEN ACCESS L. 1, 8 (2014) (examining law libraries of Maryland through case study).\n\nSee Dana D. Dyson & Kathryn Schellenberg, Access to Justice: The Readability of Legal Services Corporation Legal Aid Internet Services, 20 J. POVERTY 1 (2016).\n\nA google search for “divorce” in Massachusetts in March 2017, for example, resulted in the following top ten search results: .Divorcenet.com, Divorcesource.com, and Pransky.com (a law firm). The authors encourage the reader to conduct a search with the search engine and jurisdiction of their choice.\n\nSee Courtney Minick, LexisTexas:  Privatizing Access to Public Courts, JUSTIA LAW BLOG (Feb. 4, 2011), https://lawblog.justia.com/2011/02/04/lexistexas-privatizing-access-to-public-courts/ [http://perma.cc/F7CP-UKDA] (explaining court order mandating electronic filing).\n\nSee id. (detailing costs for electronic filing).\n\nSee Barbara Gladden Adamick, District Clerk Implements New E-Filing System February 1, 2014, MONTGOMERY COUNTY, TEXAS http://www.mctx.org/departments/departments_d_-_f/district_clerk/electronic_filing.php (last visited Feb. 16, 2017) [https://perma.cc/5A4B-JGZB].  Some jurisdictions appear to maintain the type of restrictive contract that Montgomery County ended.  See R. Lainie Wilson Harris, Ready or Not Here We E-Come:  Remaining Persuasive Amidst the Shift Towards Electronic Filing, 12 LEGAL COMM. & RHETORIC:  JAWLD 83, 90-93, 98, 100, 102-04 (2015) (describing the status of each state’s rules and vendors for electronic filing). Harris’s survey of e-filing requirements across states seems to indicate that attorneys are required to use a single vendor for mandatory electronic filing certain case types and locations within several jurisdictions:  Arizona appellate courts (AZTurboCourt, not yet rolled out at trial level); Delaware (civil complaints using File & Serve Xpress); and several courts that have adopted Tyler Technologies Odyssey File & Serve (Michigan pilot courts, several Minnesota districts, Oregon, and South Dakota.  See id.  In addition to third-party vendors, several states have developed their own in-house platforms that are the only option for complying with mandatory e-filing:  Colorado, New York, and Summit County Ohio.  See id.\n\nSee Minick, supra note 77(noting Montgomery County received one dollar back from service provider per filing).\n\nSee Michael J. Wolf, Collaborative Technology Improves Access to Justice, 15 N.Y.U. J. LEGIS. & PUB. POL'Y 759, 771 (2012) (arguing online dispute resolution, document assembly, other “collaborative” technology solutions benefitting pro se litigants); see also Stephanie Kimbro, Using Technology to Unbundle in the Legal Services Community, HARV. J.L. & TECH. OCCASIONAL PAPER SERIES 1, 14-15 (Feb. 2013) (remarking on how groups can unbundle legal services with help of technological innovation). \n\nThomas M. Clarke & Victor E. Flango, Triage: Case Management for the 21st Century, NAT’L CTR. FOR STATE COURTS: FUTURE TRENDS IN STATE COURTS 146, 146 (2012).\n\n\n\nSee Ross C. Brownson et al., Measuring the Impact of Public Health Policy, 7 PREVENTING CHRONIC DISEASE 1, 1 (July 2010) (borrowing “upstream” terminology from public health literature).  Upstream policy changes “affect large populations through regulation, increased access, or economic incentives” as opposed to “downstream” intervention that occurs on individual-level approaches.  Id. \n\nSee Rebecca Sandefur, What We Know and Need to Know About the Legal Needs of the Public, 67 S.C. L. REV. 443, 443-44 (2016) (highlighting obstacle for obtaining legal aid early on in process).\n\nSee REPORT ON THE FUTURE OF LEGAL SERVICES, ABA 43-44 (Aug. 2016), http://www.americanbar.org/content/dam/aba/images/abanews/2016FLSReport_FNL_WEB.pdf [https://perma.cc/9F7Q-87H7] (recommending “legal checkup” concept).\n\nSee id. \n\nSee id. (noting value of “legal checkups”); see also SUSSKIND, supra note 59, at 91 (discussing how technology can aid lawyers to assist “latent legal market”).  Id.  \n\nSee Richard Zorza, The Access to Justice “Sorting Hat”:  Towards a Triage and Intake that Maximizes Access and Outcomes, 89 DENV. UNIV. L. REV. 859, 874-75 (2012) [hereinafter Zorza, Sorting Hat] (providing new triage model for legal aid). \n\nSee RICHARD ZORZA, THE SELF-HELP FRIENDLY COURT:  DESIGNED FROM THE GROUND UP TO WORK FOR PEOPLE WITHOUT LAWYERS, National Center for State Courts 17-18 (2002), http://www.zorza.net/Res_ProSe_SelfHelpCtPub.pdf [https://perma.cc/7N9H-8SPP] (pointing out specific hurdles for pro se litigants).\n\nSee REBECCA L. SANDEFUR & AARON C. SMYTH, ACCESS ACROSS AMERICA:  FIRST REPORT OF THE CIVIL JUSTICE INFRASTRUCTURE MAPPING PROJECT ix (American Bar Foundation, 2011) (ebook) (providing comprehensive research on status of state civil legal aid infrastructure).  “The results [of the research] are sobering. They underscore a fundamental absence of coordination in the system, fragmentation and inequality in who gets served and how, and arbitrariness in access to justice depending on where one lives.”  Id.  (emphasizing gravity of authors’ findings). \n\nTom Clarke et al., Triage Protocols for Litigant Portals:  A Coordinated Strategy Between Courts and Service Providers, ST. JUST. INST./NAT’L CTR. ST. CT. 3 (Dec. 2013), http://www.srln.org/system/files/attachments/Triage_Protocols_for_Litigant_Portals__A_Coordinated_Strategy_Between_Courts_and_Service_Providers.pdf [https://perma.cc/F5X5-RVY5]. \n\nSee Legal Services Corporation Request for Proposals:  Microsoft Statewide Access Project, LEGAL SERV. CORP., 2, (Nov. 22, 2016), http://lsc.gov/sites/default/files/attach/2016/11/MS-statesidePortalProject.pdf [https://perma.cc/3B34-4A4L] (announcing project, requesting proposals). \n\nSee generally Thomas M. Clarke, Building a Litigant Portal:  Business and Technical Requirements, ST. JUST. INST./NAT’L CTR. ST. CT. (Nov. 2015), http://www.srln.org/system/files/attachments/Report%20Building%20a%20Litigant%20Portal%20%28Clarke%202015%29.pdf [https://perma.cc/L5E2-GB2X].  This is an expansive and aspirational view of triage.  Id. at 1-3 (providing vision for statewide triage portal).  While private entities like Avvo and nonprofits like the ABA Free Legal Answers Initiative begin to connect individuals with legal questions to answers (and attorneys), for the most part a direct connection between the initial search and the ultimate resource still does not exist. See Legal Services Corporation Request for Proposals, supra note , at 3 (describing the current “complex patchwork of services”).  \n\nSee generally MICROSOFT STATEWIDE ACCESS PROJECT, supra note 92, (failing to mention integration of existing systems).\n\nSee id. at 3 (explaining component of integrated service delivery system). \n\nHarris, supra note 79, at 89 & n.36 (2015) (listing thirty-one states requiring e-filing in at least some case types).  See also TIMOTHY A. GUDAS, E-FILING IN STATE APPELLATE COURTS:  AN UPDATED APPRAISAL 2-4 (Sept. 2014), http://www.appellatecourtclerks.org/publications-reports/docs/NCACC_E-Filing_White_Paper September2014.pdf [https://perma.cc/N4UH-4GH9] (describing origins of federal court electronic filing).\n\nSee, e.g., Lynne Marek, Patchwork E-Filing Frustrates Lawyers\n\nTHE NAT’L L.J., (Feb. 26, 2008), http://www.nationallawjournal.com/id=900005504188/Patchwork-EFiling-Frustrates-Lawyers?slreturn=20170215192225. Cf. Ben Farrell, Note, Spoliation in a Digital World:  Proposing a New Standard of Culpability in Massachusetts for an Adverse Inference Instruction, 14 SUFFOLK J. TRIAL & APP. ADVOC. 110, 112 (2009) (suggesting court treat electronic discovery documents the same way they treat paper ones). \n\nSee Randall Davis, Amplifying Expertise with Expert Systems, in THE AI BUSINESS: COMMERCIAL USES OF ARTIFICIAL INTELLIGENCE 18-19 (Patrick H. Winston & Karen A. Pendergast eds., 1984) (characterizing expert systems). \n\nSee Claudia Johnson, Online Document Assembly Initiatives to Aid the Self-Represented, in INNOVATIONS FOR SELF-REPRESENTED LITIGANTS 97, 98–99 (Bonnie Rose Hough & Pamela Cardullo Ortiz eds., 2011) (defining online document assembly); see also Kimbro, supra note 81, at 22 (defining guided walkthroughs).\n\nSee Deborah Saunders et al., Access Brief: Forms and Document Assembly 1, CTR. ON CT. ACCESS TO JUSTICE FOR ALL (2012), http://ncsc.contentdm.oclc.org/utils/ getfile/collection/accessfair/id/264/filename/265.pdf [https://perma.cc/3R7A-TETS] (describing how to provide clearly-written forms for common procedures).  The notion that document assembly could “transform the legal profession” originated in the 1970s. See Ronald Staudt, All the Wild Possibilities:  Technology That Attacks Barriers to Justice, 42 LOY. L.A. L. REV. 1117, 1122 & n.18 (2009) (citing James A. Sprowl, Developing Computerized Practice Aids for Tomorrow’s Law Practice, 12 LEGAL ECON. 37, 44 (1986)). \n\nSee Ronald W. Staudt & Andrew P. Medeiros, Access to Justice and Technology Clinics:  A 4% Solution, 88 CHI.-KENT L. REV. 695, 708-09 (2013) (discussing easy-to-use features of guided interview interfaces).\n\nSee Phil Malone et al., BEST PRACTICES IN THE USE OF TECHNOLOGY TO FACILITATE ACCESS TO JUSTICE INITIATIVES 24 (2010), http://cyber.law.harvard.edu/ sites/cyber.law.harvard.edu/files/A2J_Report_Final_073010.pdf [https://perma.cc/533Z-SDP6] (providing overview of guided interview process, benefits).\n\nSee Rochelle Klempner, The Case for Court-Based Document Assembly Programs:  A Review of the New York State Court System’s DIY Forms, 41 FORDHAM URB. L. J 1189, 1193 (2014) (describing New York court system use of A2J Author guided interviews).\n\nSee Kimbro, supra note 81, at 22 (describing guided interview system as useful self-help tool).\n\nSee James E. Cabral et al., Using Technology to Enhance Access to Justice, 26 HARV. J.L. & TECH. 241, 307, 323 (2012) (evaluating barriers to technological progress).\n\nSee Klempner, supra note 103, at 1193 (explaining success of New York City’s DIY forms).\n\nSee MINNESOTA I-CAN! FORMS, MINNESOTA JUDICIAL BRANCH, http://www.mncourts.gov/ican.aspx (last visited Feb. 16, 2017) [https://perma.cc/58BU-KW2K] (abandoning Legal I-CAN! software); see also Leila Atassi, Court Administrators Say Suspicious Contract Caused Court Reform Delays, CLEVELAND PLAIN DEALER BLOG (Jan. 4, 2010), http://blog.cleveland.com/metro/2009/12/court_administrators_say_suspi.html [https://perma.cc/QAM3-E56K] (recognizing challenges to implementing proprietary software in Ohio led to delays).\n\nSee James E. McMillan et al., A Guidebook for Electronic Court Filing, National Center for State Courts 4 NAT’L CTR. ST. CTS. (Williamsburg, VA 1998) (discussing policy and practice considerations for initiating electronic filing).The federal courts’ Public Access to Court Electronic Records (PACER) system is an exception in that it combines e-filing and electronic access to records: PACER makes each case’s docket sheet and substantive pleadings available to the parties and the public.\n\nSee Rudy Kleysteuber, Note, Tenant Screening Thirty Years Later:  A Statutory Proposal to Protect Public Records, 116 YALE L.J. 1344, 1349 & n.20 (2007) (describing tenant blacklists created by culling court records of eviction proceedings).\n\nSee Trial Court Rule XIV:  Uniform Rules on Public Access to Court Records, MASS. CT. SYS., http://www.mass.gov/courts/case-legal-res/rules-of-court/trial-court/tc-rule-14-records/ (last visited Feb. 24, 2017) [https://perma.cc/E6LY-5LSV] (granting attorneys access to electronic case records).  The rule does not grant remote electronic access of “audio, audiovisual, or electronic images” to the general public.  See id.  (noting lack of public access).  But see My Court Card Portal, SUPERIOR CT. OF CAL., https://selfhelp.occourts.org (last visited Feb. 24, 2017) [https://perma.cc/S5HF-GPJX] (creating self-help portal for general public). \n\nSee Peter W. Martin, Online Access to Court Records-from Documents to Data, Particulars to Patterns, 53 VILL. L. REV. 855, 864 (2008) (emphasizing technological progress ignoring general public and litigants).  ”The federal courts did not establish computer-based case management systems or subsequent electronic filing and document management systems in order to provide the public with better access to court records. Those systems were created because they offered major gains for judges and court administrators.  See id. \n\nSee AM. BAR ASS’N, REPORT ON THE FUTURE OF LEGAL SERVICES, supra note 85, at 19, 47 (recommending court-annexed ODR); JOINT TECHNOLOGY COMMITTEE, ONLINE DISPUTE RESOLUTION AND THE COURTS 1, 3 (Nov. 30 2016), http://www.ncsc.org/~/media/Files/PDF/About%20Us/Committees/JTC/ODR%20QR%20final%20V1%20-%20Nov.ashx [https://perma.cc/CN58-JDVS] (explaining court use of ODR); Suzanne Van Arsdale, User Protection in Online Dispute Resolution, 21 HARV. NEGOT. L. REV. 107, 109-10 (2015) (highlighting increased use of ODR). Additionally, it is worth noting that expanding the use of ODR in to the criminal justice context would clearly raise questions under the Confrontation Clause of the Sixth Amendment to the United States Constitution.. \n\nSee OASIS Legal XML Online Dispute Resolution TC, OASIS, https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=legalxml-odr (last visited Feb. 24, 2017) [https://perma.cc/B3Rs-7F2J] (providing model data standards and technical committee for ODR). \n\nSee, e.g., The Honorable Jonathan Lippman, The Judiciary as the Leader of the Access-to-Justice Revolution, 89 N.Y.U. L. REV. 1569, 1569-70 (2014) (advocating for judicial leadership in access to justice movement); Gerry Singsen, Observing Change, MGMT. INFO. EXCH. J. 3, 6-7 (Fall 2014), http://www.msbf.org/futuresandatj/Observing-Change-article.pdf [https://perma.cc/42PC-GQFV] (recognizing judicial branch’s role in access to justice realm); Richard Zorza, Access to Justice:  The Emerging Consensus and Some Questions and Implications, 94 JUDICATURE 156, 156-57 (2011) (suggesting court has strong role in developing emerging consensus, including court simplification and triage).\n\nSee JUSTICE FOR ALL STRATEGIC PLANNING GUIDANCE MATERIALS (Aug. 3, 2016), http://www.ncsc.org/~/media/Microsites/Files/access/Justice%20for%20All%20Guidance%20Materials%20Final.ashx [https://perma.cc/W4NG-NQPJ] (explaining importance of gathering all different stakeholders in court system); Justice For All Project, NAT’L CTR. ST. CTS., http://ncsc.org/jfap (last visited Feb. 24, 2017) [https://perma.cc/2SSY-RAF4] (giving brief description of project). \n\nSee Justice For All Grants Announcement, NAT’L CTR. ST. CTS., (Nov. 2016), http://www.ncsc.org/~/media/Microsites/Files/access/JFA%20Awards%20Announcement%20Final.ashx [https://perma.cc/CT7A-U4XN] (announcing grant recipients). \n\nSee SANDEFUR & SMYTH, supra note 90, at v (highlighting findings of legal service survey); but see Elizabeth L. MacDowell, Reimagining Access to Justice in the Poor People’s Courts, 22 GEO. J. ON POVERTY L. & POL’Y 473, 473 (2015) (noting access to justice initiatives prioritize access over justice).\n\nJUSTICE FOR ALL GRANTS ANNOUNCEMENT, supra note 116.\n\nMichael C. Dorf, Legal Indeterminacy and Institutional Design, 78 N.Y.U. L. REV. 875, 882 & n.6 (2003), citing HENRY M. HART, JR. & ALBERT M. SACKS, THE LEGAL PROCESS: BASIC PROBLEMS IN THE MAKING AND APPLICATION OF LAW (William N. Eskridge, Jr. & Philip P. Frickey eds., 1994) (illustrating importance of being informed regarding decision-making process).\n\nSteinberg, supra note 53, at 786 (describing approaches for understanding court operations).\n\nSee REDESIGNING HOUSING COURT, NE. UNIV. L. LAB, IDEO, & MASS. HOUS. CT., http://nulawlab.org/view/redesigning-housing-court [https://perma.cc/2Q4Q-UMD3] (giving example of court redesign experiment).\n\nSee Erika J. Rickard, The Agile Court:  Evidence-Based Approaches to Improve Access to Justice and the Court User Experience, WESTERN NEW ENGLAND L. REV. (2017) (forthcoming). Research and evidence-based approaches are in their infancy in the civil legal system. See, e.g., Laura K. Abel, Evidence-Based Access To Justice, 13 U. PA. J.L. & SOC. CHANGE 295, 297 (2010) (highlighting need for evidence-based approach in civil legal aid programs); D. James Greiner & Andrea Matthews, Randomized Control Trials in the United States Legal Profession, 12 ANNUAL REVIEW OF LAW AND SOCIAL SCIENCE 295, 295-312 (2016) (outlining the dearth of randomized control trials in the law); Deborah L. Rhode, Access to Justice:  An Agenda for Legal Education and Research, 62 J. LEGAL EDUC. 531, 533 (2013) (noting lack of research).\n",
    "model": "gpt-4o-mini",
    "temperature": 0.7,
    "max_tokens": 250,
    "output": 0,
    "json_mode": 0,
    "output_to": 4,
    "behavior": "Summarize & question paper",
    "hide_button": false
  },
  "Beyond Readability with RateMyPDF (Steenhuis, Willey, & Colarusso, 2023)": {
    "prompt": "Beyond Readability with RateMyPDF: A Combined Rule-based and Machine Learning Approach to Improving Court Forms  \n\nQuinten Steenhuis Legal Innovation and Technology Lab Suffolk University Law School Boston, Massachusetts USA qsteenhuis@suffolk.edu  \n\nBryce Willey Legal Innovation and Technology Lab Suffolk University Law School Boston, Massachusetts USA bwilley@suffolk.edu  \n\nDavid Colarusso Legal Innovation and TechnologyLab Suffolk University Law School Boston, Massachusetts USA dcolarusso@suffolk.edu  \n\nABSTRACT  \n\nIn this paper, we describe RateMyPDF, a web application that helps authors measure and improve the usability of court forms. It offers a score together with automated suggestions to improve the form drawn from both traditional machine learning approaches and the general purpose GPT-3 large language model. We worked with form authors and usability experts to determine the set of features we measure and validated them by gathering a dataset of approximately 24,000 PDF forms from 46 U.S. States and the District of Columbia. Our tool and automated measures allow a form author or court tasked with improving a large library of forms to work at scale.  \n\nThis paper describes the features that we find improve form usability, the results from our analysis of the large form dataset, details of the tool, and the implications of our tool on access to justice for self-represented litigants. We found that the RateMyPDF score significantly correlates to the score of expert reviewers.  \n\nWhile the current version of the tool allows automated analysis of Microsoft Word and PDF court forms, the findings of our research apply equally to the growing number of automated wizard-driven interactive legal applications that replace paper forms with interactive websites.  \n\nCCS CONCEPTS  \n\n• Applied computing → Document analysis; Law; • Human- centered computing → Accessibility; Accessibility technologies; Accessibility design and evaluation methods • Information systems → Content analysis and feature selection  \n\nKEYWORDS  \n\nAccessibility, Law, Administrative Burden, Readability, Court Forms, Automated Analysis  \n\nACM Reference format:  \n\nQuinten Steenhuis, Bryce Willey, and David Colarusso. 2023. Beyond Readability with RateMyPDF: A Combined Rule-based and Machine Learning Approach to Improving Court Forms. In Proceedings of International Conference on Artificial Intelligence and Law (ICAIL’23).  \n\n1 Introduction  \n\nThe legal form is the primary way that self-represented litigants interact with courts across the United States. Self-represented litigants make up the vast majority of users of the civil court system. In 2015, only 24% of cases in U.S. civil courts had representation for both parties [12].Within a single jurisdiction, court systems may provide litigants with up to 1,500 standardized forms that address different legal rights. In 2015, only 24% of cases in U.S. civil courts had representation for both parties [12], Within a single jurisdiction, court systems may provide litigants with up to 1,500 standardized forms that address different legal rights. Legal forms require untrained litigants to read, understand, gather information, and apply legal reasoning. Difficult forms place a time and emotional burden on litigants, can make it hard for judges to understand what litigants want, and can lead to unfair outcomes in court.  \n\nIn some jurisdictions, a small number of forms have been converted into what are called interactive legal applications [15] or guided interviews. These expert system-like question and answer tools can greatly improve the ease of use of forms, but they take a large amount of time and effort to create. Our lab’s ongoing work has been to build tools that simplify and increase the speed of automation of forms [24], but the vast majority across the United States remain available only as Microsoft Word or PDF documents.  \n\nIn our experience working with courts across the country, we have observed that court forms are often created by untrained internal staff, without specialized tools. Forms are often designed in a word processor, such as Microsoft Word, or in better funded courts, desktop publishing tools, like Adobe InDesign. A small number of state courts have a “forms committee” tasked with gathering the input of various stakeholders in the form’s design. Stakeholders may include attorneys and court clerks who practice in the area the form covers. On occasion, form authors use templates and style guides to ensure that the form requests information consistently. Almost never do the form committees we have observed include:  \n\n• Self-represented users of the court system  \n\n• Designers  \n\n• Plain language and readability experts  \n\nThe most reliable way to improve a form is to conduct an observational study of real self-represented litigants completing the form and then to identify areas where litigants experience difficulty (i.e., a traditional usability test). However, usability tests alone cannot provide guidance for the creation of the first draft of a new form. In addition, usability testing can be time consuming and even modest compensation for usability test subjects may be outside of a court’s budget. Expert guidelines can address an important need in the creation of easy to use court forms. The guidelines discussed in this paper will help court staff and legal providers revise forms so that they are simpler, easier to understand, and easier to fill out accurately and completely. The RateMyPDF tool extends the value of the expert guidelines by helping courts quickly identify areas for improvement in either a single form or a large group of forms in an automated way. RateMyPDF allows form authors to “work at scale.” While RateMyPDF measures features of printable court forms, many of the rules and guidance that apply to printable forms are also applicable to interactive legal applications.  \n\n2 Evaluating form difficulty  \n\nWhat makes a form easy or hard to fill in? We propose that the difficulty of a court form depends on a typical self-represented litigant’s ability to:  \n\n• Comprehend the form prompts and instructions  \n\n• Accurately provide the requested information  \n\n• Consistently provide a complete response to the form  \n\nAs well as:  \n\n• The time burden imposed on the form’s user  \n\n• The psychological burden or harm imposed by requiring the user to recount traumatic events  \n\nTo build the guidelines below, we looked at existing written material, including the U.K.’s guidance on writing good questions [35], relied on our own experience as constructors and usability test conductors of dozens of interactive legal applications over the past 5 years, and interviews with authors of interactive legal applications and designers of court forms from 8 legal aid programs and courts from 8 different states in the United States. In this section, we discuss the guidelines and briefly identify strategies for measuring them. In the next two sections we discuss how we benchmarked each measure by evaluating forms from 46 States and the District of Columbia and then implemented each guideline in the RateMyPDF web application.  \n\n2.1 Helping users comprehend form prompts and instructions  \n\nWithin important limits, a form’s prompts, labels and instructions are easier to comprehend when they are written at a lower reading grade level. Reading grade level is a common metric produced by readability instruments that measure the ease of comprehension of narrative texts. In the United States, the median reader can comprehend texts written for a grade level between 8 and 9 [7,22], which means a significant percent of the population requires a lower reading grade level to easily comprehend the text. We therefore join a long tradition by recommending that form authors target writing for forms at a 6th grade reading level [16]. The concept of measuring readability and assigning it a score became popular in 1948, which is the year that the two most used measures, Flesch-Kincaid [9] and Dale-Chall [5], were first published. Both measures are friendly to computation by hand and use simple metrics. Flesch-Kincaid [9], for example, assigns texts a “grade level” score based on the length of sentences and the number of syllables in each word. The Dale-Chall formula adds a table of the most common 3,000 words in the English language [5]. Texts that include words that do not appear on the table are scored as more difficult to read. The two formulas often reach equivalent results on similar texts.  \n\nEven when measuring their target of narrative text, readability instruments have limits [1,20]. The text that people must read in forms is quite different from the narrative text that readability instruments were first created to measure. Forms are primarily composed of a mix of instructions, labels, and prompts, often without punctuation for headings and labels. Instructions are often minimal. Labels are often a single word: “Name,” “Address,” and so on. How to accurately turn these fragments into “sentences” that readability instruments can analyze is not clear. Forms may use common words like “Answer” and “Complaint” in legally- specific ways. These features combine to defeat the reliability of readability measures, which use sentence length and vocabulary as a proxy for complexity in forms.  \n\nBecause of these limitations, readability measures are not sufficient as a final measure of the ease of comprehending a form’s instructions. We suggest using readability measures as a starting point and separately measuring the difficulty of a form’s instructions with the use of a vocabulary list such as the Dale- Chall difficult word list [5].  \n\n2.2 Guiding users in providing accurate and complete responses  \n\nForms require the reader to write responses, not simply to understand and recall information. Both the substance of the expected answer and the input type can affect both accuracy and completeness. Court forms are high stakes. Court forms, as compared to other forms that members of the public may use to interact with corporations or government, are more likely to involve emotionally difficult material and usually involve two opposing parties in conflict, features that can reduce a litigant’s ability to process information [29]. Common court forms help tenants respond to eviction actions, domestic violence survivors get restraining orders against their abusers, and parties in divorce actions resolve disputes over the custody of their children. Lack of money, and the difficulties of navigating the bureaucratic hurdles that come with a low income life in the United States, may further burden litigant processing speed and accuracy in completing forms [8].  \n\n2.2.1 How substance affects litigant accuracy in completing forms  \n\nNot all responses on a form are alike. In Forms That Work, Jarrett and Gaffney [14] propose a framework for classifying form responses as follows:  \n\n• Slot-in responses, which can be provided without thinking, such as name and address.  \n\n• Gathered responses, which require the reader to spend some time locating and then entering the information that is still readily available, such as a driver’s license number.  \n\n• Third-party responses, which require the reader to provide information that is in another person’s control. For example, the income of a household member.  \n\n• Created answers, which require the reader to create a new response, draft a narrative, or choose among options that they had not previously considered.  \n\nSlot-in answers are the simplest to provide in complete and accurate detail. Users may face difficulty transcribing “gathered” answers accurately (e.g., they might transpose two digits when typing an ID number), and both gathered and third-party responses may be impossible to obtain. Allen Russell “Rusty” Boehm’s “Ohio Method” approach to enforcing the Ohio Forms Burden Reduction Act [2] describes a similar framework to that of Jarrett and Gaffney, although he uses the terms “Standard information,” “Semi-standard information,” and “Limited Access Information,” and a catch-all “Other.”  \n\nLitigant errors in both accuracy and completeness are most likely when providing created answers. Creating an answer requires the user to:  \n\n• Read the instructions  \n\n• Recall the information or facts that will be needed to create the answer  \n\n• Accurately apply the instructions (which may be a legal rule) to their facts.  \n\nConsider this “created” response:  \n\n[ ] Do you want a jury trial?  \n\nIt requires the litigant to check or not check a single box. The format of the answer could not be simpler. But the litigant may have never considered that a trial was a possibility, let alone have an opinion ready to provide. We can imagine the litigant’s thought process goes something like this:  \n\n“What is a jury trial? Oh, I guess it’s like on Law and Order. But what does that mean? I don’t really want a trial, I just want someone to give me a restraining order. If I say ‘no’ does that mean I get the decision quicker?”  \n\nThe litigant may be stuck thinking about this apparently simple question for quite a long time. (In fact, on an answer to eviction guided interview created by one of this paper’s authors, Google Analytics showed that this question stumped many tenants facing eviction).  \n\nThis second example is drawn from a Massachusetts restraining order petition:  \n\n“AFFIDAVIT: Describe in detail the most recent incidents of abuse. The Judge requires as much information as possible, such as what happened, each person’s actions, the dates, locations, any injuries, and any medical or other services sought. Also describe any history of abuse, with as much of the above detail as possible.” [36]  \n\nThe user is provided a full blank page to provide the requested response.  \n\nAs we will discuss later, this response requires the litigant to recount traumatic details of an event that may affect memory, processing speed, and accuracy. The format of the question also requires the litigant to make many choices about the level of detail, format, and structure of their response. It would be easy for a litigant to leave out important information. In comparison, the domestic violence restraining order petition in Washington State [28] separates the narrative into 7 individual questions. These include sections asking the litigant to describe recent incidents of abuse, past incidents of abuse, medical treatment, suicidal behavior, substance abuse (with detailed checkboxes), and the effect of the abuse on minor children. The Washington petition also provides the litigant a checklist of supporting evidence. Within the emotionally burdened context of high-stakes litigation, form authors must carefully choose the proper input type for each question to maximize ease of use, and should consider replacing some long, open-ended questions that require a lot of effort to respond to with fact-oriented questions that a litigant can respond to more automatically. The litigant should be spared from the task of synthesizing, organizing, and structuring a long narrative response when the form author can easily break a long question into smaller sections.  \n\nIn addition, because even simple questions place some burden on the litigant, form authors should consider removing questions that are not required for the fact finder’s decision or that can be obtained by the court from an existing data source.  \n\n2.2.2 How format and input selection affect litigant accuracy in completing forms  \n\nForm completion ease is affected by both the choice of inputs that the form author made and the layout and organization of the fields on the page.  \n\nInput selection  \n\nCommon response styles on written forms include:  \n\n• Short answer text fields  \n\n• Inter-lineal text responses  \n\n• Long answer text fields  \n\n• Check boxes  \n\n• Radio buttons (exclusive checkboxes)  \n\n• “Circle one” fields  \n\nThese input styles range in difficulty. Checkboxes are easy to mark, although too many choices or an incomplete list can make them challenging to answer correctly. Short free-text responses make more sense than checkboxes when the user’s choices can cover a very wide range of correct answers. Longer narrative answers can be time consuming for the user to interact with, but they are appropriate when the fact finder needs unstructured responses, such as “why” and “what happened.” Longer narratives also give the litigant a chance to tell their story. “Circle one” inputs are uncommon and can therefore be confusing on a paper form and should be replaced with checkboxes. Similarly, interlineal text responses, such as “The Defendant is the child’s ________ (mother or father)” are uncommon and should be avoided. On a printed form, radio button can be inherently confusing, unless limited to choices that exclusively describe a party, such as “Plaintiff” and “Defendant” or “Male,” “Female” and “Nonbinary.”  \n\nFormat, order, and layout of fields on the page Input choices are not the only choice that a form designer can make that affects the ability of a litigant to complete the form: layout, density of fields on the page, use of whitespace, and logical grouping are also important. In addition, the use of appropriate capitalization [18] can influence reading speed and comprehension. We have little to say about font size, as the optimal font size for readability has been described with as wide a range as 9 to 18 points [11,18,21]. Some of these format choices are easy to measure, and some are difficult.  \n\nA consistent brand identity, with shared headings and a familiar layout across forms will reduce the litigant’s effort to locate and provide an appropriate response to each question. Field density, as a proxy for whitespace [20], is also important. Form authors sometimes try to fit a form onto a small number of pages, at the expense of readability. Placing too many fields on a page can reduce the litigant’s ability to fit their response on the form as well as their ability to locate the most important information on the form. Correctly used, whitespace can also provide semantic grouping of information [20].  \n\nFinally, grouping questions in a logical order can affect form completion time. Jarrett and Gaffney [14] discuss the negative effect on completion rates of having questions in a surprising order. This is particularly true of questions that require the litigant to gather information from the same source. We suggest form authors read their forms carefully to make sure that like fields are grouped together.  \n\n2.3 Measuring burden on form users  \n\nThe Paperwork Reduction Act of 1995 [31] in the United States tasked the United States Government with minimizing form completion burden on the person filling out the form. Because easy to read forms can still be overly long, burden is best considered independently from other complexity metrics. Burden is usually measured as a function of the respondent’s cost in time and money [27,37]. Time burden is the most relevant for court forms, and may include the time it takes the user to:  \n\n• Read the form  \n\n• Gather information  \n\n• Respond to the form  \n\nIn addition, while many forms that users interact with request routine, unemotional facts, court forms often are centered on a traumatic or high-conflict experience. Therefore, when focusing on court forms, it is also important to consider the psychological burden imposed on the user.  \n\n2.3.1 Measuring time burden  \n\nThe time it takes to complete a form depends on the time it takes to:  \n\n1. Read any instructions and field prompts.  \n\n2. Write down the response once it has been retrieved or created.  \n\n3. Retrieve or create a response to each field.  \n\nTrauzettel-Klosinski et. al. [26] measured reading speed across 17 languages using standardized text, and found that across populations the average reading speed was 184 words per minute. Reading speed is much slower than the average for significant sub-populations, such as those with dyslexia [17]. Our formula assumes a reading speed of 150 words per minute to account for population variation and to reach something more than a bare majority of readers. Average handwriting speed has been measured at 40 characters per minute [30].  \n\nWe assigned a “time to answer” to each classification of field in the Jarrett and Gaffney framework of slot-in, gathered, third- party and created fields [14]. Both the time to answer and the distribution are a simplified estimate based on our collective experience working with low-income and self-represented litigant populations. We assume a normal distribution of answer times. We selected the times in table 1 to approximate answer time for each answer type.  \n\nThe time to create an answer is then added to the length of the field in characters divided by the average handwriting speed of 40 characters per minute, after categorizing the field length into one of the following buckets:  \n\n1. One-line answers (assuming a typical line length of 80 characters), which we assume require about 1 word of writing  \n\n2. Short-answer questions, which we round up to 2 lines  \n\n3. Medium answer questions, which we round to 5 lines  \n\n4. Long answer questions, which exceed 5 lines of text and we round to 10 lines  \n\nTable 1: Time to produce for each answer type  \n\nAnswer type Mean time to  \n\nproduce  \n\nStandard  \n\ndeviation  \n\nSlot-in .25 minutes .1 minutes  \n\nGathered 3 2  \n\nThird-party 5 2  \n\nCreated 5 4  \n\nWe normalize answer length into these buckets rather than directly using character count because we assume that the blank space on the page is likely to be constrained by court considerations, such as limiting the number of pages that the form will use, rather than directly providing information about the actual average answer length.  \n\nTo account for the variability in time to answer, we assume a normal distribution, run a Monte Carlo simulation for the time to answer each individual field and then sum the simulated values. The RateMyPDF time to answer metric is shown separately from the form’s overall complexity score.  \n\n2.3.2 Measuring emotional burden  \n\nLegal forms may require litigants to disclose details of traumatic personal events. Most litigation involves conflict between two parties, and this conflict can be very personal and involve elements such as domestic violence, abusive landlord-tenant relationships, and more. Trauma has a direct link to processing speed and cognition [29]. Form authors should carefully consider the benefits and drawbacks of asking a litigant to recount such detailed information, and ensure that when it is requested it has a direct corresponding benefit to the litigant in helping obtain the relief that they requested.  \n\nBecause the emotional burden of completing the form cannot always be eliminated or reduced, we do not use emotional burden as an element of our complexity score. Disclosing the traumatic event is often, but not always, central to getting relief based on that event. For example, while it is appropriate for the court to ask for an affidavit recounting an episode of abuse to grant a restraining order, it is not appropriate to ask the litigant to repeat that information in a purely administrative form that the litigant can use to enforce an order that has already been granted. These distinctions are difficult to make in an automated way. We use the Spot [25] NLP classifier to create a first guess about the form’s classification using the LIST (Legal Issues Taxonomy) taxonomy of legal problems [38]. This classification can be a useful signal as to the form’s emotional burden.  \n\n3 The state form dataset  \n\nWe started by surveying 50 States and the District of Columbia to identify a location on an official court website that listed standardized court forms (an initial version of this list of state court form pages was graciously shared with us by the Stanford Legal Design Lab). Some states do not have an official website that lists forms or only have forms in Microsoft Word format. Other states do not have any state-wide standardized forms or have a small number. At the low end, Louisiana had about 5 state- wide forms while at the high end, California had 1,500. States that lack state-wide forms may have forms that vary by judicial district or county. With the exception of a small number of county-level forms in Florida, we did not include these “local” form variations in our survey.  \n\nUltimately, we gathered PDF forms from 46 states and the District of Columbia. We have created a website that allows form authors to explore all the forms that we have collected and processed in one place, called the Form Explorer [39].  \n\n3.1 Method of gathering forms  \n\nForms were gathered by scraping, primarily with Python’s requests[40] library and custom logic. While we tried multiple approaches to scraping court websites that required varying levels of effort, we finally landed on an approach that simply crawls the main form index page and a set depth of cross-linked pages on the same domain for PDF files and downloads them all. Within FormFyxer we later apply GPT-3 to the full text of the form to create a title and a description and use the Spot NLP classifier [25] to assign it a category, information that might otherwise be obtained from more hand-tuned manual scraping. We also ask GPT-3 to write the description and summary of the form’s text at a 6th grade reading level. This flexible approach to scraping court forms will allow us to keep our dataset up to date with minimal effort. The automated plain language summaries may also prove useful to the work of court staff who maintain libraries of hundreds of forms.  \n\n3.2 Results from our form benchmarking  \n\nWe ran a large subset (about 15,000) of the 24,000 forms we gathered through our FormFyxer tool. We excluded some forms that could not be automatically processed, appeared to be in a language other than English, or were obtained too late in our process. This subset represents forms from 24 states. Ultimately, we obtained benchmark scores in table 2 from that dataset. We notice that there is a lot of variability in the dataset, for almost every measure. Within a single standard deviation, for example, we range from 20 fields to less than 1 field per page. This is likely due to jurisdiction variations as to whether there are cover pages with instructions. This is an area that deserves further close attention.  \n\nThe average reading grade level of almost 10th grade likely understates the difficulty of reading the form labels and prompts, due to the lower accuracy of readability scores as applied to forms, but is well above the target reading grade level of 6th grade. There is a high observed percentage of difficult words and a high percentage of sentences written in the passive voice. Almost 13% of words in the average court form were outside of the Dale-Chall word list of the most common 3,000 English language words, while 24% of sentences were written in the passive voice. Legal citations were observed less often in the dataset, but we note that the EyeCite [4] library is not yet capable of reliably detecting state-specific short form citations.  \n\nTable 2: Form Benchmark Scores  \n\nMeasure Mean Standard  \n\nDeviation  \n\nComplexity score 23.46 10.71  \n\nTime to answer 37 minutes 87 minutes  \n\nReading grade level [34] 9.7 3.07  \n\nPage count 2.76 4.54  \n\nField count 48.35 80.23  \n\nFields per page 20.03 20.15  \n\nNormalized character  \n\ncount per field  \n\n7.83 2.10  \n\nSentences per page 12.35 7.81  \n\nDifficult word percent 12.86% 4.9%  \n\nPassive voice sentence  \n\npercent  \n\n29.88% 22.96%  \n\nCitation count 1.22 4.37  \n\nPercent of words in all  \n\ncapital letters  \n\n8.29% 7.70%  \n\nSlot-in field percent 63.81% 27.14%  \n\nGathered field percent 31.36% 24.09%  \n\nThird party field percent 0.00% 0.00%  \n\nCreated field percent 0.002% 0.15%  \n\nForms in our dataset ask a lot of litigants, with the mean page containing 20 separate fields, and the mean form asking the litigant to provide 48 separate pieces of information. We detected fewer third-party and created fields than expected, indicating we are undercounting those, but the difference between third-party, created, and gathered fields is relatively small in our complexity score and time to answer score. Our current formula estimates that the mean form requires a litigant to spend 37 minutes and the mean plus one standard deviation is 2 hours.  \n\n3.3 RateMyPDF score correlation to expert ratings  \n\nAfter building the initial version of the RateMyPDF score, we selected a random subset of forms and assigned them to a panel of 6 nationally recognized expert reviewers who variously work in the field of plain language, participate in form committees, and regularly work with self-represented litigants. We asked experts to rate a form’s complexity on a scale from 1 to 5. Reviewers rated between 20 and 35 forms each, and each of 40 forms was reviewed by at least 3 raters.  \n\nWe found statistically significant intraclass correlations among experts and between the average human rating and that of RateMyPDF. We normalized all scores for each reviewer and for RateMyPDF before further processing. The expert reviewers showed agreement with each other about which forms were complex (ICC1 0.3139, p-value=0.02), and the RateMyPDF score correlated with the average expert rating (ICC3 0.5861, p- value=0.00).  \n\nAdditionally, when treated as a seventh reviewer the RateMyPDF score improved the groups agreement (ICC1 0.3931, p-value 0.00). These results were significant (p<0.05), suggesting that both human and RateMyPDF ratings perform better than a random number generator at assigning a complexity score to a form. We can reject the null hypothesis that expert human and machine ratings act as random number generators. Human raters assigned scores with more agreement than would be expected by chance, and RateMyPDF scores look more like these human raters than expected by chance. Details of our analysis can be found in the FormFyxer GitHub repository [33].  \n\n4 RateMyPDF  \n\nRateMyPDF, available at https://ratemypdf.com, is a Python + FastAPI website that allows a user to upload a single PDF with form fields and obtain a variety of statistics, including a “complexity score” that compares the PDF to our benchmark dataset.  \n\nThe code is split into two repositories that are available on GitHub: FormFyxer [33], and RateMyPDF [32], which is the FastAPI frontend to the Python modules contained in FormFyxer. The FormFyxer library incorporates work from several existing open source projects, including EyeCite [4], PassivePy [23], scikit-learn [19] and spaCy [13], and leverages the commercial GPT-3 large language model for some additional machine learning tasks (specifically, text summarization), as well as the Suffolk Legal Innovation and Technology Lab’s NLP issue spotter, Spot [25].  \n\nWhile RateMyPDF currently operates on a single form at a time, the underlying FormFyxer library was created for bulk processing of PDF forms. We used it to obtain our benchmark scores from the full state dataset, but it can also be used to rank and compare forms within a single jurisdiction. We display aggregate scores for jurisdictions in our companion website, the Form Explorer [39].  \n\nRateMyPDF has a simple interface. The first screen prompts the user to upload a Microsoft Word or PDF file; once the file is uploaded, the website displays both summary and detailed statistics that compare the form to our benchmark form set. In addition, a number of suggestions are displayed to help authors improve the form, including suggested word substitutions from both U.S. and U.K. official plain language sites.  \n\nFigure 1: screen capture of RateMyPDF.com  \n\nAs of the writing of this paper, RateMyPDF measures 14 features of a form. Those features are then weighted and aggregated into a single score. The score can then be compared to our benchmark set of 24,000 forms.  \n\nThe features we measure are:  \n\n• Reading grade level (a consensus score)  \n\n• Percent of difficult words (currently drawn from the Dale- Chall word list)  \n\n• Use of calculations  \n\n• Number of pages  \n\n• Number of legal citations per field  \n\n• Average number of fields per page  \n\n• Normalized answer length per field  \n\n• Sentences per page  \n\n• Percent of passive voice sentences  \n\n• Percent of words written in all capital letters  \n\n• Percent of “slot-in” fields  \n\n• Percent of “gathered” fields  \n\n• Percent of “third-party” fields  \n\n• Percent of “created” fields  \n\nThese features represent what we have identified as the most important non-correlated features of a form.  \n\n4.1 NLP-based field normalization and classification  \n\nOur complexity score relies on our field name normalization and our automated classification of normalized fields into “slot-in,” “gathered,” “third-party” and “created”. We trained a proof of concept ML model using traditional classification techniques to assign normalized field names based on several features including: (1) the name of a field based on adjacent text; (2) the normalized name of the previous field; (3) the relative location of the field on the form; and (3) the topic of the field as identified by Spot[25]. This model is combined with simple heuristics to assign a meaningful label to each field in PDF forms. We normalize field names so that they are snake case (lower case words separated by a “_” character), under 30 characters in length, and where possible so that they match the Suffolk LIT Lab’s Document Assembly Line standard for PDF field labels [41]. Early experiments with GPT-3 suggest adding an LLM to our heuristics may improve our automated labeling.  \n\nFields that match the Assembly Line standard have a known semantic content and can be classified with heuristics. For example, keywords, such as “address,” are used as a signal combined with the NLP model to determine that a field prompting for the litigant’s address is a “slot-in” field that requires little time to prepare a response.  \n\nIn addition to helping with the classification of the fields by type, these standardized labels allow interactive legal application authors to save significant time and help standardize the interactive legal applications built around these forms, facilitating processes like that our lab applied to automate dozens of forms during the Covid-19 pandemic in Massachusetts [24].  \n\nWe built a pipeline around open source tools to facilitate this field normalization. As we gathered our state form dataset, we noticed that there was a lot of variability in the contents of the PDFs. Some had existing form fields and labels added. Some of those labels appeared to be added using Adobe Acrobat’s “recognize form fields” function. Some were added by hand. And some were complex, nested structures that were not recognized using the most common open source PDF libraries. Some states had many forms in the Adobe LiveCycle (XFA) format. The XFA format is proprietary to Adobe and was not compatible with our field normalization tool. We built a small pipeline to convert XFA forms to standard PDFs. No existing open source tools offered this feature.  \n\nTo address the remaining lack of standardized forms, we built several PDF manipulation functions into the FormFyxer [33] library. Those new functions include an auto field recognition function that works similarly to the Adobe Acrobat function. We use the well-known computer vision tool OpenCV [3] to identify boxes and lines on the PDF, as well as searching the text of the PDF for checkboxes created by using an open and closing square bracket: “[ ]”. Either a checkbox or text field is added at the location of the identified “blank” space on the form. A draft field name is then created by gathering text that surrounds the field and using the field normalization model to create an automatic summarization of the full text.  \n\n4.2 Use of GPT-3 large language model  \n\nWe use GPT-3 in three ways: (1) to help identify the nature of data and discard improperly formatted or irrelevant data, (2) to extract metadata from existing text, and (3) to summarize existing text.  \n\nGPT-3 performs well at identifying poorly formed source data, allowing us to save time that would otherwise be required to clean and correct large sets of PDFs with conditional prompts. For example, to obtain a plain language name for each form, we provide GPT-3 with a prompt that contains the full text of a PDF followed by “If the above is a court form, write the form’s name, otherwise respond with the word ‘PoorlyFormedForm.’” This conditional prompt also limits the number of GPT-3 API calls that are required.  \n\nThe third use we make of GPT-3 in our project is to transform the source data with a prompt to summarize and rewrite it at a 6th grade reading level. This output is presented to the end-user as a set of suggestions, allowing them to double-check the tool’s work. This use, anchored to the source data, reduce the risk of LLM’s known tendency to “hallucinate,” or provide factually incorrect responses.  \n\n4.3 How recommendations are presented  \n\nWhen working on RateMyPDF, we relied on frequent workshopping of the tool with potential users. We shared and presented it with a group of 8 legal aid providers who meet with our team weekly to build interactive legal applications. We have workshopped early versions of the tool with Michigan court staff who are implementing a wide-scale form simplification project. We also discussed the project with staff at Pew Charitable Trusts and early versions of the guidelines were shared for review with document automation experts who participate in Law Help Interactive’s monthly trainings and panels. A consistent request from our reviewers was to add easily implementable suggestions along with the statistical information.  \n\nRateMyPDF currently makes the following suggestions:  \n\n• Each component of the score is listed separately, together with a mean and a standard deviation from our larger dataset of forms.  \n\n• We use GPT-3 to provide a plain language draft of the form’s name and a summary of the form based on the form’s text.  \n\n• We identify citations with EyeCite [4] and suggest removing them from the document.  \n\n• We identify sentences that contain passive voice with the PassivePy library [23] and highlight the “passive” portion of the text. PassivePy also leverages the spaCy NLP tool to classify sentences as passive or not.  \n\n• We list words that do not appear on the Dale-Chall wordlist  \n\n• We highlight suggested replacements for complex terms that appear on the U.S. government’s plainlanguage.gov [42] site  \n\n• We suggest replacement of gendered terms with gender neutral alternatives  \n\nOne limitation that we observed with the use of the EyeCite [4] citation extractor is that it performed best on citations to federal case law and reported decisions. It did not identify state short-form citations common on legal forms, which leads to citations being undercounted.  \n\nDavison and Kantor [6] observe that a formula that measures readability has limitations for improving the readability of the texts it has measured. The features it measures can be accurate in naturally written text, but as soon as an author works to improve the score, they may reach for fixes that fool the instrument without improving the text’s readability. Including specific recommendations for improvements that will improve the form’s usability without “fooling” the algorithm can reduce this risk.  \n\n4.4 Using RateMyPDF to compare forms across jurisdictions  \n\nRateMyPDF allows for real-time evaluation of individual forms. We have built a companion website, called the Form Explorer [39], which uses our full 24,000 form dataset to allow court form authors to search and compare forms across jurisdictions. For example, if a court in Michigan is building a new fee waiver petition, they can use the Form Explorer website to locate semantically similar forms in other jurisdictions and compare them across several dimensions. The forms in our Form Explorer website are classified by issue type with Spot [25] and have field names that are normalized with the ML model within FormFyxer. One insight we hope form authors can obtain from this information is to identify which fields are common across jurisdictions and which ones are unique. We expect that this information can help form authors support an argument for process simplification in their jurisdictions. We hope to combine the form comparison feature in the Form Explorer with RateMyPDF so that a form author can compare an arbitrary PDF with forms in other states.  \n\n4.5 Comparing to prior work  \n\nWhen researching existing examples of automated improvement of administrative forms that looked beyond traditional readability measures, we discovered AMesure, a web platform that evaluates and offers suggestions to improve French-language administrative texts [10]. Like RateMyPDF, AMesure uses a statistical approach that leverages language models when scoring text rather than the mechanical approach in readability measures like Flesch-Kincaid and Dale-Chall. However, AMesure is aimed at texts, not forms, and it does not purport to measure burden, only readability.  \n\n5 Directions for future research  \n\n5.1 Assigning a target score  \n\nCurrently, RateMyPDF reports a complexity score for each form, but the score is value neutral. We provide the context of where the form is in comparison to the mean and standard deviation for the full population of state forms. From our experience working with self-represented litigants, we find it likely that the “ideal” form is less complex than the mean form. We have started this work by asking our panel of 6 experts to assign both a complexity score and a value judgment about how “good” the form is. Interestingly, answers to this question from our experts were much more varied than the responses to our question about how complex each form was, but we hope to eventually be able to assign something like a letter grade (A-F) to each form. This might allow us to meaningfully group and compare sets of forms for ease of use without false precision.  \n\n5.2 Refining estimates with real-world benchmarking  \n\nAfter creating our formula that provides a time to answer for a form, we learned of research by Dr. Cyprian Ejiasa [2] that analyzed real-world timing for completing 124 representative government forms in Ohio. Those times are in table 3.  \n\nTable 3: Time to complete from real-word timing  \n\nAnswer Type Low Medium High  \n\nStandard information 1.02 minutes 1.74 2.46  \n\nSemi-standard 0.82 1.11 1.40  \n\nLimited access 5.29 5.65 6.01  \n\nOther 2.83  \n\nOur estimated times in table 1 correspond roughly to the real- world figures discovered in Dr. Ejiasa’s research in 1980. It would be useful to revisit this research and obtain an updated benchmark with real users on high-stakes court forms. We note that some features of court forms are relatively unique, such as long narrative responses and affidavits.  \n\n5.3 Large Language Models as a tool for directly improving readability of text  \n\nWe have had promising early results when using GPT-3 to re- write sentences that are complex with simple prompts like “Write the following at a 6th grade reading level,” and we make use of similar prompts when asking GPT-3 to summarize text. As instructors who have taught plain language techniques to law students and recognize the difficulty that students face with translating complex legal topics into clear writing at a 6th grade level, we feel this may be an important future task for GPT-3 that deserves further investigation. We are considering a cost effective and responsible way to integrate a redraft of the form with GPT-3 rewritten sentences into the recommendations provided by RateMyPDF.  \n\n5.4 Metrics to consider for the RateMyPDF score  \n\nWe asked our panel of experts for their thoughts on the measures that we included in RateMyPDF. These give us good direction to consider re-weighting the current metrics. A future version of RateMyPDF might benefit from including:  \n\n• A direct measure of whitespace. We will consider using the OpenCV library for this task.  \n\n• A measure of field ordering. One approach we are investigating is to measure distance from a grouping created by GPT-3. Early experiments are promising.  \n\n• A different word list. The Dale-Chall list, created in 1948 with texts for children and updated in 1995 (and omitting words like divorce, tenant, and email), is not fully representative of difficult words in modern court forms read by adults. We plan to investigate the use of our dataset of the text from 24,000 court forms to create a more tailored list that can be tested with self-represented litigants.  \n\n5.5 Extension to guided interviews  \n\nPrior to beginning the work that led to RateMyPDF, our lab was focused on building guided interviews with the Docassemble web framework. We built a simple tool to analyze Docassemble interviews, but realized that a tool that analyzed printable forms would have broader use. A valuable future project would be to combine these tools.  \n\n6 Conclusion  \n\nGiven the vast quantity of standardized legal forms in the United States, form simplification can be a daunting task. Washington State spent almost a decade simplifying its official legal forms. Michigan is currently in the middle of a two-year engagement with consultants to improve the readability of its legal forms, and the project will likely end with a subset of the forms that will be models for the court’s form authors to continue to simplify on their own. When applied in batch to a court’s library of forms, tools like RateMyPDF can help enforce standards, give clear direction to revise forms, benchmark progress, and focus the court’s efforts on the forms that will provide the greatest payoff. RateMyPDF can scale the court’s efforts to help self-represented litigants with simpler forms.  \n\nYet form simplification can only go so far. Court form authors are constrained by the square corners of a piece of paper, and often try to avoid more than 2 or 3 pages for a single form. When designing a form for a legal problem with several options, such as restraining orders that depend on the relationships of the parties or a divorce proceeding that may or not involve children and accompanying custody decisions, form authors need to choose whether to include long pages of instructions and whether to combine or separate forms. Tradeoffs between comprehensive help, ease of locating the proper form, and overwhelming litigants can be complex. Interactive legal applications solve these problems. Branching logic, just-in-time instructions and context can improve the accuracy of form completion. Conditional text can make one form into many, simplifying the litigant’s task in selecting the correct form.  \n\nRateMyPDF and the FormFyxer library are only a small piece of an ecosystem of tools that our lab is building. We have used the FormFyxer library to normalize fields and extract information from PDFs that can then be used to build draft automations. The normalized fields are linked to standardized questions. This ability to speed up automation may end up being the most important way that RateMyPDF can improve access to justice, but better court forms are an important first step.  \n\nACKNOWLEDGMENTS  \n\nMichelle Bernstein, Caroline Robinson, and Lily Yang each gave generously of their time to help identify key features of forms that affect difficulty in completion. Our expert reviewers were Laurie Garber, Marc Lauritsen, Josh Lazar, Maria Mindlin, Matthew Newsted, and Christian Noble.  \n\nWe would also like to thank Caroline Jarrett and Gerry Gaffney whose text Forms That Work was an important inspiration for our framework to classify fields by answer type.  \n\nREFERENCES  \n\n[1] Rebekah George Benjamin. 2012. Reconstructing Readability: Recent Developments and Recommendations in the Analysis of Text Difficulty. Educ Psychol Rev 24, 1 (March 2012), 63–88. DOI:https://doi.org/10.1007/s10648- 011-9181-8  \n\n[2] Allen Russell Boehm. Ohio Forms Burden Reduction Act. Ohio (on file with author).  \n\n[3] G. Bradski. 2000. The OpenCV Library. Dr. Dobb’s Journal of Software Tools (2000).  \n\n[4] Jack Cushman, Matthew Dahl, and Michael Lissner. 2021. eyecite: A tool for parsing legal citations. JOSS 6, 66 (October 2021), 3617. DOI:https://doi.org/10.21105/joss.03617  \n\n[5] Edgar Dale and Jeanne S. Chall. 1948. A Formula for Predicting Readability: Instructions. Educational Research Bulletin 27, 2 (1948), 37–54.  \n\n[6] Alice Davison and Robert N. Kantor. 1982. On the Failure of Readability Formulas to Define Readable Texts: A Case Study from Adaptations. Reading Research Quarterly 17, 2 (1982), 187–209. DOI:https://doi.org/10.2307/747483  \n\n[7] William H. DuBay. 2007. Smart Language: Readers, Readability, and the Grading of Text. Retrieved February 3, 2023 from https://eric.ed.gov/?id=ED506403  \n\n[8] Anne Fernald, Virginia A. Marchman, and Adriana Weisleder. 2013. SES differences in language processing skill and vocabulary are evident at 18 months. Developmental Science 16, 2 (2013), 234–248. DOI:https://doi.org/10.1111/desc.12019  \n\n[9] Rudolph Flesch. 1948. A new readability yardstick. Journal of Applied Psychology 32, (1948), 221–233. DOI:https://doi.org/10.1037/h0057532  \n\n[10] Thomas François, Adeline Müller, Eva Rolin, and Magali Norré. 2020. AMesure: A Web Platform to Assist the Clear Writing of Administrative Texts. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations, Association for Computational Linguistics, Suzhou, China, 1–7. Retrieved November 9, 2022 from https://aclanthology.org/2020.aacl-demo.1  \n\n[11] Dr Jörg Fuchs, Tina Heyer, and Diana Langenhan. 2008. Influence of Font Sizes on the Readability and Comprehensibility of Package Inserts. Pharm. Ind. (2008).  \n\n[12] Paula Hannaford, Scott Graves, and Shelley Spacek Miller. 2015. The Landscape of Civil Litigation in State Courts. National Center for State Courts. Retrieved May 1, 2023 from https://www.ncsc.org/__data/assets/pdf_file/0020/13376/civiljusticereport- 2015.pdf  \n\n[13] Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. Retrieved February 2, 2023 from https://spacy.io/  \n\n[14] Caroline Jarrett, Gerry Gaffney, and Steve Krug. 2008. Forms that Work: Designing Web Forms for Usability (1st edition ed.). Morgan Kaufmann, Amsterdam ; Boston.  \n\n[15] Marc Lauritsen and Quinten Steenhuis. 2019. Substantive Legal Software Quality: A Gathering Storm? In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law, ACM, Montreal QC Canada, 52–62. DOI:https://doi.org/10.1145/3322640.3326706  \n\n[16] Irving Lorge and Raphael Blau. 1941. Reading Comprehension of Adults. Teachers College Record 43, 3 (December 1941), 1–6. DOI:https://doi.org/10.1177/016146814104300303  \n\n[17] Shelley Miller-Shaul. 2005. The characteristics of young and adult dyslexics readers on reading and reading related cognitive tasks as compared to normal readers. Dyslexia 11, 2 (2005), 132–151. DOI:https://doi.org/10.1002/dys.290  \n\n[18] A. Miniukovich, A. De angeli, S. Sulpizio, and P. Venuti. 2017. Design guidelines for web readability. In DIS 2017 - Proceedings of the 2017 ACM Conference on Designing Interactive Systems, Association for Computing Machinery, Inc, Edinburgh, 285–296. DOI:https://doi.org/10.1145/3064663.3064711  \n\n[19] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12, (2011), 2825–2830.  \n\n[20] Janice Redish. 2000. Readability formulas have even more limitations than Klare discusses. ACM J. Comput. Doc. 24, 3 (August 2000), 132–137. DOI:https://doi.org/10.1145/344599.344637  \n\n[21] Luz Rello, Martin Pielot, and Mari-Carmen Marcos. 2016. Make It Big! The Effect of Font Size and Line Spacing on Online Readability. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16), Association for Computing Machinery, New York, NY, USA, 3637–3648. DOI:https://doi.org/10.1145/2858036.2858204  \n\n[22] John Sabatini. 2015. Understanding the Basic Reading Skills of U.S. Adults: Reading Components in the PIAAC Literacy Survey. ETS Center for Research on Human Capital and Education. Retrieved February 3, 2023 from https://eric.ed.gov/?id=ED593006  \n\n[23] Amir Sepehri, David Matthew Markowitz, and Mitra Mir. 2022. PassivePy: A Tool to Automatically Identify Passive Voice in Big Text Dat. DOI:https://doi.org/10.31234/osf.io/bwp3t  \n\n[24] Quinten Steenhuis and David Colarusso. 2021. Digital Curb Cuts: Towards an Open Forms Ecosystem. Akron Law Review 54, 4 (2021), 2.  \n\n[25] Suffolk Law School’s Legal Innovation and Technology Lab. About Spot. Retrieved February 9, 2021 from https://spot.suffolklitlab.org/  \n\n[26] Susanne Trauzettel-Klosinski, Klaus Dietz, and the IReST Study Group. 2012. Standardized Assessment of Reading Performance: The New International Reading Speed Texts IReST. Investigative Ophthalmology & Visual Science 53, 9 (August 2012), 5452–5461. DOI:https://doi.org/10.1167/iovs.11-8284  \n\n[27] Linda Veiga, Tomasz Janowski, and Luís Soares Barbosa. 2016. Digital Government and Administrative Burden Reduction. In Proceedings of the 9th International Conference on Theory and Practice of Electronic Governance (ICEGOV ’15-16), Association for Computing Machinery, New York, NY, USA, 323–326. DOI:https://doi.org/10.1145/2910019.2910107  \n\n[28] Washington Law Help. 2022. How to File Petition for Order of Protection. Retrieved February 6, 2023 from https://www.washingtonlawhelp.org/files/C9D2EA3F-0350-D9AF-ACAE- BF37E9BC9FFA/attachments/9100D6C9-D107-4B15-87B3- A898F12B6FD8/3701en_how-to-file-petition-for-order-of-protection.pdf  \n\n[29] Antoinette Welsh. 2013. Effects of Trauma Induced Stress on Attention, Executive Functioning, Processing Speed, and Resilience in Urban Children. Seton Hall University Dissertations and Theses (ETDs) (December 2013). Retrieved from https://scholarship.shu.edu/dissertations/1907  \n\n[30] Jenny Ziviani and John Elkins. 1984. An Evaluation of Handwriting Performance. Educational Review 36, 3 (November 1984), 249–261. DOI:https://doi.org/10.1080/0013191840360304  \n\n[31] 2015. Paperwork Reduction Act (44 U.S.C. 3501 et seq.). Digital.gov. Retrieved February 2, 2023 from https://digital.gov/resources/paperwork- reduction-act-44-u-s-c-3501-et-seq/  \n\n[32] 2023. RateMyPDF. Retrieved February 3, 2023 from https://github.com/SuffolkLITLab/RateMyPDF  \n\n[33] 2023. FormFyxer. Retrieved February 3, 2023 from https://github.com/SuffolkLITLab/FormFyxer  \n\n[34] 2023. Textstat. Retrieved February 7, 2023 from https://github.com/textstat/textstat  \n\n[35] How to write good questions for forms - NHS digital service manual. nhs.uk. Retrieved February 6, 2023 from https://service-manual.nhs.uk  \n\n[36] Restraining order/abuse prevention order court forms | Mass.gov. Retrieved February 6, 2023 from https://www.mass.gov/lists/restraining-orderabuse- prevention-order-court-forms  \n\n[37] How to estimate burden | A Guide to the Paperwork Reduction Act. Retrieved November 9, 2022 from https://pra.digital.gov/burden/estimation/  \n\n[38] LIST:Legal Issues Taxonomy. LIST: Legal Issues Taxonomy. Retrieved February 7, 2023 from https://taxonomy.legal/  \n\n[39] About the Form Explorer? Retrieved February 7, 2023 from https://suffolklitlab.org/form-explorer/  \n\n[40] Requests: HTTP for HumansTM — Requests 2.28.2 documentation. Retrieved February 3, 2023 from https://requests.readthedocs.io/en/latest/  \n\n[41] Field labels to use in template files | The Document Assembly Line Project. Retrieved February 3, 2023 from https://suffolklitlab.org/docassemble- AssemblyLine-documentation/docs/label_variables  \n\n[42] plainlanguage.gov | Choose your words carefully. Retrieved April 29, 2023 from https://www.plainlanguage.gov/guidelines/words/\n",
    "model": "gpt-4o-mini",
    "temperature": 0.7,
    "max_tokens": 250,
    "output": 0,
    "json_mode": 0,
    "output_to": 4,
    "behavior": "Summarize & question paper",
    "hide_button": false
  },
  "Weaving Pathways for Justice with GPT (Steenhuis, Colarusso, & Willey, 2023)": {
    "prompt": "Weaving Pathways for Justice with GPT: LLM-driven automated drafting of interactive legal applications\n\nQuinten STEENHUIS, David COLARUSSO, and Bryce WILLEY Suffolk University Law School\n\nORCiD ID: Quinten Steenhuis https://orcid.org/0009-0001-0110-064X, David Colarusso https://orcid.org/0009-0003-6287-9284, Bryce Willey https://orcid.org/0000-0003-1775-2869\n\nAbstract. \n\nCan generative AI help us speed up the authoring of tools to help self- represented litigants?\n\nIn this paper, we describe 3 approaches to automating the completion of court forms: a generative AI approach that uses GPT-3 to iteratively prompt the user to answer questions, a constrained template-driven approach that uses GPT-4-turbo to generate a draft of questions that are subject to human review, and a hybrid method. We use the open source Docassemble platform in all 3 experiments, together with a tool created at Suffolk University Law School called the Assembly Line Weaver. We conclude that the hybrid model of constrained automated drafting with human review is best suited to the task of authoring guided interviews. Keywords. document automation, generative AI, large language models, forms, form automation, guided interviews, interactive legal applications\n\n1. Introduction\n\nCan generative AI help us speed up the authoring of tools to help self-represented liti- gants?\n\nThe traditional way to help self-represented litigants with court forms is to create a hand-authored interactive legal application, or guided interview. Lauritsen and Steenhuis [1] describe a long list of tools that can be used in this traditional approach, with the most popular tools being HotDocs, A2J Author, and Docassemble. A recent entrant in this space focused on legal applications is JusticeBot [2].\n\nWhen we say “forms,” we mean the full range of documents that litigators and transactional attorneys might work with. These can include: legal complaints, answers to those complaints, deeds, wills, and demand letters. These documents often look like they only need simple fill in the blanks, but require the application of judgment to be used correctly. For example: wills and trusts have clauses that apply only to some classes of testators. Complaints and answers assert the violation of a law, which may or may not be true depending on individual facts. These rules are not always visible within the four corners of the form.\n\nCreating interactive legal applications with traditional tools is slow and careful work. Authors add markup to templates in PDF or Word format, craft labels for each variable, create pages that place the labeled variables in context, write logical rules to show and hide follow-up questions and text in the final output, and add instructions and help. Suffolk Law School’s Legal Innovation and Technology Lab created a tool to help speed up this task in 2020, during the early months of the COVID-19 pandemic [3]. This tool, the Assembly Line Weaver, scans templates for variables and uses a mix of pre- written questions and heuristics to generate a draft guided interview for the Docassemble [4] platform. The interview itself uses customizable templates for the YAML format that Docassemble uses, and can later be edited by hand. The tool was designed to allow volunteers to help scale the work of expert form automators, and during the pandemic more than 200 volunteers from around the world helped the lab automate about 30 key processes with it from start to finish [3].\n\nOnce the hundreds of volunteers left, the lab’s work shifted. It didn’t make sense for the lab to spend the many hours required to automate each of the almost 800 forms that remained in Massachusetts, even with the Weaver’s help. We focused on improving the tools and helping others use them for the most important forms in their own states. GPT-3 and 4 have inspired computer scientists to experiment with having an AI re- organize legal information. Some works have found success in using GPT-4 to convert other forms of legal knowledge, like legislation, into formal structures [5]. Having an AI do all of the work of forms (scanning a template for variables, asking the user questions, and following up until all variables are defined) is also being debated in hallways at AI and law conferences; touted on Reddit [6]; and discussed as an idea in general interest magazines [7]. We do not think that it is a good idea except for very simple forms. In- stead, we suggest a hybrid approach, with a GPT assisting with the first draft and a hu- man editor turning it into a usable interactive legal application. If it succeeds, this ap- proach can lower the cost to automate court and similar forms dramatically, making it re- alistic for a state to automate hundreds of forms and reap the benefits for self-represented litigants, including a mobile-friendly interface and integration with electronic filing sys- tems.\n\nSome of our experiments are tentative and early. We acknowledge the limits of our approaches so far. But the promise is hard to deny.\n\n1.1. Is the form the thing?\n\nBefore we describe our project, we acknowledge the centrality of forms in current ap- proaches to serving self-represented litigants, and that forms may not be the best possi- ble approach to serve that goal. Branting [8] has argued that narratives, not forms, are the tools that lawyers best use to structure effective legal claims. Whether forms or narratives are the best presentation, ultimately, what decision makers need are the facts to make a decision.\n\nDespite that, we start our work with the form. We annotate it, add labels, and turn it into a guided interview in an automated way. There are thousands of existing legal forms (at least 25,000 in the US alone [9]) that represent codified legal systems and processes. The work that went into producing the paper form lives in the form’s instructions, fields, and checkboxes, ready for the author of an interactive tool to interpret. Even if we want to abandon them eventually, forms are a great place to start.\n\n2. Traditional interactive legal application authoring approaches An interactive legal application that completes a form has three components:\n\n1. An output document, turned into a template with fields.\n\n2. A series of questions, organized into one or more screens.\n\n3. A set of logical rules that decide which followup questions are needed and which boxes or paragraphs are included in the final output.\n\nHuman authors perform 4 tasks related to these components:\n\n1. Label fields in the template.\n\n2. Create a brief prompt for each field.\n\n3. Group fields into a logical order.\n\n4. Add any conditional logic needed.\n\nThis work can take hundreds of hours. One representative project, Massachusetts Defense for Eviction (MADE) [10], which took about a year to complete, contains:\n\n1. 1,100 lines of Python code\n\n2. 6,141 lines of YAML (or about 30,000 words)\n\n3. 13 documents in Microsoft Word format, totalling 13,000 words\n\n2.1. Interactive legal applications require thoughtful choices that affect usability While it is tempting to think of the process of creating an interactive legal application from a form as simply letting the user “fill in the blanks,” the process requires judgment in several areas. Jarrett and Gaffney’s text Forms That Work [11] discusses some of those choices, including: the order of questions, how to provide help, and how to respect the user of your form. The UK’s National Health Service [12] and SurveyMonkey [13] have similar guidance. Our lab developed a detailed style guide with guidance for the authors of legal apps [14]. We also describe these choices that affect burden in our paper RateMyPDF [9].\n\nSome of these choices can be made once and repeated again and again. For example: we ran usability tests with our questions that ask for a participant’s name. We researched and applied best practices to gather a litigant’s pronouns and gender. Through practice, we determined an optimal structure for a guided interview that completes a court form. We built the Assembly Line Weaver tool to help us implement these and other choices consistently across each form that we turned into an interactive legal application.\n\n3. The Assembly Line Weaver\n\nThe Assembly Line Weaver (Figure 1) is a template-driven guided interview that pro- duces drafts of guided interviews in the YAML format. It strikes a balance between the ease of use of fully graphical authoring environments and the power and flexibility of Docassemble.\n\nThe Weaver:\n\n1. Scans a pre-prepared Word or PDF template for field names.\n\n2. Asks for metadata about the form.\n\nFigure 1. The opening screen of the Assembly Line Weaver\n\n3. Allows the author to customize the location, branding, and template used to au- thor the interview.\n\n4. Asks the author to provide “before and after” context to the form, including next steps the user will need to take.\n\n5. Allows the author to add a prompt for each field.\n\n6. Asks the author to assign fields to screens, with any extra context that is needed. In the context of the Weaver, “template-driven” means that we have created a YAML file that has the basic structure of a Docassemble interview. The provided template includes the following screens:\n\n1. A title screen.\n\n2. A “before you start” screen.\n\n3. Questions, both those created by the author and from the question library for information like party names, pronouns, addresses, and more.\n\n4. A preview screen.\n\n5. A “review” screen that allows the user to edit their responses.\n\n6. A signature screen (with ability to text or email the document for signature by a third party).\n\n7. A download screen.\n\nThe output of the Weaver is a .ZIP file that includes the customized YAML file, the output template, and a template representing the “Next steps” document that can all be further customized. The Weaver does not assist the author with adding logic (or rules) to the guided interview that they produce.\n\n4. A vision for reducing human effort in form automation At our lab, we were interested to see if generative AI could help reduce the human labor needed to create good guided interviews. Ultimately, we prototyped the automatic adding of labels to fields, drafting of prompts, and grouping questions with the assistance of GPT-3 and GPT-4 turbo. We experimented with using ChatGPT+ to convert laws into Python code, but because identifying the statute often requires research outside of the form, we do not plan to integrate this with our automated authoring approach. In order to maximize our team’s productivity, we took two approaches to two unique document types: with Word documents, we focused on identifying and labeling the place- holder variables, which was a significant gap in our existing Weaver tool’s capabilities. We left room for these labels to be fed directly into the Weaver in the future, which would take advantage of the structure of our template-driven guided interview authoring. With PDF documents, we prototyped a solution that directly authors a limited version of a Docassemble interview in YAML format. This let us quickly experiment with the inter- view flow that a large language model would produce. While this still allows for human editing and review, it doesn’t take advantage of the existing work we have done with the Weaver to standardize the output. Ultimately, we expect to combine the two approaches.\n\n5. Auto-labeling of Word documents\n\nPDF and Word (DOCX) documents have some key differences: PDFs are usually final documents, meant to be printed and filled in by hand at least some of the time. That means that they almost always include lines, boxes, and circles where the user’s answer goes. These features can be detected with traditional computer vision tools like OpenCV [15].\n\nIn contrast, Word documents can be edited in a word processor before they are printed and filed. Word documents are more likely to include placeholder text, like [CLIENT NAME], than blank spaces marked by lines or boxes. While one document might consistently use square brackets, another might mix in curly brackets, or leave out the placeholder marker altogether in favor of in-line text like “Your name” or “Dear Merchant.” This means that totally rules-based systems, like regular expressions, do not perform well at identifying and labeling placeholders. Statistical approaches, enabled by large language models, can better handle the many variations.\n\nSeparately, it was important that whatever method we adopted for labeling the Word document did not disrupt the existing formatting of the document. These features include paragraphs, automatic numbering, and bold and italics, features that are not present in plain Unicode text. That meant the naive approach of sending just the text of the docu- ment wouldn’t work. Internally, a Word document is an XML document, which can be represented in plain text. However: sending the full uncompressed XML of the document would almost always exceed GPT-4’s context window of 4,096 tokens. Although GPT-4 turbo expanded the context window for inputs, the context window for replies remains 4,096 tokens.\n\nWe did not want totally free-form labels for variables in our documents. We wanted the automatic labeling to follow some basic rules:\n\n• Variables have to be valid Python identifiers, and follow Python conventions like using underscores and lower case letters.\n\n• We have a pre-defined set of nouns, like users, other parties, attorneys, and more that we preferred to use when applicable, and we always use these nouns as members of a list.\n\n• We have a set of predefined attributes for a person, like name, address, and so on that we wanted to use exactly the same way whenever collecting names, addresses, or similar common attributes.\n\nWe include these variable naming conventions in our prompt to the model. A Python notebook with our code can be found in our GitHub repository: https: //github.com/SuffolkLITLab/FormFyxer/blob/docx-fields-experiment/ explore_labeling_docx.ipynb\n\n5.1. Tokenizing the input document\n\nTo label our Word document, first we use the open source python-docx [16] library to extract the paragraphs and runs of formatted text in each paragraph. These indices in the Python representation can be referenced and used to modify the formatted document later.\n\nNext, we turn the paragraphs and runs of text into a JSON structure, like this:\n\n[ [0, 1, \"Dear John Smith:\"], [1, 0, \"I am writing to claim ...\"], [2, 0, \"[Optional: if ...]\"], ]\n\nWhere each entry in the JSON list is a 3-tuple of paragraph number, run number, and the text of that run. Note: this approach does not yet handle tables, which sometimes appeared in our dataset.\n\n5.2. Prompting the large language model\n\nOur first attempt was to instruct the model to reply with a JSON structure that includes a tuple of the paragraph number, run number, the Jinja2 variable, and the starting and ending position of the text to be replaced. We also instructed the model to reply with only the modified text, rather than returning the full modified document. In gpt-4o-mini this prompt ignored our instruction to return only the modified runs, instead returning all of the text of the modified document in the reply. In both GPT-3.5 and GPT-4, this prompt failed to accurately mark the start and end position of the text in a way that allowed us to insert Jinja2 variables in context. When we used the positions that were returned by the model, the Jinja2 variables would be inserted haphazardly in the middle of the existing placeholder text, leaving a messy output document that required a lot of manual cleanup.\n\nSee Figure 2.\n\nA more successful approach (that also used more input tokens) was to prompt the model to return the full text of each modified run. We then iterated through the return value to replace each run with its modified text. Because a run in the DOCX format all has the same formatting, we could safely replace the text of the original run with the mod- ified run. This worked even with very idiosyncratic markup in the example document, as shown below.\n\nOur revised prompt succeeding in identifying the placeholder text and replacing it in full. See Figure 4.\n\nFigure 2. Example of failed GPT-3.5 prompt, with Jinja2 variables inserted in the middle of placeholder phrases\n\nFigure 3. An example document from the wild, demonstrating a wide variety of strategies to mark placeholder text within a single document\n\nThis approach appeared to perform well across a range of documents, a sample of which can be viewed in the GitHub repository. We did not, however, have a large set of pre-annotated documents to benchmark this performance.\n\n5.3. Using the output to build an interactive legal application\n\nOnce the Word document is labeled, it can be uploaded to the Weaver. While the author can then assign questions manually for each field, the author can also choose to use our “auto drafting mode,” which leverages heuristics and a traditional machine learning approach, but requires a lot of human editing. We describe this existing approach in our paper describing RateMyPDF.[9]\n\n6. Building interactive legal apps automatically with PDF documents\n\nOur experiment with PDF documents authors Docassemble interviews in YAML format without any human intervention. With important limits, our approach was successful.\n\nFigure 4. Example of successful output with the revised prompt\n\nFigure 5. The first page of an automatically generated name change interview\n\nFigure 6. Final screen of the automatically generated name change interview, side by side with the completed PDF\n\nWe generated contextually appropriate labels, questions, and created screens for each question. (Figure 5) The interviews ran and filled in fields in the PDF document. (Figure 6) But the PDF’s stream-based format made it difficult to accurately identify and label all fields, particularly checkboxes. We describe our success rate and the different ways we worked around this limit below.\n\nWe tested this approach with name change forms taken from 12 jurisdictions. We were able to automatically recognize and write questions for 62% to 69% of the existing fields. On the best performing form, we were able to do so for 93% of the fields, com- pared to 27% on the worst performing form. For the fields that we could not identify, the problem stemmed from difficulties placing them in context as part of the text provided to the GPT.\n\nAll of the code, along with our input and output documents, for this section can be found in an interactive Python notebook, presented here: https://colab.research.google.com/drive/1HRv5imbtHIVYt37h4liwFkW9YlPUNNKI?usp=sharing.\n\n6.1. Finding the fields in context\n\nWe wanted to feed the GPT model the full text of the form, with the field’s placeholders in the right spot in the text, so that the model could come up with appropriate names, just like we did for DOCX files. Ultimately, we found an approach that succeeded most of the time, and we conclude this is a potential place for either further engineering work or adding a human in the loop.\n\nFirst, we experimented with the native PDF feature of ChatGPT+. As of this writing, you can directly upload a PDF to ChatGPT+ and interact with its contents. We were able to ask ChatGPT+ what fields it thought a PDF of a court form had and get back an answer. However, this feature interacts only with the text, and not the form layer of the PDF. In some cases, it identified fields in the PDF that were not in the form layer, because they were designed to be completed by the court clerk (for example, a “case number” field). In addition, it cannot be used to add text to the PDF at all.\n\nNext, we experimented with text-only approaches. It is possible to get the full text of a PDF with existing open source tools, but this text does not contain any representation of the form layer and its fields. The PDF file format is stream-based, with field elements occupying a stream independent of the PDF’s text. Each field element has a defined position on the page, and so does the text, but understanding the relative position of the two requires rendering the PDF, particularly if, as is often the case, there is one large text object in the PDF, with only whitespace added to make room for the PDF fields.\n\nThe final, complex, approach that we found was the most reliable was to:\n\n1. fill each field in the form layer with placeholder text (e.g, field 01)\n\n2. convert each page of the PDF into a PNG image\n\n3. apply optical character recognition (OCR) to the images to get the placeholders in context (e.g., “Your name: field 01”).\n\nThis approach worked well for fields where the user was expected to write an answer, but it often failed for checkboxes. They were simply too small to add OCR-visible text to. To address this, we did not label small fields in step 1. We reintroduced checkboxes and other small fields when asking the large language model to generate questions, described below.\n\n6.2. Leveraging context to create variable names, definitions, and questions Given a string for the form’s text with variable placeholders in-line, it is possible to have a large language model create appropriate variable names, along with definitions and questions. Building upon the output of step 3 from Section 6.1, we can:\n\n1. use an LLM to generate a name and description for the document;\n\n2. use an LLM to change placeholder names into semantically appropriate names (e.g., first name) based on the output string;\n\n3. use an LLM to write definitions for said variables, again based on the above string;\n\n4. leverage these definitions and an LLM to author questions for users and guess at their data types; and\n\n5. convert these questions into a Docassemble interview by writing a YAML file that takes user answers and fills them into a processed version of the original PDF.\n\nCheckboxes are handled specially by our prompt. Because we skip checkboxes at our form filling step, at this stage we prompt the large language model to try to assign each checkbox a label and definition based on any text element adjacent to the field on the PDF. This approach could only pair checkbox fields with related text 28% of the time. We note that checkbox fields can be important on court forms. The user can ask for relief, identify a legal claim in a complaint, or exercise their right to a jury trial by checking the appropriate box. Therefore, while the OCR method we describe has clear promise, an important future task is to improve our accuracy with checkbox fields.\n\n6.3. Validating user interactions\n\nWe explored two methods to add input validation in our generation of interviews for PDF forms: a “large language model in the loop” when the user interacted with the form, and an approach that asked the model to classify the input’s data type when it first parsed the form.\n\nThe “large language model in the loop” approach, while approaching a natural con- versation, was resource intensive. After every response by a user, we asked GPT if the answer responded to the question. If not, the user was told why and asked to answer again. In addition to being expensive, it also risked annoying users by asking too many questions. In testing, it handled redirecting answers like “how is the weather today?” when presenting the user with the question “What is your full name?” well. But we were concerned that the model could be too rigid. For example, would it refuse valid but un- usual address lines? What about unusual names, like X? Given the risk of offense and the cost, we abandoned this approach early on.\n\nThe up-front classification of answers into datatypes also had risks. For example, phone numbers were often assigned a numeric data type, which precluded users from entering phone numbers of the form 555-5555, accepting only 5555555. ZIP codes were assigned a numeric type too, which strips leading 0’s from ZIP codes, causing Mas- sachusetts’s ZIP codes to appear incorrect when output. While human review could catch these validation failures, using standardized questions wherever possible, like the Weaver, would cut down the amount of review an author needed to perform.\n\n6.4. Additional Limitations & Potential Points of Intervention\n\nThe section above does not address the handling of fields that were neither in the col- lection of potential checkboxes nor those for which a new name and definition could be made. Only 14% of the fields in our sample of forms couldn’t be placed in-line with the OCR method or identified as a potential checkbox. On the best performing form, every field was either placed in-line or identified as a check box. On the worst performing form, 28% of the fields remained unidentified.\n\nThe authors of Docassemble interviews can examine and edit the output of the final YAML file to correct any errors and improve the usability of the generated interview. But in the best-case scenario the full document can be completed. This was most likely to be true when the original PDF had few or no checkboxes.\n\nWe noticed that some errors could compound. For example: a missing or incorrectly labeled field would lead to an incorrect question and an incorrect series of screens. Early review, at multiple stages in the automation, could simplify the author’s final editing task.\n\nWe identified 2 helpful intervention points:\n\n• Before the the title, description, variable names and definitions are finalized\n\n• Before questions and data types are finalized\n\nWe prototyped this review process by allowing the author to edit a JSON object in our Python notebook at each stage.\n\n7. Reaching level 1\n\nIn our paper Digital Curb Cuts [3] we described a maturity model for guided interviews, with a level 1 form being at least as good as the experience of completing the original PDF, and level 4 representing a highly polished user experience. Many forms, now locked up in PDFs, would benefit simply from the increased usability of a responsive, mobile friendly design that can be integrated with electronic filing. Our experiments with GPT show that achieving this basic level 1 of automation is possible with a large language model and some Python code alone.\n\n7.1. Weaving the two approaches together\n\nWe had better success with identifying and labeling fields in Word documents than PDFs, despite the more ambiguous markers that the Word documents used to label placeholders (e.g., see Figure 3). But we only used the GPT model to draft questions and put them in order with PDF documents. It’s likely that the full interview authoring approach that we took with PDF documents would succeed with labeled Word documents. We could also continue to experiment with solutions to the context problem with PDF checkbox fields. We think the best way to approach both experimental features is to integrate them with our existing Weaver tool. The unconstrained LLM authoring approach we took with the PDF automation would be enhanced with the template-driven and shared-question approach used in the Weaver. This would limit the need for auto-generated definitions and questions to only those field types not in the Weaver’s library. We could get the best of both worlds: vetted, usability tested questions for common fields, and good drafts of the unique questions drafted by the large language model.\n\n8. Conclusion\n\nGenerative AI can identify user inputs and create interview questions for a wide variety of forms. It can work with both DOCX and PDF formats. For the very simplest forms, especially those without very much conditional logic, large language models can help to produce draft automations that require almost no human intervention at all. However, this AI cannot reliably automate the large number of more complex legal documents without human assistance. Human review and editing of these interviews remains essen- tial. Integrating this human review flexibly at different points in the automation process allows humans to intervene as much or as little as needed, allowing humans to adjust to the needs of a wide range of input documents.\n\nThe hybrid approach of automated drafting of a traditional guided interview can get the time saving benefits of the large language model without the risks of fully automated question and answer driven form-filling. It has the potential to significantly expand the number and kind of forms that can benefit from document automation.\n\nReferences\n\n[1] Lauritsen M, Steenhuis Q. Substantive Legal Software Quality: A Gathering Storm? In: Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law. Montreal QC Canada: ACM; 2019. p. 52-62. Available from: https://dl.acm.org/doi/10.1145/3322640.3326706.\n\n[2] Westermann H, Benyekhlef K. JusticeBot: A Methodology for Building Augmented Intelligence Tools for Laypeople to Increase Access to Justice. In: Proceedings of the Nineteenth International Confer- ence on Artificial Intelligence and Law. ICAIL ’23. New York, NY, USA: Association for Computing Machinery; 2023. p. 351-60. Available from: https://doi.org/10.1145/3594536.3595166.\n\n[3] Steenhuis Q, Colarusso D. Digital Curb Cuts: Towards an Open Forms Ecosystem. Akron Law Review. 2021;54(4):2. Available from: https://ideaexchange.uakron.edu/akronlawreview/vol54/i ss4/2/.\n\n[4] Pyle J. Docassemble; 2021. Available from: http://docassemble.org/.\n\n[5] Janatian S, Westermann H, Tan J, Savelka J, Benyekhlef K. From Text to Structure: Using Large Lan- guage Models to Support the Development of Legal Expert Systems; 2023.\n\n[6] talktothelampa. How to use LLM in order to fill an intake form? [Reddit Post]; 2023. Available from: www.reddit.com/r/LangChain/comments/140jhln/how_to_use_llm_in_order_to_fill_ an_intake_form/.\n\n[7] Broadway M. Can ChatGPT Fill Out Forms? Yes, here’s how; 2023. Available from: https://www. pcguide.com/apps/can-chatgpt-fill-out-forms-yes-heres-how/.\n\n[8] Branting K, McLeod S. Narrative-Driven Case Elicitation. Workshop on Artificial Intelligence for Access to Justice (AI4AJ 2023). 2023 Jun.\n\n[9] Steenhuis Q, Willey B, Colarusso D. Beyond Readability with RateMyPDF: A Combined Rule-based and Machine Learning Approach to Improving Court Forms. Proceedings of International Conference on Artificial Intelligence and Law (ICAIL 2023). 2023:287-96.\n\n[10] Steenhuis Q. Making MADE: User-centered Design in Practice – Quinten Steenhuis; 2019. Available from: https://www.nonprofittechy.com/2019/05/12/making-made-user-centered-des ign-in-practice/.\n\n[11] Jarrett C, Gaffney G, Krug S. Forms that Work: Designing Web Forms for Usability. 1st ed. Amsterdam; Boston: Morgan Kaufmann; 2008.\n\n[12] How to write good questions for forms - NHS digital service manual;. Available from: https://serv ice-manual.nhs.uk.\n\n[13] Survey Monkey. Smart Survey Design. Survey Monkey; 2008. Available from: https://s3.amazo naws.com/SurveyMonkeyFiles/SmartSurvey.pdf.\n\n[14] Writing good questions | The Document Assembly Line Project;. Available from: https://suffolkl itlab.org/docassemble-AssemblyLine-documentation/docs/style_guide/question_o verview.\n\n[15] Bradski G. The OpenCV Library. Dr Dobb’s Journal of Software Tools. 2000.\n\n[16] python-docx — python-docx 1.1.0 documentation;. Available from: https://python-docx.readth edocs.io/en/latest/. \n",
    "model": "gpt-4o-mini",
    "temperature": 0.7,
    "max_tokens": 250,
    "output": 0,
    "json_mode": 0,
    "output_to": 4,
    "behavior": "Summarize & question paper",
    "hide_button": false
  }
}