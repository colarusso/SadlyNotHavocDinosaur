<!DOCTYPE html>
<HTML><HEAD>

  <!-- Set base for this page equal to domain root -->
  <base href="../../">

  <!-- Page-specific metadata -->
  <title>Flag Logical Fallacies with a Browser Extension: Have AI read a webpage and flag logical falacies</title>
  <meta property="og:type" content="website"/>
  <meta property="og:publish_date" content="2024-02-21T00:00:00-0500"/>
  <meta property="og:title" content="Flag Logical Fallacies with a Browser Extension: Have AI read a webpage and flag logical falacies"/>
  <meta property="og:description" content="I'm not an AI doomer in the sense that I don't think much about p(doom)—the probability that AI will end humanity. I'm much more worried about megacorps exploiting AI for profit at the expense of the marginalized. Which is to say, I'm more concerned about the world of RoboCop than I am of the Terminator. I am, however, sympathetic to those who worry that AI-generated misinformation could supercharge the asymmetry at the center of Brandolini's Law, which states, &quot;The amount of energy needed to refute bullshit is an order of magnitude bigger than that needed to produce it.&quot; There's a lot to worry about in a world where the cost of BS production drops nearly to zero. So, I thought I'd explore what could be done for the side of the angles. Today's template aims to spot and flag logical fallacies based on the text of a webpage—like an AI fact checker of sorts. "/>
  <meta property="og:image" content="images/50-days/bingo_square.png"/>
  <meta property="og:image:width" content="1024" />
  <meta property="og:image:height" content="1024" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ST9X6H808L"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-ST9X6H808L');
  </script>

  <!-- Metadata for mobile -->
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
  <meta name="apple-mobile-web-app-capable" content="no" />
  <link rel="apple-touch-icon" href="images/comic.png"/>

  <!-- JS & style -->
  <link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
  <link rel="stylesheet" type="text/css" href="css/style.css?v=2024-01-30">
  <script src="js/functions.js?v=2024-01-30"></script>
  <script src="js/spin.js"></script>

  <link rel="stylesheet" href="css/prism.css" data-noprefix="">
  <script type="text/javascript" src="js/prism.js"></script>

  <!--<script id="MathJax-script" async src="js/mathjax/tex-mml-chtml.js"></script>

  <link rel="stylesheet" type="text/css" href="css/green-audio-player.css">
  <script src="js/green-audio-player.js"></script>-->

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="Sadly Not, Havoc Dinosaur" href="https://sadlynothavocdinosaur.com/feed.xml" />

</HEAD>
<BODY BGCOLOR="#ffffff" BACKGROUND="" MARGINWIDTH="0" MARGINHEIGHT="0">

<!-- Message Banner -->
<div id="msg_bar" style="display:none;"></div>

<!-- Title and search -->
<div class="title_bar">
  <div class="home">
    <a href="./" tabindex="1"><img src="images/home.png" class="home_btn"></a>
  </div>
  <div class="search">
    <a href="javascript:show_search();" tabindex="3"><img src="images/search.png" class="search_btn"></a>
    <input id="query" type="text" tabindex="2"/>
  </div>
  <span id="title"><a href="./" class="title_home">Sadly Not, Havoc Dinosaur</a></span>
</div>

<div class="content">
  <!-- START PAGE CONTENT -->

  <div id="page">
  <!--
    =================================================

                      INTRODUCTION

    =================================================
  -->
  <h1 class="post_title_01">Flag Logical Fallacies with a Browser Extension</h1>
  <div class="post_title_02">Have AI read a webpage and flag logical falacies</div>
  <div class="featured_img_right">
    <!--<div class="audio_container_container" style="display:show;">
      <div class="audio_container">
        <b>Hear the author read <i>TK</i></b>
        <div class="gap-example player-accessible">
          <audio>
              <source src="mp3s/title.mp3" type="audio/mpeg">
          </audio>
        </div>
        <span class="playback">
          Speed: <a href="javascript:void('')" onClick="set_speed(0.5)" class="playback" id="pb05">0.5x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(1)" class="playback" id="pb10" style="font-weight:900;">1x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(1.5)" class="playback" id="pb15">1.5x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(2)" class="playback" id="pb20">2x</a> &bull;
                  <a href="javascript:void('')" onClick="set_speed(3)" class="playback" id="pb30">3x</a>
        </span>
      </div>
    </div>-->
    <a href="images/50-days/bingo.png"><img src="images/50-days/bingo.png" ALT="a collection of dogs and cats playing bingo" class="list_img_file"/></a>
    <div class="caption">
      Dogs and Cats Playing BINGO, latent space "photography" by <a href="https://mastodon.social/@Colarusso" target="_blank" class="captionlnk">Colarusso</a>
    </div>
  </div>
  <p class="post_p">
    <a href="https://mastodon.social/@Colarusso" target="_blank" class="body_links"><img src="images/colarusso.jpg" class="headshot_small" alt="Headshot of the author, Colarusso." style="margin-top: 7px;"/></a>
    David Colaursso<br><span class="post_date">Co-director, Suffolk's <a href="https://suffolklitlab.org/" target="_blank" class="captionlnk">Legal Innovation &amp; Tech Lab</a></span>
  </p>
  <p><i>This is <b>the 23rd</b> post in my series <a href="posts/50-days-of-lit-prompts">50 Days of LIT Prompts</i></a>.</p>

  <p>
    I'm not an AI doomer in the sense that I don't think much about <a href="https://www.nytimes.com/2023/12/06/business/dealbook/silicon-valley-artificial-intelligence.html" target="_blank">p(doom)</a>—the probability that AI will end humanity. I'm much more worried about megacorps <a href="https://link.springer.com/article/10.1007/s13347-020-00405-8" target="_blank">exploiting AI for profit at the expense of the marginalized</a>. Which is to say, I'm more concerned about the world of RoboCop than I am of the Terminator. So, I am sympathetic to those who worry that AI-generated misinformation could supercharge the asymmetry at the center of <a href="https://en.wikipedia.org/wiki/Brandolini%27s_law" target="_blank">Brandolini's Law</a>, which states, "The amount of energy needed to refute bullshit is an order of magnitude bigger than that needed to produce it." There's a lot to worry about in a world where the cost of BS production drops nearly to zero. So, I thought I'd explore what could be done for the side of the angels. Today's template aims to spot and flag logical fallacies based on the text of a webpage—like an AI fact checker of sorts. Below is output from today's template. It's what the template produced from the text of <a href="https://www.science.org/content/article/republican-lawmaker-rocks-tumbling-ocean-causing-sea-level-rise" target="_blank">an article</a> documenting a hearing on global warming before the U.S. House of Representatives Science, Space and Technology Committee from May 2018. I found it, and the article that follows in the dateset associated with this post on an <a href="https://www.logical-fallacy.com/articles/dataset-review/" target="_blank">training logical fallacy detectors</a>.
  </p>
  <section class="line-numbers">
    <pre class="language-javascript" style="white-space:pre-wrap;"><code>{
      "fallacies": [
        {
          "fallacy": "False cause",
          "explanation": "The assertion that rocks tumbling into the ocean are causing sea level rise is an example of a false cause fallacy. This argument incorrectly assumes a causal relationship between two events (erosion and sea level rise) without sufficient evidence to prove that one causes the other, especially in the context of global sea level rise driven by climate change."
        },
        {
          "fallacy": "Appeal to authority",
          "explanation": "When Rep. Lamar Smith cites an opinion piece published in The Wall Street Journal that claims sea levels are not rising because of climate change, it's an appeal to authority fallacy. This fallacy occurs because the argument relies on the authority of the publication and the author, rather than on scientific evidence or consensus regarding climate change and sea level rise."
        },
        {
          "fallacy": "Personal incredulity",
          "explanation": "Rep. Mo Brooks expressing skepticism about mainstream scientific consensus on the factors contributing to sea level rise, despite being presented with evidence from reputable scientific organizations, is an example of personal incredulity. This fallacy is present when someone finds a concept difficult to understand, or outside their personal experience, and therefore concludes it cannot be true."
        }
      ]
    }</code></pre>
  </section>
  <p>In case you didn't click through to the article, it was published on <i>Science.com</i> and titled <i> Republican lawmaker: Rocks tumbling into ocean causing sea level rise. Climate scientist tries to correct false statements at hearing</i>. Maybe this is a promising tool we can release at scale to address all that ails or fractured media ecosystem. If we can't get critical thinking an literacy taught in school, maybe this is the silver bullet we've been looking for... Hold that thought. Here's the output from another <a href="https://www.theguardian.com/environment/2017/mar/21/record-breaking-climate-change-world-uncharted-territory" target="_blank">article</a>, this one from 2017 titled <i>Record-breaking climate change pushes world into 'uncharted territory'</i>.</p>
  <section class="line-numbers">
    <pre class="language-javascript" style="white-space:pre-wrap;"><code>{
      "fallacies": [
        {
          "fallacy":"Appeal to emotion",
          "explanation":"The text uses emotionally charged language, such as describing the climate situation as 'startling' and stating that 'our children and grandchildren will look back on the climate deniers and ask how they could have sacrificed the planet.' This is designed to evoke an emotional response rather than presenting a purely logical argument."
        },
        {
          "fallacy":"Ad hominem",
          "explanation":"The text indirectly attacks climate change deniers and the Trump administration by suggesting they 'continue to bury their heads in the sand' and implying they are sacrificing the planet for 'cheap fossil fuel energy.' This could be seen as attacking the character or motives of those who disagree with the presented viewpoint on climate change, rather than addressing their arguments directly."
        },
        {
          "fallacy":"Appeal to authority",
          "explanation":"The article references the opinions and statements of various scientists and experts to support its claims about climate change. While their expertise is relevant, the argument leans heavily on their authority without providing detailed evidence for all claims, which could be seen as an appeal to authority."
        }
      ]
    }</code></pre>
  </section>
  <p>
    Originally, I was going to call this post "Bias Bingo," hence the image of the cats and dogs playing bingo, but I ended up setteling on fallacies instead of biases. That being said, I feel the need to focus on one bias for a moment. It's a pet peeve of mine—<a href="https://en.wikipedia.org/wiki/False_balance" target="_blank">false balance</a>, sometimes called bothsidesism. False ballance is the tendency of the media to treat both sides of an argument as if they are equally valid even when this isn't supported by the evidence (e.g., providing equal air time to Flat Earthers and NASA scientist). The above smells a lot like bothsidesism. The latter list has a certain feel of truthiness to it. Yes, the second articel does appeal to emotion, but sometimes such appeals are appropriate. Likewise, both articles appeal to authority. However, the fallacy applies only when such an appeal is made blindly without regard for the evidence or relevant expertise such an authority might possess. That being said, the template flagging these aspects of the article did actually force me to more deeply engage with the arguments. So, maybe there's something to it. If I had to pick a use for the template as is, I'd suggest something with a lot of back and forth. I think it could be good for writers to who want to strengthen their arguments. They could point the template at their own writing and defend against any purported fallacies. I wouldn't, however, call its output authoritative. 
  </p>
  <p>
    Evey few years someone comes along saying AI will solve content moderation, but the truth is <a href="https://www.techdirt.com/2023/05/11/moderator-mayhem-a-mobile-game-to-see-how-well-you-can-handle-content-moderation/" target="_blank">content moderation is hard</a>, because just like spotting fallacies it requires an understanding of human nuance. Let's be honest, the template was playing the part of a high school debater, looking for structural weaknesses in an argument and narrowly interpreting what facts are in evidence. It's acting like a <a href="https://tvtropes.org/pmwiki/pmwiki.php/Main/StrawVulcan" target="_blank">straw Vulcan</a>, failing not because rationality failed but because it isn't actually acting rationally. Remember, LLMs are sentence completion machines. Here they are engaged in making only what appears to be a rational analysis.
  </p>
  <p>
    You might be wondering if it's worth the time it would take to work through the example below, and I would say, yes. When you <a href="posts/flag-fallacies/#tires">kick the tires</a>, I'll ask you to do your best to "fix" the output, and just like playing <a href="https://moderatormayhem.engine.is/" target="_blank">Moderator Mayhem</a> (a game that simulates content moderation) the process will help illustrate the difficulties inherent in getting an LLM to "act logically." Do I think these tools will get better? Yes. Do I think that something like this template could actually help people spot logical fallacies? Yes, but just like content moderation, I doubt AI as is <a href="https://www.techdirt.com/2023/08/17/openai-says-gpt-4-is-great-for-content-moderation-but-seems-a-bit-too-trusting/" target="_blank">can do it all</a>. That being said...
  </p>

  <!-- END INTRO -->

  <h3><a name="build" href="posts/socrates-lives/#build" class="anchor" alt="deep link to this section"></a>Let's build something!</h3>
  <p>
    We'll do our building in the LIT Prompts extension. If you aren't familiar with the LIT Prompts extension, don't worry. We'll walk you through setting things up before we start building. If you have used the LIT Prompts extension before, skip to <a href="posts/flag-fallacies/#template">The Prompt Pattern (Template)</a>.
  </p>
  <h3><a name="upnext" href="posts/flag-fallacies/#upnext" class="anchor" alt="deep link to this section"></a>Up Next</h3>
  <ul>
    <li><a href="posts/flag-fallacies/#setup" onClick="expand_setup();">Setup LIT Prompts</a></li>
    <ul>
      <li><a href="posts/flag-fallacies/#install" onClick="expand_setup();">Install the extension</a></li>
      <li><a href="posts/flag-fallacies/#point" onClick="expand_setup();">Point it at an API</a></li>
    </ul>
    <li><a href="posts/flag-fallacies/#template">The Prompt Pattern (Template)</a></li>
    <li><a href="posts/flag-fallacies/#tires">Kick the Tires</a></li>
    <li><a href="posts/flag-fallacies/#references">TL;DR References</a></li>
  </ul>
  <p>
    <b>Questions or comments?</b> I'm on Mastodon <a href="https://mastodon.social/@Colarusso" target="_blank">@Colarusso@mastodon.social</a>
  </p>
  <!--
    =================================================

                   Setup LIT Prompts

    =================================================
  -->
  <hr>
  <h2><a name="setup" href="posts/flag-fallacies/#setup" onClick="expand_setup();" class="anchor" alt="deep link to this section"></a>Setup LIT Prompts </h2>
  <div id="expand_setup" style="text-align: left;display:none;font-size: small;">
    <a href="javascript:expand_setup();" style="text-decoration: none;">&#9658; Expand</a>
  </div>
  <div id="collapse_setup" style="text-align: left;font-size: small;">
    <a href="javascript:collapse_setup();" style="text-decoration: none;">&#9660; Collapse</a>
  </div>
  <div id="setup_extension">
    <div class="list_vid">
      <iframe class="embed_vid" src="https://www.youtube-nocookie.com/embed/Ql8aXGvLBGU" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
      <div class="caption">
        7 min intro video
      </div>
    </div>
    <p>
      <i><b>LIT Prompts</b></i> is a browser extension built at Suffolk University Law School's <a href="https://suffolklitlab.org/" target="_blank">Legal Innovation and Technology Lab</a> to help folks explore the use of <a href="https://en.wikipedia.org/wiki/Large_language_model" target="_blank">Large Language Models</a> (LLMs) and <a href="https://en.wikipedia.org/wiki/Prompt_engineering" target="_blank">prompt engineering</a>. LLMs are sentence completion machines, and prompts are the text upon which they build. Feed an LLM a prompt, and it will return a plausible-sounding follow-up (e.g., "Four score and seven..." might return "years ago our fathers brought forth..."). LIT Prompts lets users create and save prompt templates based on data from an active browser window (e.g., selected text or the whole text of a webpage) along with text from a user. Below we'll walk through a specific example.
    </p>
    <p>
      To get started, follow <b>the first four minutes</b> of the intro video or the steps outlined below. <i>Note: The video only shows Firefox, but once you've installed the extension, the steps are the same.</i>
    </p>
    <h3><a name="install" href="posts/flag-fallacies/#install" class="anchor" alt="deep link to this section"></a>Install the extension</h3>
    <p>Follow the links for your browser.</p>
    <ul>
      <li>
        <b>Firefox:</b> (1) visit the extension's <a href="https://addons.mozilla.org/en-US/firefox/addon/lit-prompts/" target="_blank">add-ons page</a>; (2) click "Add to Firefox;" and (3) grant permissions.
      </li>
      <li>
        <b>Chrome:</b>  (1) visit the extension's <a href="https://chromewebstore.google.com/detail/lit-prompts/hfeojjmldhebkeknfapoghcohkhffcmp" target="_blank">web store page</a>; (2) click "Add to Chrome;" and (3) review permissions / "Add extension."
      </li>
    </ul>
    <p>
      If you don't have Firefox, you can <a href="https://www.mozilla.org/en-US/firefox/new/" target="_blank">download it here</a>. Would you rather use Chrome? <a href="https://www.google.com/chrome/" target="_blank">Download it here</a>.
    </p>
    <h3><a name="point" href="posts/flag-fallacies/#point" class="anchor" alt="deep link to this section"></a>Point it at an API</h3>
    <p>
      Here we'll walk through how to use an LLM provided by OpenAI, but you don't have to use their offering. If you're interested in alternatives, you can find them <a href="https://github.com/SuffolkLITLab/prompts/tree/main#openai-compatible-api-integration" target="_blank">here</a>. You can even run your LLM locally, avoiding the need to share your prompts with a third-party. If you need an OpenAI account, you can <a href="https://platform.openai.com/signup" target="_blank">create one here</a>. Note: when you create a new OpenAI account you are given a limited amount of free API credits. If you created an account some time ago, however, these may have expired. If your credits have expired, you will need to enter a <a href="https://platform.openai.com/account/billing/overview" target="_blank">billing method</a> before you can use the API. You can check the state of any credits <a href="https://platform.openai.com/usage" target="_blank">here</a>.
    </p>
    <p>
      Login to <a href="https://openai.com/" target="_blank">OpenAI</a>, and navigate to the <a href="https://platform.openai.com/docs/" target="_blank">API documentation</a>.
    </p>
    <div class="featured_img_center">
      <a href="images/50-days/OpenAI_keys.png"><img src="images/50-days/OpenAI_keys.png" ALT="Screenshot of the OpenAI API Keys page showing where to click to create a new key." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
    </div>
    <p>Once you are looking at the API docs, follow the steps outlined in the image above. That is:
    <ol>
      <li>Select "API keys" from the left menu</li>
      <li>Click "+ Create new secret key"</li>
    </ol>
    <hr>
    <p>
      On LIT Prompt's <i>Templates & Settings</i> screen, set your API Base to <code>https://api.openai.com/v1/chat/completions</code> and your API Key equal to the value you got above after clicking "+ Create new secret key".  You get there by clicking the <i>Templates & Settings</i> button in the extension's popup:
    </p>
    <ol>
      <li>open the extension</li>
      <li>click on  <i>Templates & Settings</i></li>
      <li>enter the API Base and Key (under the section <i>OpenAI-Compatible API Integration</i>)</li>
    </ol>
    <div class="featured_img_center">
      <a href="images/50-days/popup.png"><img src="images/50-days/popup.png" ALT="Screenshot of the LIT Prompts extension popup." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
    </div>
    <p>
      Once those two bits of information (the API Base and Key) are in place, you're good to go. Now you can edit, create, and run prompt templates. Just open the LIT Prompts extension, and click one of the options. I suggest, however, that you read through the <i>Templates and Settings</i> screen to get oriented. You might even try out a few of the preloaded prompt templates. This will let you jump right in and get your hands dirty in the next section.
    </p>
    <div class="featured_img_center">
      <a href="images/50-days/credentials.png"><img src="images/50-days/credentials.png" ALT="Screenshot of the LIT Prompts extension popup." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
    </div>
    <p>
      <span style="background:yellow;">If you receive an error when trying to run a template after entering your Base and Key, and you are using OpenAI, make sure to check the state of any credits <a href="https://platform.openai.com/usage" target="_blank">here</a>. If you don't have any credits, you will need a billing method on file.</span>
    </p>
    <p>
      <i>If you found this hard to follow, consider following along with the first four minutes of the video <a href="posts/flag-fallacies/#setup">above</a>. It covers the same content. It focuses on Firefox, but once you've installed the extension, the steps are the same.</i>
    </p>
  </div>

  <!--
    =================================================

                   Write Your Template

    =================================================
  -->
  <hr>
  <h2><a name="template" href="posts/flag-fallacies/#template" class="anchor" alt="deep link to this section"></a>The Prompt Pattern (Template)</h2>

  <div class="featured_img_right">
    <a href="images/boxquote.png"><img src="images/boxquote.png" ALT="A slide showing the Georeg Box quote: All models are wrong, but some models are useful." class="list_img_file"/></a>
    <div class="caption">
      Maps are models; they don't show everything. That's okay as long as you don't confuse the map for the territory.
    </div>
  </div>

  <p>
    When crafting a LIT Prompts template, we use a mix of plain language and variable placeholders. Specifically, you can use double curly brackets to encase predefined variables. If the text between the brackets matches one of our predefined variable names, that section of text will be replaced with the variable's value. Today we'll be using <code>{{innerText}}</code>. See the extension's <a href="https://github.com/SuffolkLITLab/prompts#prompt-templates" target="_blank">documentation</a>.
  </p>

  <p>
    The <code>{{innerText}}</code> variable will be replaced by the <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/innerText" target="_blank">innerText</a> of your current page (roughly speaking the hard-coded text of a page). If you read through the prompt below, its operation is pretty strightforward. You'll note that just like <a href="posts/rpg-quanta">Monday's template</a>, we're using <a href="https://en.wikipedia.org/wiki/JSON" target="_blank">JSON</a> to help structre our output while providing a frame for a <a href="https://en.wikipedia.org/wiki/Prompt_engineering#Chain-of-thought" target="_blank">chain-of-thought</a> prompt.
  </p>
  <p>FWIW, the list of logical fallacies and their definitions were drawn from the excellent <a href="https://yourlogicalfallacyis.com/" target="_blank">Thou shalt not commit logical fallacies</a> poster, as described <a href="https://www.pesec.no/24-most-common-logical-fallacies/" target="_blank">here</a> and reformatted by gpt-3.5.</p>

  <p>Here's the template's title.</p>
  <p><code>Flag Logical Fallacies with a Browser Extension</code></p>
  <p>Here's the template's text.</p>
  <!--
    #########################
    #########################
    #####   start code   ####
    #########################
    #########################
  -->
  <section class="line-numbers">
    <pre class="language-xxx" style="white-space:pre-wrap;"><code>You are a high school debate coach, working with science and English teachers to help your students better understand common logical fallacies. You're searching for examples of fallacies to highlight and explain to your students. In a moment, I will give you a text to evaluate for logical fallacies. The text may or may not contain fallacies you can use. Be on the lookout for any of the following common logical fallacies: 

1. Strawman: Misrepresenting someone's argument to make it easier to attack.
2. False cause: Assuming a relationship between things means that one is the cause of the other.
3. Appeal to emotion: Manipulating an emotional response instead of a valid argument.
4. Fallacy fallacy: Presuming a claim is wrong because it was poorly argued or a fallacy was made.
5. Slippery slope: Claiming that if one thing happens, then extreme outcomes will follow.
6. Ad hominem: Attacking an opponent's character to undermine their argument.
7. Tu quoque: Avoiding criticism by turning it back on the accuser.
8. Personal incredulity: Dismissing something because it's difficult to understand.
9. Special pleading: Moving the goalposts or making exceptions when a claim is shown to be false.
10. Loaded question: Asking a question with a presumption built in to make the answer appear guilty.
11. Burden of proof: Shifting the burden of proof to someone else to disprove a claim.
12. Ambiguity: Using double meanings or ambiguous language to mislead.
13. Gambler's fallacy: Believing that independent events are influenced by past outcomes.
14. Bandwagon: Appealing to popularity as a form of validation.
15. Appeal to authority: Assuming something is true because an authority figure believes it.
16. Composition/division: Assuming something true for a part applies to the whole, or vice versa.
17. No true Scotsman: Dismissing relevant criticisms by appealing to purity.
18. Genetic: Judging something as good or bad based on where it comes from.
19. Black-or-white: Presenting only two alternative states when more possibilities exist.
20. Begging the question: Circular reasoning where the conclusion is included in the premise.
21. Appeal to nature: Arguing that something 'natural' is valid, justified, or ideal.
22. Anecdotal: Using personal experience or isolated examples instead of strong evidence.
23. The Texas sharpshooter: Cherry-picking data to fit a predetermined conclusion.
24. Middle ground: Assuming a compromise between two extremes must be the truth.

---

Here's the text:

{{innerText}}

---

Now that you've read the text. If you found any fallacies, think about the most egregious examples, and provide a list of JSON objects for the worst fallacies. The list should be of the following structure: 

{
  "fallacies": [
   {
    "fallacy":"name of fallacy",
     "explanation":"explanation of why the text is an example of this fallacy"
   },
   {
    "fallacy":"name of fallacy",
     "explanation":"explanation of why the text is an example of this fallacy"
   }
  ]
}

If multiple fallacies are present, provide multiple examples, but no more than three in total. If there are no clear fallacies return:

{
  "fallacies": []
}


</code></pre>
  </section>
  <!--
    #########################
    #########################
    #####    end code    ####
    #########################
    #########################
  -->
  <p>And here are the template's parameters:</p>
  <ul>
    <li><b>Output Type:</b> <code>LLM</code>. This choice means that we'll "run" the template through an LLM (i.e., this will ping an LLM and return a result). Alternatively, we could have chosen "Prompt," in which case the extension would return the text of the completed template. </li>
    <li><b>Model:</b> <code>gpt-4-turbo-preview</code>. This input specifies what model we should use when running the prompt. Available models differ based on your API provider. See e.g., <a href="https://platform.openai.com/docs/models" target="_blank">OpenAI's list of models</a>.</li>
    <li><b>Temperature:</b> <code>0</code>. Temperature runs from 0 to 1 and specifies how "random" the answer should be. Since we're seeking fidelity to a text, I went with the least "creative" setting—0.</li>
    <li><b>Max Tokens:</b> <code>1000</code>. This number specifies how long the reply can be. Tokens are chunks of text the model uses to do its thing. They don't quite match up with words but are close. 1 token is something like 3/4 of a word. Smaller token limits run faster.</li>
    <li><b>JSON:</b> <code>Yes</code>. This asks the model to output its answer in something called JSON, which is a nice machine-readable way to structure data. See <a href="https://en.wikipedia.org/wiki/JSON" target="_blank">https://en.wikipedia.org/wiki/JSON</a></li>
    <li><b>Output To:</b> <code>Screen Only</code>. We can output the first reply from the LLM to a number of places, the screen, the clipboard... Here, we're content just to have it go to the screen.</li>
    <li><b>Post-run Behavior:</b> <code>FULL STOP</code>. Like the choice of output, we can decide what to do after a template runs. To keep things simple, I went with "FULL STOP."</li>    <li><b>Hide Button:</b> <code>unchecked</code>. This determines if a button is displayed for this template in the extension's popup window. </li>  </ul>
<h3><a name="working" href="posts/flag-fallacies/#working" class="anchor" alt="deep link to this section"></a>Working with the above template</h3>
  <p>
    To work with the above template, you could copy it and its parameters into LIT Prompts one by one, or you could download a single prompts file and upload it from the extension's <i>Templates &amp; Settings</i> screen. This will replace your existing prompts.
  </p>

  <div class="featured_img_center" style="max-width:900px;">
    <a href="images/50-days/template_upload.png"><img src="images/50-days/template_upload.png" ALT="Screenshot of the LIT Prompts Templates and Settings page showing where to upload prompts files." class="list_img_file" style="border: 1px solid #a5a5a5;"/></a>
  </div>
  <p>
    You can download a prompts file (the above template and its parameters) suitable for upload by clicking this button:
  </p>

  <div class="button_row">
    <a href="javascript:void('');" onClick="saveTextAsFile(prompts,'prompt_template.txt')" class="button" style="width:220px;">Download prompts file</a>
  </div>
  <!--
    =================================================

                    Kick the Tires

    =================================================
  -->
  <hr>
  <h2><a name="tires" href="posts/flag-fallacies/#tires" class="anchor" alt="deep link to this section"></a>Kick the Tires</h2>
  <p>
    It's one thing to read about something and another to put what you've learned into practice. Let's see how this template performs.
  </p>
  <ul>
  
    <li>
      <b>Pick a fight</b>. Open three articles you think are particularly insightful and three "hate reads" you think wildly miss the mark. Run the template on them all, and see what you think of its classifications. Do you see where it's coming from? Does it have a point? If you disagree, why? Can you tweak the template to have it behave the way you think it should? What do you have to add? Do you have to tell it that some things are beyond argument? If so, what? Could you defend your choices to someone who disagreed with you on the topic of the offending text?
    </li>
    
  </ul>
    <!--
    =================================================

                       References

    =================================================
  -->
  <hr>
  <h2><a name="references" href="posts/flag-fallacies/#references" class="anchor" alt="deep link to this section"></a>TL;DR References</h2>
  <p>
    ICYMI, here are blubs for a selection of works I linked to in this post. If you didn't click through above, you might want to give them a look now.   </p>
  <ul> 


    <li>
      <a href="https://www.nytimes.com/2023/12/06/business/dealbook/silicon-valley-artificial-intelligence.html" target="_blank">Silicon Valley Confronts a Grim New A.I. Metric</a> by Kevin Roose. 
      P(doom), or the probability of doom, is a statistic that some artificial intelligence researchers use to assess the likelihood of an AI apocalypse or other catastrophic event caused by AI. It has become a popular topic of discussion in Silicon Valley, with techies casually asking each other about their p(doom) as a way to gauge their views on the potential risks of AI. The term originated on an online message board called LessWrong and has since been adopted by members of the Effective Altruism movement. However, p(doom) is not a precise measurement and is more about where someone stands on the spectrum of utopia to dystopia. It reflects their thoughts on the potential impact of AI and its regulation.
      <i>Summary based on a draft from our <a href="posts/summarize-and-question">day one template</a>.</i>
    </li>

    <li>
      <a href="https://link.springer.com/article/10.1007/s13347-020-00405-8" target="_blank">Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence</a> by Shakir Mohamed, Marie-Therese Png & William Isaac. 
      The article discusses the integration of decolonial theory into artificial intelligence (AI) to address ethical and societal impacts. It highlights the importance of critical science and post-colonial theories in understanding AI's role in modern societies, emphasizing the need for a decolonial approach to prevent harm to vulnerable populations. The paper proposes tactics for developing a decolonial AI, including creating a critical technical practice, seeking reverse tutelage, and renewing affective and political communities. These strategies aim to align AI research and technology development with ethical principles, centering on the well-being of all individuals, especially those most affected by technological advancements.
      <i>Summary based on a draft from our <a href="posts/summarize-and-question">day one template</a>.</i>
    </li>

    <li>
      <a href="https://www.techdirt.com/2023/05/11/moderator-mayhem-a-mobile-game-to-see-how-well-you-can-handle-content-moderation/" target="_blank">Moderator Mayhem: A Mobile Game To See How Well YOU Can Handle Content Moderation</a> by Mike Masnick. 
        Moderator Mayhem is a mobile browser-based game developed in partnership with Engine that allows players to experience the challenges of content moderation. In the game, players act as front-line content moderators for a fictional review website called TrustHive. They must make quick decisions about whether to keep up or take down user-generated content based on the company's policies. The game aims to provide a realistic understanding of the complex scenarios and competing pressures faced by content moderators. Players receive feedback on their performance and can see how their decisions are perceived by the public and their superiors. The game does not provide a "correct" answer, as content moderation often involves subjective judgment. 
      <i>Summary based on a draft from our <a href="posts/summarize-and-question">day one template</a>.</i>
    </li>

    <li>
      <a href="https://yourlogicalfallacyis.com/" target="_blank">Thou shalt not commit logical fallacies</a> by School of Thought. 
      FWIW, this poster hangs in my office at the law school. From the site's description, "A logical fallacy is a flaw in reasoning. Logical fallacies are like tricks or illusions of thought, and they're often very sneakily used by politicians and the media to fool people. Don't be fooled! This website has been designed to help you identify and call out dodgy logic wherever it may raise its ugly, incoherent head."
    </li>

  </ul>

  <!--
  =================================================

                  Preview projects

  =================================================
  -->
  <div id="previews"></div>

  </div>
  <!-- END PAGE CONTENT -->
  <div class="footer">
      <span class="footer_links">
        <a href="https://mastodon.social/@Colarusso" target="_blank">Mastodon</a>
        | <a href="https://github.com/colarusso" target="_blank">GitHub</a>
        | <a href="./privacy">Privacy</a>
        | <a href="https://sadlynothavocdinosaur.com/feed.xml">RSS</a>
      </span>
      <span class="byline">Site by David Colarusso</span>
  </div>
</div>

<script>
  /*new GreenAudioPlayer('.gap-example');
  const audio_object = document.querySelector('.gap-example  audio');

  try {
    MathJax.typeset();
	} catch (error) {}*/

  (async () => {
    prompts = await loadFile('posts/flag-fallacies/prompt_template.txt');
  })()
</script>

</BODY></HTML>
